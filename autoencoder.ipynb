{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondad424485431ff4a8aa076a99cfa3fb48c",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "600"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = iris['data']\n",
    "dataset.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'list'>\n"
    }
   ],
   "source": [
    "splitdata = np.split(dataset, [120])\n",
    "print(type(splitdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(splitdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trainset flattened size: 480, Rows: 120, Columns: 4\n"
    }
   ],
   "source": [
    "print(\"Trainset flattened size: {}, Rows: {}, Columns: {}\".format(splitdata[0].size, splitdata[0].shape[0], splitdata[0].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[5.1 3.5 0.  0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]\n [4.6 3.1 1.5 0.2]\n [5.  3.6 1.4 0.2]\n [5.4 3.9 1.7 0.4]\n [4.6 3.4 1.4 0.3]\n [5.  0.  1.5 0.2]\n [4.4 2.9 1.4 0.2]\n [0.  3.1 1.5 0.1]\n [5.4 3.7 1.5 0.2]\n [0.  3.4 1.6 0.2]\n [4.8 3.  1.4 0.1]\n [4.3 3.  1.1 0.1]\n [5.8 4.  1.2 0. ]\n [5.7 4.4 1.5 0. ]\n [5.4 3.9 1.3 0.4]\n [5.1 3.5 1.4 0.3]\n [5.7 0.  0.  0. ]\n [0.  3.8 1.5 0.3]\n [5.4 3.4 1.7 0. ]\n [5.1 3.7 1.5 0. ]\n [4.6 3.6 1.  0.2]\n [5.1 3.3 1.7 0.5]\n [4.8 3.4 1.9 0.2]\n [5.  3.  1.6 0.2]\n [5.  3.4 1.6 0.4]\n [5.2 3.5 1.5 0.2]\n [5.2 3.4 1.4 0.2]\n [4.7 3.2 1.6 0.2]\n [4.8 3.1 1.6 0.2]\n [5.4 3.4 0.  0.4]\n [5.2 4.1 1.5 0.1]\n [5.5 4.2 1.4 0.2]\n [4.9 3.1 0.  0.2]\n [5.  0.  1.2 0.2]\n [5.5 3.5 1.3 0.2]\n [4.9 3.6 1.4 0.1]\n [4.4 3.  1.3 0.2]\n [5.1 3.4 0.  0.2]\n [5.  3.5 1.3 0.3]\n [4.5 2.3 1.3 0.3]\n [4.4 3.2 1.3 0.2]\n [5.  0.  1.6 0.6]\n [0.  3.8 1.9 0.4]\n [4.8 3.  1.4 0.3]\n [5.1 3.8 0.  0.2]\n [4.6 3.2 1.4 0.2]\n [5.3 3.7 1.5 0.2]\n [5.  3.3 1.4 0.2]\n [7.  3.2 0.  1.4]\n [0.  3.2 4.5 1.5]\n [6.9 0.  4.9 1.5]\n [5.5 2.3 4.  1.3]\n [6.5 2.8 4.6 1.5]\n [5.7 2.8 4.5 0. ]\n [0.  3.3 4.7 1.6]\n [4.9 0.  3.3 0. ]\n [6.6 2.9 4.6 1.3]\n [5.2 0.  3.9 1.4]\n [5.  2.  0.  1. ]\n [5.9 3.  4.2 1.5]\n [6.  2.2 0.  1. ]\n [6.1 0.  0.  0. ]\n [5.6 2.9 3.6 1.3]\n [6.7 3.1 4.4 1.4]\n [5.6 3.  4.5 1.5]\n [5.8 2.7 0.  1. ]\n [0.  2.2 4.5 1.5]\n [5.6 2.5 3.9 1.1]\n [5.9 3.2 4.8 1.8]\n [6.1 2.8 4.  1.3]\n [6.3 2.5 0.  1.5]\n [6.1 2.8 4.7 1.2]\n [6.4 2.9 4.3 0. ]\n [0.  0.  4.4 1.4]\n [6.8 2.8 4.8 1.4]\n [6.7 3.  5.  1.7]\n [0.  2.9 4.5 1.5]\n [5.7 0.  3.5 1. ]\n [0.  2.4 3.8 1.1]\n [5.5 2.4 3.7 1. ]\n [5.8 2.7 0.  0. ]\n [6.  2.7 5.1 0. ]\n [5.4 3.  4.5 1.5]\n [6.  3.4 4.5 1.6]\n [6.7 0.  4.7 1.5]\n [6.3 2.3 4.4 1.3]\n [5.6 3.  4.1 1.3]\n [5.5 2.5 4.  1.3]\n [5.5 2.6 4.4 1.2]\n [6.1 3.  4.6 1.4]\n [5.8 2.6 4.  1.2]\n [5.  2.3 3.3 1. ]\n [5.6 2.7 4.2 1.3]\n [5.7 3.  4.2 1.2]\n [5.7 2.9 4.2 1.3]\n [0.  2.9 4.3 0. ]\n [5.1 2.5 3.  0. ]\n [0.  2.8 4.1 1.3]\n [6.3 3.3 6.  2.5]\n [0.  2.7 5.1 1.9]\n [7.1 3.  0.  2.1]\n [0.  2.9 5.6 0. ]\n [6.5 3.  5.8 2.2]\n [7.6 3.  6.6 2.1]\n [0.  0.  4.5 1.7]\n [7.3 2.9 6.3 1.8]\n [6.7 2.5 5.8 1.8]\n [7.2 3.6 6.1 2.5]\n [6.5 3.2 5.1 0. ]\n [6.4 2.7 0.  1.9]\n [6.8 3.  5.5 2.1]\n [5.7 2.5 5.  2. ]\n [5.8 2.8 5.1 0. ]\n [6.4 0.  5.3 2.3]\n [6.5 3.  0.  1.8]\n [7.7 3.8 6.7 2.2]\n [7.7 2.6 6.9 2.3]\n [6.  2.2 5.  1.5]]\n"
    }
   ],
   "source": [
    "REPLACE_COUNT = 60\n",
    "NAN = 0\n",
    "\n",
    "mdata = splitdata[0].copy()\n",
    "mdata.flat[np.random.choice(mdata.size, REPLACE_COUNT, replace=False)] = NAN\n",
    "\n",
    "testdata = splitdata[1].copy()\n",
    "testdata.flat[np.random.choice(testdata.size, int(REPLACE_COUNT*0.2), replace=False)] = NAN\n",
    "print(mdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 120, 4])\n"
    }
   ],
   "source": [
    "# y_train = splitdata[0].copy()\n",
    "\n",
    "# x_train = mdata.copy()\n",
    "\n",
    "# y_test = splitdata[1].copy()\n",
    "\n",
    "# x_test = testdata.copy()\n",
    "\n",
    "\n",
    "y_train = torch.from_numpy(splitdata[0])\n",
    "y_train = y_train.view(1, 120, 4)\n",
    "\n",
    "x_train = torch.from_numpy(mdata)\n",
    "x_train = x_train.view(1, 120, 4)\n",
    "\n",
    "y_test = torch.from_numpy(splitdata[1])\n",
    "y_test = y_test.view(1, 30, 4)\n",
    "\n",
    "x_test = torch.from_numpy(testdata)\n",
    "x_test = x_test.view(1, 30, 4)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Trainset flattened size: 480, Rows: 120, Columns: 4\nTarget flattened size: <built-in method size of Tensor object at 0x000002ADBF313438>, Rows: 1, Columns: 120\nTestset flattened size: <built-in method size of Tensor object at 0x000002ADBF269D80>, Rows: 1, Columns: 30\nTarget Test flattened size: <built-in method size of Tensor object at 0x000002ADBF313F78>, Rows: 1, Columns: 30\n"
    }
   ],
   "source": [
    "print(\"Trainset flattened size: {}, Rows: {}, Columns: {}\".format(mdata.size, mdata.shape[0], mdata.shape[1]))\n",
    "print(\"Target flattened size: {}, Rows: {}, Columns: {}\".format(y_train.size, y_train.shape[0], y_train.shape[1]))\n",
    "print(\"Testset flattened size: {}, Rows: {}, Columns: {}\".format(x_test.size, x_test.shape[0], x_test.shape[1]))\n",
    "print(\"Target Test flattened size: {}, Rows: {}, Columns: {}\".format(y_test.size, y_test.shape[0], y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_features=4, out_features=3)\n",
    "        self.enc2 = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "        self.dec1 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.dec2 = nn.Linear(in_features=3, out_features=4)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.enc1(x))\n",
    "        x = F.leaky_relu(self.enc2(x))\n",
    "        x = F.leaky_relu(self.dec1(x))\n",
    "        x = self.dec2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Autoencoder(\n  (enc1): Linear(in_features=4, out_features=3, bias=True)\n  (enc2): Linear(in_features=3, out_features=2, bias=True)\n  (dec1): Linear(in_features=2, out_features=3, bias=True)\n  (dec2): Linear(in_features=3, out_features=4, bias=True)\n)\n"
    }
   ],
   "source": [
    "net = Autoencoder().double()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n  module=Autoencoder(\n    (enc1): Linear(in_features=4, out_features=3, bias=True)\n    (enc2): Linear(in_features=3, out_features=2, bias=True)\n    (dec1): Linear(in_features=2, out_features=3, bias=True)\n    (dec2): Linear(in_features=3, out_features=4, bias=True)\n  ),\n)\n"
    }
   ],
   "source": [
    "net = NeuralNetRegressor(\n",
    "    Autoencoder().double(),\n",
    "    max_epochs=500,\n",
    "    lr=LEARNING_RATE,\n",
    "    device='cuda',\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer = optim.Adam,\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 6000\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1\n"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    train_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data, labels in zip(x_train, y_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if epoch == NUM_EPOCHS-1:\n",
    "                for i in range(len(data)):\n",
    "                    print(\"Input: \", data[i])\n",
    "                    print(\"Target: \", labels[i])\n",
    "                    print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_train)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'\n",
    "            .format(epoch+1, NUM_EPOCHS, loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(net):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        running_loss = 0.0\n",
    "        for data, labels in zip(x_test, y_test):\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            for i in range(len(data)):\n",
    "                print(\"Input: \", data[i])\n",
    "                print(\"Target: \", labels[i])\n",
    "                print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_test)\n",
    "        test_loss.append(loss)\n",
    "        print('Test Loss: {:.3f}'.format(loss))\n",
    "\n",
    "        return test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.4000, 0.0000, 0.2000], dtype=torch.float64)\nTarget:  tensor([5.1000, 3.4000, 1.5000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([5.3195, 3.4625, 1.6914, 0.2911], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.5000, 1.3000, 0.3000], dtype=torch.float64)\nTarget:  tensor([5.0000, 3.5000, 1.3000, 0.3000], dtype=torch.float64)\nOutputs:  tensor([4.9656, 3.3986, 1.2953, 0.1574], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([4.5000, 2.3000, 1.3000, 0.3000], dtype=torch.float64)\nTarget:  tensor([4.5000, 2.3000, 1.3000, 0.3000], dtype=torch.float64)\nOutputs:  tensor([4.5119, 3.1230, 1.3250, 0.2332], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([4.4000, 3.2000, 1.3000, 0.2000], dtype=torch.float64)\nTarget:  tensor([4.4000, 3.2000, 1.3000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([4.4383, 3.0885, 1.3014, 0.2324], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 0.0000, 1.6000, 0.6000], dtype=torch.float64)\nTarget:  tensor([5.0000, 3.5000, 1.6000, 0.6000], dtype=torch.float64)\nOutputs:  tensor([4.9985, 3.2441, 1.7777, 0.3747], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 3.8000, 1.9000, 0.4000], dtype=torch.float64)\nTarget:  tensor([5.1000, 3.8000, 1.9000, 0.4000], dtype=torch.float64)\nOutputs:  tensor([5.1686, 3.3049, 1.8845, 0.4006], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.0000, 1.4000, 0.3000], dtype=torch.float64)\nTarget:  tensor([4.8000, 3.0000, 1.4000, 0.3000], dtype=torch.float64)\nOutputs:  tensor([4.7881, 3.2546, 1.4074, 0.2332], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 0.0000, 0.2000], dtype=torch.float64)\nTarget:  tensor([5.1000, 3.8000, 1.6000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([5.2906, 3.4710, 1.6211, 0.2628], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.2000, 1.4000, 0.2000], dtype=torch.float64)\nTarget:  tensor([4.6000, 3.2000, 1.4000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([4.6155, 3.1560, 1.4014, 0.2541], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.3000, 3.7000, 1.5000, 0.2000], dtype=torch.float64)\nTarget:  tensor([5.3000, 3.7000, 1.5000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([5.2307, 3.4821, 1.4932, 0.2121], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.3000, 1.4000, 0.2000], dtype=torch.float64)\nTarget:  tensor([5.0000, 3.3000, 1.4000, 0.2000], dtype=torch.float64)\nOutputs:  tensor([4.9629, 3.3595, 1.3994, 0.2057], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.0000, 3.2000, 0.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([7.0000, 3.2000, 4.7000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.8985, 3.0158, 5.4916, 1.8226], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 3.2000, 4.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.4000, 3.2000, 4.5000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.1675, 2.9465, 4.4992, 1.4663], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 0.0000, 4.9000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.1000, 4.9000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.6886, 3.0513, 5.0530, 1.6496], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.3000, 4.0000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.5000, 2.3000, 4.0000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.5287, 2.7447, 4.0241, 1.3352], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 2.8000, 4.6000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.5000, 2.8000, 4.6000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.4349, 3.0620, 4.6121, 1.4816], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.8000, 4.5000, 0.0000], dtype=torch.float64)\nTarget:  tensor([5.7000, 2.8000, 4.5000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.6874, 2.6652, 4.5020, 1.5332], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 3.3000, 4.7000, 1.6000], dtype=torch.float64)\nTarget:  tensor([6.3000, 3.3000, 4.7000, 1.6000], dtype=torch.float64)\nOutputs:  tensor([6.2592, 2.9289, 4.6968, 1.5445], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 0.0000, 3.3000, 0.0000], dtype=torch.float64)\nTarget:  tensor([4.9000, 2.4000, 3.3000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([4.8933, 2.6071, 3.3758, 1.1240], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.6000, 2.9000, 4.6000, 1.3000], dtype=torch.float64)\nTarget:  tensor([6.6000, 2.9000, 4.6000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([6.5095, 3.1036, 4.6172, 1.4737], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 0.0000, 3.9000, 1.4000], dtype=torch.float64)\nTarget:  tensor([5.2000, 2.7000, 3.9000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([5.2271, 2.5834, 3.9830, 1.3575], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5., 2., 0., 1.], dtype=torch.float64)\nTarget:  tensor([5.0000, 2.0000, 3.5000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([5.6545, 3.3646, 2.5064, 0.6201], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.9000, 3.0000, 4.2000, 1.5000], dtype=torch.float64)\nTarget:  tensor([5.9000, 3.0000, 4.2000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([5.9026, 2.8969, 4.2074, 1.3683], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.2000, 0.0000, 1.0000], dtype=torch.float64)\nTarget:  tensor([6.0000, 2.2000, 4.0000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([6.2320, 3.2030, 3.8915, 1.1780], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 0.0000, 0.0000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.1000, 2.9000, 4.7000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.0834, 3.2459, 3.5314, 1.0327], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.9000, 3.6000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.6000, 2.9000, 3.6000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.6084, 2.9402, 3.6102, 1.1340], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.1000, 4.4000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.1000, 4.4000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.4851, 3.1123, 4.5534, 1.4477], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 3.0000, 4.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([5.6000, 3.0000, 4.5000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([5.6565, 2.6457, 4.5061, 1.5393], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 0.0000, 1.0000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.7000, 4.1000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([6.0775, 3.2467, 3.5196, 1.0281], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.2000, 4.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.2000, 2.2000, 4.5000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([5.9715, 2.8230, 4.5244, 1.5047], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.5000, 3.9000, 1.1000], dtype=torch.float64)\nTarget:  tensor([5.6000, 2.5000, 3.9000, 1.1000], dtype=torch.float64)\nOutputs:  tensor([5.6074, 2.8287, 3.9182, 1.2757], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.9000, 3.2000, 4.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([5.9000, 3.2000, 4.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([5.9428, 2.7062, 4.8023, 1.6364], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.0000, 1.3000], dtype=torch.float64)\nTarget:  tensor([6.1000, 2.8000, 4.0000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([6.0561, 3.0569, 4.0122, 1.2575], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.5000, 0.0000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.5000, 4.9000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.5712, 3.1065, 4.7093, 1.5076], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.7000, 1.2000], dtype=torch.float64)\nTarget:  tensor([6.1000, 2.8000, 4.7000, 1.2000], dtype=torch.float64)\nOutputs:  tensor([6.0833, 2.8217, 4.7094, 1.5744], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.9000, 4.3000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.9000, 4.3000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([6.2854, 3.0866, 4.3011, 1.3590], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 0.0000, 4.4000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.6000, 3.0000, 4.4000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([5.5489, 2.5924, 4.4796, 1.5418], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.8000, 2.8000, 4.8000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.8000, 2.8000, 4.8000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.6276, 3.0695, 4.9034, 1.5892], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.0000, 5.0000, 1.7000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.0000, 5.0000, 1.7000], dtype=torch.float64)\nOutputs:  tensor([6.6358, 3.0369, 5.0073, 1.6358], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.9000, 4.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.0000, 2.9000, 4.5000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.1087, 2.9094, 4.5068, 1.4778], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 0.0000, 3.5000, 1.0000], dtype=torch.float64)\nTarget:  tensor([5.7000, 2.6000, 3.5000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([5.6279, 2.9616, 3.5825, 1.1185], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.4000, 3.8000, 1.1000], dtype=torch.float64)\nTarget:  tensor([5.5000, 2.4000, 3.8000, 1.1000], dtype=torch.float64)\nOutputs:  tensor([5.8030, 2.9786, 3.8190, 1.2033], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.4000, 3.7000, 1.0000], dtype=torch.float64)\nTarget:  tensor([5.5000, 2.4000, 3.7000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([5.5070, 2.8413, 3.7206, 1.1986], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 0.0000, 0.0000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.7000, 3.9000, 1.2000], dtype=torch.float64)\nOutputs:  tensor([5.7110, 3.3528, 2.6308, 0.6696], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.7000, 5.1000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.0000, 2.7000, 5.1000, 1.6000], dtype=torch.float64)\nOutputs:  tensor([5.9711, 2.6144, 5.1031, 1.7708], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.0000, 4.5000, 1.5000], dtype=torch.float64)\nTarget: tensor([5.4000, 3.0000, 4.5000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([5.4837, 2.5449, 4.5058, 1.5628], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 3.4000, 4.5000, 1.6000], dtype=torch.float64)\nTarget:  tensor([6.0000, 3.4000, 4.5000, 1.6000], dtype=torch.float64)\nOutputs:  tensor([6.0128, 2.8569, 4.4971, 1.4864], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 0.0000, 4.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.1000, 4.7000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.5617, 3.0744, 4.7832, 1.5429], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.3000, 4.4000, 1.3000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.3000, 4.4000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([6.2375, 3.0144, 4.4239, 1.4220], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 3.0000, 4.1000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.6000, 3.0000, 4.1000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.6321, 2.7755, 4.1061, 1.3587], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.5000, 4.0000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.5000, 2.5000, 4.0000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.5323, 2.7486, 4.0190, 1.3323], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.6000, 4.4000, 1.2000], dtype=torch.float64)\nTarget:  tensor([5.5000, 2.6000, 4.4000, 1.2000], dtype=torch.float64)\nOutputs:  tensor([5.5481, 2.6154, 4.4145, 1.5121], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 3.0000, 4.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.1000, 3.0000, 4.6000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.0895, 2.8625, 4.6058, 1.5260], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.6000, 4.0000, 1.2000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.6000, 4.0000, 1.2000], dtype=torch.float64)\nOutputs:  tensor([5.7899, 2.8999, 4.0162, 1.2958], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 2.3000, 3.3000, 1.0000], dtype=torch.float64)\nTarget:  tensor([5.0000, 2.3000, 3.3000, 1.0000], dtype=torch.float64)\nOutputs:  tensor([5.0557, 2.7207, 3.3237, 1.0778], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.7000, 4.2000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.6000, 2.7000, 4.2000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.6311, 2.7362, 4.2134, 1.4082], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 3.0000, 4.2000, 1.2000], dtype=torch.float64)\nTarget:  tensor([5.7000, 3.0000, 4.2000, 1.2000], dtype=torch.float64)\nOutputs:  tensor([5.7194, 2.7907, 4.2053, 1.3924], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.9000, 4.2000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.7000, 2.9000, 4.2000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.7211, 2.7906, 4.2085, 1.3936], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.9000, 4.3000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.2000, 2.9000, 4.3000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([6.4255, 3.1755, 4.2817, 1.3309], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 2.5000, 3.0000, 0.0000], dtype=torch.float64)\nTarget:  tensor([5.1000, 2.5000, 3.0000, 1.1000], dtype=torch.float64)\nOutputs:  tensor([5.0980, 2.8571, 3.0136, 0.9294], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.8000, 4.1000, 1.3000], dtype=torch.float64)\nTarget:  tensor([5.7000, 2.8000, 4.1000, 1.3000], dtype=torch.float64)\nOutputs:  tensor([5.9448, 2.9569, 4.1092, 1.3174], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 3.3000, 6.0000, 2.5000], dtype=torch.float64)\nTarget:  tensor([6.3000, 3.3000, 6.0000, 2.5000], dtype=torch.float64)\nOutputs:  tensor([6.3669, 2.5222, 6.0008, 2.1295], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([6.1964, 2.7425, 5.1125, 1.7443], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.1000, 3.0000, 0.0000, 2.1000], dtype=torch.float64)\nTarget:  tensor([7.1000, 3.0000, 5.9000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([7.2286, 2.9208, 6.2910, 2.1450], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 2.9000, 5.6000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.9000, 5.6000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.9362, 3.0578, 5.4362, 1.7919], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 5.8000, 2.2000], dtype=torch.float64)\nTarget:  tensor([6.5000, 3.0000, 5.8000, 2.2000], dtype=torch.float64)\nOutputs:  tensor([6.5152, 2.6783, 5.8075, 2.0203], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.6000, 3.0000, 6.6000, 2.1000], dtype=torch.float64)\nTarget:  tensor([7.6000, 3.0000, 6.6000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([7.3844, 2.8461, 6.7507, 2.3351], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([0.0000, 0.0000, 4.5000, 1.7000], dtype=torch.float64)\nTarget:  tensor([4.9000, 2.5000, 4.5000, 1.7000], dtype=torch.float64)\nOutputs: tensor([5.3613, 2.4464, 4.5809, 1.6141], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.3000, 2.9000, 6.3000, 1.8000], dtype=torch.float64)\nTarget:  tensor([7.3000, 2.9000, 6.3000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([7.2052, 2.8977, 6.3169, 2.1601], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 2.5000, 5.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.7000, 2.5000, 5.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.6652, 2.7621, 5.8180, 2.0046], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.2000, 3.6000, 6.1000, 2.5000], dtype=torch.float64)\nTarget:  tensor([7.2000, 3.6000, 6.1000, 2.5000], dtype=torch.float64)\nOutputs:  tensor([7.1281, 2.9208, 6.1279, 2.0837], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.2000, 5.1000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.5000, 3.2000, 5.1000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([6.4122, 2.8762, 5.0911, 1.7049], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.7000, 0.0000, 1.9000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.7000, 5.3000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([6.7626, 3.0517, 5.1717, 1.6941], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.8000, 3.0000, 5.5000, 2.1000], dtype=torch.float64)\nTarget:  tensor([6.8000, 3.0000, 5.5000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.7579, 2.9278, 5.5083, 1.8495], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.5000, 5.0000, 2.0000], dtype=torch.float64)\nTarget:  tensor([5.7000, 2.5000, 5.0000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([5.7731, 2.5285, 5.0204, 1.7599], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.8000, 5.1000, 0.0000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.8000, 5.1000, 2.4000], dtype=torch.float64)\nOutputs:  tensor([5.8001, 2.5156, 5.1003, 1.7929], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 0.0000, 5.3000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.4000, 3.2000, 5.3000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.3564, 2.7375, 5.3858, 1.8481], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 0.0000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.5000, 3.0000, 5.5000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.7635, 3.0521, 5.1721, 1.6941], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 3.8000, 6.7000, 2.2000], dtype=torch.float64)\nTarget:  tensor([7.7000, 3.8000, 6.7000, 2.2000], dtype=torch.float64)\nOutputs:  tensor([7.4438, 2.8292, 6.8940, 2.3928], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 2.6000, 6.9000, 2.3000], dtype=torch.float64)\nTarget:  tensor([7.7000, 2.6000, 6.9000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([7.5017, 2.8111, 7.0381, 2.4512], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.2000, 5.0000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.0000, 2.2000, 5.0000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.0096, 2.6648, 5.0254, 1.7298], dtype=torch.float64,\n       grad_fn=<SelectBackward>)\nEpoch 6000 of 6000, Train Loss: 0.069\n"
    }
   ],
   "source": [
    "train_loss1 = train(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Loss')"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 382.603125 277.314375\" width=\"382.603125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 382.603125 277.314375 \r\nL 382.603125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 40.603125 239.758125 \r\nL 375.403125 239.758125 \r\nL 375.403125 22.318125 \r\nL 40.603125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m47164f55f3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.821307\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(52.640057 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"116.816024\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(107.272274 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.81074\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(168.26699 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.805457\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(229.261707 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.800174\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(290.256424 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.79489\" xlink:href=\"#m47164f55f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(351.25114 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Epochs -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(190.0875 268.034687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"ma1cd3dd682\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"235.43567\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.240625 239.234889)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"201.304697\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(27.240625 205.103916)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"167.173723\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(27.240625 170.972942)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"133.04275\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.240625 136.841968)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"98.911776\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(27.240625 102.710995)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"64.780803\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(20.878125 68.580021)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#ma1cd3dd682\" y=\"30.649829\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 12 -->\r\n      <g transform=\"translate(20.878125 34.449048)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- Loss -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p892b4cecc5)\" d=\"M 55.821307 32.201761 \r\nL 57.651148 156.153033 \r\nL 58.261095 182.166532 \r\nL 58.871043 199.603024 \r\nL 59.48099 209.444382 \r\nL 60.090937 213.844729 \r\nL 60.700884 215.438846 \r\nL 61.310831 215.958022 \r\nL 62.530726 216.223677 \r\nL 64.970514 216.33237 \r\nL 74.729669 216.350456 \r\nL 106.446922 216.449411 \r\nL 113.766288 216.714937 \r\nL 120.475707 217.193899 \r\nL 134.504491 218.60166 \r\nL 143.653699 219.615788 \r\nL 144.263646 209.653528 \r\nL 144.873593 219.6629 \r\nL 145.48354 219.735345 \r\nL 146.093488 219.155357 \r\nL 146.703435 219.816405 \r\nL 149.753171 220.064279 \r\nL 150.363118 219.223733 \r\nL 150.973065 220.13564 \r\nL 157.682484 220.627566 \r\nL 197.32905 223.404234 \r\nL 208.308099 224.472781 \r\nL 222.946831 226.116318 \r\nL 236.975615 227.64512 \r\nL 251.614347 228.694552 \r\nL 260.153608 229.031965 \r\nL 274.182393 229.365213 \r\nL 280.891811 229.473744 \r\nL 283.941547 229.512254 \r\nL 284.551494 229.363156 \r\nL 285.161442 229.530275 \r\nL 299.800174 229.686875 \r\nL 302.849909 229.714391 \r\nL 303.459857 229.584825 \r\nL 304.069804 229.719578 \r\nL 337.006951 229.856192 \r\nL 337.616898 229.688816 \r\nL 338.226845 229.850035 \r\nL 343.106423 229.865432 \r\nL 343.71637 229.691958 \r\nL 344.326317 229.852885 \r\nL 352.865577 229.874039 \r\nL 353.475524 229.709058 \r\nL 354.085472 229.860662 \r\nL 358.355102 229.874146 \r\nL 358.965049 229.765728 \r\nL 360.184943 229.874489 \r\nL 360.184943 229.874489 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 40.603125 239.758125 \r\nL 40.603125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 375.403125 239.758125 \r\nL 375.403125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 40.603125 239.758125 \r\nL 375.403125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 40.603125 22.318125 \r\nL 375.403125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_16\">\r\n    <!-- Train Loss -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n     <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n    </defs>\r\n    <g transform=\"translate(178.543125 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"271.675781\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"325.638672\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"386.820312\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"438.919922\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p892b4cecc5\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss1)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input:  tensor([6.9000, 3.2000, 5.7000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.2000, 5.7000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.8570, 2.9360, 5.6496, 1.8968], dtype=torch.float64)\nInput:  tensor([5.6000, 0.0000, 4.9000, 2.0000], dtype=torch.float64)\nTarget:  tensor([5.6000, 2.8000, 4.9000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([5.6345, 2.4775, 4.9404, 1.7386], dtype=torch.float64)\nInput:  tensor([7.7000, 2.8000, 6.7000, 0.0000], dtype=torch.float64)\nTarget:  tensor([7.7000, 2.8000, 6.7000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([7.4058, 2.8527, 6.7711, 2.3369], dtype=torch.float64)\nInput:  tensor([6.3000, 2.7000, 4.9000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.7000, 4.9000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.2785, 2.8801, 4.8666, 1.6164], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000, 2.1000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.3000, 5.7000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.6792, 2.8333, 5.6464, 1.9197], dtype=torch.float64)\nInput:  tensor([7.2000, 0.0000, 0.0000, 1.8000], dtype=torch.float64)\nTarget:  tensor([7.2000, 3.2000, 6.0000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([7.3956, 2.8852, 6.6642, 2.2892], dtype=torch.float64)\nInput:  tensor([6.2000, 2.8000, 4.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.2000, 2.8000, 4.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.1894, 2.8647, 4.7648, 1.5818], dtype=torch.float64)\nInput:  tensor([6.1000, 3.0000, 4.9000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.1000, 3.0000, 4.9000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.1108, 2.7848, 4.8591, 1.6359], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000, 2.1000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.8000, 5.6000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.4072, 2.7053, 5.5608, 1.9177], dtype=torch.float64)\nInput:  tensor([7.2000, 3.0000, 5.8000, 1.6000], dtype=torch.float64)\nTarget:  tensor([7.2000, 3.0000, 5.8000, 1.6000], dtype=torch.float64)\nOutputs:  tensor([7.0233, 2.9645, 5.8402, 1.9616], dtype=torch.float64)\nInput:  tensor([7.4000, 2.8000, 6.1000, 1.9000], dtype=torch.float64)\nTarget:  tensor([7.4000, 2.8000, 6.1000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([7.1720, 2.9214, 6.2009, 2.1070], dtype=torch.float64)\nInput:  tensor([7.9000, 3.8000, 6.4000, 2.0000], dtype=torch.float64)\nTarget:  tensor([7.9000, 3.8000, 6.4000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([7.4078, 2.8554, 6.7666, 2.3347], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000, 2.2000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.8000, 5.6000, 2.2000], dtype=torch.float64)\nOutputs:  tensor([6.4105, 2.7071, 5.5613, 1.9175], dtype=torch.float64)\nInput:  tensor([6.3000, 0.0000, 5.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.8000, 5.1000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.2311, 2.7557, 5.1352, 1.7463], dtype=torch.float64)\nInput:  tensor([6.1000, 2.6000, 5.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.1000, 2.6000, 5.6000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.1210, 2.5373, 5.5633, 1.9581], dtype=torch.float64)\nInput:  tensor([7.7000, 3.0000, 6.1000, 2.3000], dtype=torch.float64)\nTarget:  tensor([7.7000, 3.0000, 6.1000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([7.2680, 2.8957, 6.4280, 2.1982], dtype=torch.float64)\nInput:  tensor([6.3000, 0.0000, 5.6000, 2.4000], dtype=torch.float64)\nTarget:  tensor([6.3000, 3.4000, 5.6000, 2.4000], dtype=torch.float64)\nOutputs:  tensor([6.2832, 2.6057, 5.6365, 1.9695], dtype=torch.float64)\nInput:  tensor([6.4000, 3.1000, 5.5000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.4000, 3.1000, 5.5000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.3978, 2.7390, 5.4521, 1.8690], dtype=torch.float64)\nInput:  tensor([6.0000, 3.0000, 4.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.0000, 3.0000, 4.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.0201, 2.7676, 4.7600, 1.6028], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.4000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.1000, 5.4000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.7648, 2.9927, 5.3427, 1.7684], dtype=torch.float64)\nInput:  tensor([6.7000, 3.1000, 0.0000, 2.4000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.1000, 5.6000, 2.4000], dtype=torch.float64)\nOutputs:  tensor([7.0955, 2.9694, 5.9435, 1.9992], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.1000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.1000, 5.1000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.7456, 3.0461, 5.1632, 1.6886], dtype=torch.float64)\nInput:  tensor([5.8000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([5.8586, 2.5628, 5.0669, 1.7660], dtype=torch.float64)\nInput:  tensor([6.8000, 3.2000, 5.9000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.8000, 3.2000, 5.9000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.7794, 2.8190, 5.8485, 1.9989], dtype=torch.float64)\nInput:  tensor([0.0000, 3.3000, 5.7000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.3000, 5.7000, 2.5000], dtype=torch.float64)\nOutputs:  tensor([6.6059, 3.2255, 4.4381, 1.3745], dtype=torch.float64)\nInput:  tensor([6.7000, 3.0000, 5.2000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.0000, 5.2000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.6591, 2.9972, 5.1584, 1.6983], dtype=torch.float64)\nInput:  tensor([6.3000, 2.5000, 5.0000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.5000, 5.0000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([6.2188, 2.8108, 4.9623, 1.6685], dtype=torch.float64)\nInput:  tensor([6.5000, 3.0000, 5.2000, 2.0000], dtype=torch.float64)\nTarget:  tensor([6.5000, 3.0000, 5.2000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([6.4762, 2.8908, 5.1574, 1.7229], dtype=torch.float64)\nInput:  tensor([6.2000, 3.4000, 5.4000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.2000, 3.4000, 5.4000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.1652, 2.6448, 5.3362, 1.8477], dtype=torch.float64)\nInput:  tensor([5.9000, 3.0000, 5.1000, 1.8000], dtype=torch.float64)\nTarget:  tensor([5.9000, 3.0000, 5.1000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([5.9467, 2.6173, 5.0583, 1.7500], dtype=torch.float64)\nTest Loss: 0.093\n"
    }
   ],
   "source": [
    "test_result = test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './iris_autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Autoencoder(\n  (enc1): Linear(in_features=4, out_features=3, bias=True)\n  (enc2): Linear(in_features=3, out_features=2, bias=True)\n  (dec1): Linear(in_features=2, out_features=3, bias=True)\n  (dec2): Linear(in_features=3, out_features=4, bias=True)\n)"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = torch.load('iris_autoencoder')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1\n"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(load_model.parameters(), lr=LEARNING_RATE)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input:  tensor([6.9000, 3.2000, 5.7000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.2000, 5.7000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.8570, 2.9360, 5.6496, 1.8968], dtype=torch.float64)\nInput:  tensor([5.6000, 0.0000, 4.9000, 2.0000], dtype=torch.float64)\nTarget:  tensor([5.6000, 2.8000, 4.9000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([5.6345, 2.4775, 4.9404, 1.7386], dtype=torch.float64)\nInput:  tensor([7.7000, 2.8000, 6.7000, 0.0000], dtype=torch.float64)\nTarget:  tensor([7.7000, 2.8000, 6.7000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([7.4058, 2.8527, 6.7711, 2.3369], dtype=torch.float64)\nInput:  tensor([6.3000, 2.7000, 4.9000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.7000, 4.9000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.2785, 2.8801, 4.8666, 1.6164], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000, 2.1000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.3000, 5.7000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.6792, 2.8333, 5.6464, 1.9197], dtype=torch.float64)\nInput:  tensor([7.2000, 0.0000, 0.0000, 1.8000], dtype=torch.float64)\nTarget:  tensor([7.2000, 3.2000, 6.0000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([7.3956, 2.8852, 6.6642, 2.2892], dtype=torch.float64)\nInput:  tensor([6.2000, 2.8000, 4.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.2000, 2.8000, 4.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.1894, 2.8647, 4.7648, 1.5818], dtype=torch.float64)\nInput:  tensor([6.1000, 3.0000, 4.9000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.1000, 3.0000, 4.9000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.1108, 2.7848, 4.8591, 1.6359], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000, 2.1000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.8000, 5.6000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.4072, 2.7053, 5.5608, 1.9177], dtype=torch.float64)\nInput:  tensor([7.2000, 3.0000, 5.8000, 1.6000], dtype=torch.float64)\nTarget:  tensor([7.2000, 3.0000, 5.8000, 1.6000], dtype=torch.float64)\nOutputs:  tensor([7.0233, 2.9645, 5.8402, 1.9616], dtype=torch.float64)\nInput:  tensor([7.4000, 2.8000, 6.1000, 1.9000], dtype=torch.float64)\nTarget:  tensor([7.4000, 2.8000, 6.1000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([7.1720, 2.9214, 6.2009, 2.1070], dtype=torch.float64)\nInput:  tensor([7.9000, 3.8000, 6.4000, 2.0000], dtype=torch.float64)\nTarget:  tensor([7.9000, 3.8000, 6.4000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([7.4078, 2.8554, 6.7666, 2.3347], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000, 2.2000], dtype=torch.float64)\nTarget:  tensor([6.4000, 2.8000, 5.6000, 2.2000], dtype=torch.float64)\nOutputs:  tensor([6.4105, 2.7071, 5.5613, 1.9175], dtype=torch.float64)\nInput:  tensor([6.3000, 0.0000, 5.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.8000, 5.1000, 1.5000], dtype=torch.float64)\nOutputs:  tensor([6.2311, 2.7557, 5.1352, 1.7463], dtype=torch.float64)\nInput:  tensor([6.1000, 2.6000, 5.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([6.1000, 2.6000, 5.6000, 1.4000], dtype=torch.float64)\nOutputs:  tensor([6.1210, 2.5373, 5.5633, 1.9581], dtype=torch.float64)\nInput:  tensor([7.7000, 3.0000, 6.1000, 2.3000], dtype=torch.float64)\nTarget:  tensor([7.7000, 3.0000, 6.1000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([7.2680, 2.8957, 6.4280, 2.1982], dtype=torch.float64)\nInput:  tensor([6.3000, 0.0000, 5.6000, 2.4000], dtype=torch.float64)\nTarget:  tensor([6.3000, 3.4000, 5.6000, 2.4000], dtype=torch.float64)\nOutputs:  tensor([6.2832, 2.6057, 5.6365, 1.9695], dtype=torch.float64)\nInput:  tensor([6.4000, 3.1000, 5.5000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.4000, 3.1000, 5.5000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.3978, 2.7390, 5.4521, 1.8690], dtype=torch.float64)\nInput:  tensor([6.0000, 3.0000, 4.8000, 1.8000], dtype=torch.float64)\nTarget:  tensor([6.0000, 3.0000, 4.8000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([6.0201, 2.7676, 4.7600, 1.6028], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.4000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.1000, 5.4000, 2.1000], dtype=torch.float64)\nOutputs:  tensor([6.7648, 2.9927, 5.3427, 1.7684], dtype=torch.float64)\nInput:  tensor([6.7000, 3.1000, 0.0000, 2.4000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.1000, 5.6000, 2.4000], dtype=torch.float64)\nOutputs:  tensor([7.0955, 2.9694, 5.9435, 1.9992], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.1000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.9000, 3.1000, 5.1000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.7456, 3.0461, 5.1632, 1.6886], dtype=torch.float64)\nInput:  tensor([5.8000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nTarget:  tensor([5.8000, 2.7000, 5.1000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([5.8586, 2.5628, 5.0669, 1.7660], dtype=torch.float64)\nInput:  tensor([6.8000, 3.2000, 5.9000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.8000, 3.2000, 5.9000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.7794, 2.8190, 5.8485, 1.9989], dtype=torch.float64)\nInput:  tensor([0.0000, 3.3000, 5.7000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.3000, 5.7000, 2.5000], dtype=torch.float64)\nOutputs:  tensor([6.6059, 3.2255, 4.4381, 1.3745], dtype=torch.float64)\nInput:  tensor([6.7000, 3.0000, 5.2000, 2.3000], dtype=torch.float64)\nTarget:  tensor([6.7000, 3.0000, 5.2000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.6591, 2.9972, 5.1584, 1.6983], dtype=torch.float64)\nInput:  tensor([6.3000, 2.5000, 5.0000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.3000, 2.5000, 5.0000, 1.9000], dtype=torch.float64)\nOutputs:  tensor([6.2188, 2.8108, 4.9623, 1.6685], dtype=torch.float64)\nInput:  tensor([6.5000, 3.0000, 5.2000, 2.0000], dtype=torch.float64)\nTarget:  tensor([6.5000, 3.0000, 5.2000, 2.0000], dtype=torch.float64)\nOutputs:  tensor([6.4762, 2.8908, 5.1574, 1.7229], dtype=torch.float64)\nInput:  tensor([6.2000, 3.4000, 5.4000, 0.0000], dtype=torch.float64)\nTarget:  tensor([6.2000, 3.4000, 5.4000, 2.3000], dtype=torch.float64)\nOutputs:  tensor([6.1652, 2.6448, 5.3362, 1.8477], dtype=torch.float64)\nInput:  tensor([5.9000, 3.0000, 5.1000, 1.8000], dtype=torch.float64)\nTarget:  tensor([5.9000, 3.0000, 5.1000, 1.8000], dtype=torch.float64)\nOutputs:  tensor([5.9467, 2.6173, 5.0583, 1.7500], dtype=torch.float64)\nTest Loss: 0.093\n"
    }
   ],
   "source": [
    "test_result = test(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'C:\\\\Users\\\\Firel\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(150, 4)"
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data = iris['data']\n",
    "extracted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[5.1000, 0.0000, 1.4000, 0.2000],\n         [4.9000, 3.0000, 1.4000, 0.2000],\n         [4.7000, 0.0000, 1.3000, 0.0000],\n         [4.6000, 3.1000, 0.0000, 0.2000],\n         [5.0000, 3.6000, 1.4000, 0.2000],\n         [5.4000, 3.9000, 1.7000, 0.0000],\n         [4.6000, 3.4000, 1.4000, 0.3000],\n         [5.0000, 3.4000, 1.5000, 0.2000],\n         [4.4000, 2.9000, 1.4000, 0.2000],\n         [4.9000, 3.1000, 1.5000, 0.1000],\n         [5.4000, 0.0000, 1.5000, 0.2000],\n         [4.8000, 3.4000, 1.6000, 0.2000],\n         [4.8000, 3.0000, 1.4000, 0.1000],\n         [4.3000, 3.0000, 0.0000, 0.1000],\n         [5.8000, 4.0000, 1.2000, 0.2000],\n         [5.7000, 4.4000, 1.5000, 0.4000],\n         [5.4000, 3.9000, 1.3000, 0.4000],\n         [5.1000, 3.5000, 1.4000, 0.3000],\n         [5.7000, 0.0000, 0.0000, 0.3000],\n         [5.1000, 3.8000, 0.0000, 0.3000],\n         [5.4000, 3.4000, 1.7000, 0.2000],\n         [5.1000, 3.7000, 1.5000, 0.4000],\n         [4.6000, 3.6000, 1.0000, 0.2000],\n         [5.1000, 3.3000, 1.7000, 0.0000],\n         [4.8000, 3.4000, 0.0000, 0.2000],\n         [5.0000, 3.0000, 0.0000, 0.2000],\n         [5.0000, 3.4000, 1.6000, 0.4000],\n         [5.2000, 0.0000, 0.0000, 0.2000],\n         [0.0000, 3.4000, 1.4000, 0.2000],\n         [4.7000, 3.2000, 1.6000, 0.2000],\n         [4.8000, 3.1000, 1.6000, 0.2000],\n         [5.4000, 3.4000, 1.5000, 0.4000],\n         [5.2000, 4.1000, 1.5000, 0.1000],\n         [5.5000, 4.2000, 1.4000, 0.2000],\n         [4.9000, 3.1000, 1.5000, 0.2000],\n         [5.0000, 3.2000, 1.2000, 0.2000],\n         [5.5000, 3.5000, 0.0000, 0.2000],\n         [4.9000, 3.6000, 1.4000, 0.1000],\n         [0.0000, 3.0000, 1.3000, 0.2000],\n         [5.1000, 3.4000, 1.5000, 0.2000],\n         [5.0000, 3.5000, 1.3000, 0.3000],\n         [4.5000, 2.3000, 1.3000, 0.3000],\n         [4.4000, 3.2000, 1.3000, 0.2000],\n         [5.0000, 3.5000, 1.6000, 0.6000],\n         [5.1000, 3.8000, 1.9000, 0.4000],\n         [4.8000, 3.0000, 1.4000, 0.3000],\n         [5.1000, 3.8000, 0.0000, 0.2000],\n         [4.6000, 3.2000, 1.4000, 0.2000],\n         [0.0000, 3.7000, 1.5000, 0.2000],\n         [5.0000, 3.3000, 1.4000, 0.2000],\n         [7.0000, 3.2000, 4.7000, 1.4000],\n         [6.4000, 3.2000, 4.5000, 1.5000],\n         [6.9000, 0.0000, 4.9000, 0.0000],\n         [5.5000, 2.3000, 4.0000, 1.3000],\n         [6.5000, 2.8000, 4.6000, 1.5000],\n         [5.7000, 2.8000, 4.5000, 1.3000],\n         [6.3000, 3.3000, 0.0000, 1.6000],\n         [4.9000, 2.4000, 3.3000, 1.0000],\n         [6.6000, 2.9000, 4.6000, 1.3000],\n         [5.2000, 2.7000, 0.0000, 1.4000],\n         [5.0000, 2.0000, 3.5000, 1.0000],\n         [5.9000, 3.0000, 4.2000, 1.5000],\n         [6.0000, 2.2000, 4.0000, 0.0000],\n         [6.1000, 2.9000, 4.7000, 1.4000],\n         [5.6000, 2.9000, 3.6000, 1.3000],\n         [6.7000, 3.1000, 0.0000, 1.4000],\n         [5.6000, 3.0000, 4.5000, 1.5000],\n         [5.8000, 2.7000, 4.1000, 0.0000],\n         [0.0000, 2.2000, 4.5000, 1.5000],\n         [5.6000, 2.5000, 3.9000, 1.1000],\n         [5.9000, 3.2000, 4.8000, 1.8000],\n         [6.1000, 2.8000, 4.0000, 1.3000],\n         [6.3000, 2.5000, 4.9000, 1.5000],\n         [0.0000, 2.8000, 4.7000, 1.2000],\n         [6.4000, 2.9000, 4.3000, 1.3000],\n         [0.0000, 3.0000, 4.4000, 1.4000],\n         [6.8000, 2.8000, 4.8000, 1.4000],\n         [6.7000, 3.0000, 5.0000, 1.7000],\n         [0.0000, 0.0000, 4.5000, 1.5000],\n         [5.7000, 2.6000, 3.5000, 1.0000],\n         [5.5000, 0.0000, 3.8000, 1.1000],\n         [5.5000, 2.4000, 3.7000, 1.0000],\n         [5.8000, 2.7000, 0.0000, 1.2000],\n         [6.0000, 2.7000, 5.1000, 1.6000],\n         [5.4000, 3.0000, 4.5000, 1.5000],\n         [6.0000, 3.4000, 4.5000, 1.6000],\n         [6.7000, 3.1000, 4.7000, 1.5000],\n         [0.0000, 2.3000, 0.0000, 1.3000],\n         [5.6000, 3.0000, 4.1000, 1.3000],\n         [5.5000, 2.5000, 4.0000, 1.3000],\n         [5.5000, 2.6000, 4.4000, 1.2000],\n         [6.1000, 3.0000, 4.6000, 1.4000],\n         [5.8000, 2.6000, 4.0000, 0.0000],\n         [5.0000, 2.3000, 3.3000, 1.0000],\n         [5.6000, 2.7000, 4.2000, 1.3000],\n         [5.7000, 3.0000, 4.2000, 1.2000],\n         [0.0000, 2.9000, 4.2000, 0.0000],\n         [6.2000, 2.9000, 4.3000, 1.3000],\n         [5.1000, 2.5000, 3.0000, 1.1000],\n         [0.0000, 2.8000, 4.1000, 0.0000],\n         [6.3000, 3.3000, 6.0000, 2.5000],\n         [5.8000, 2.7000, 5.1000, 1.9000],\n         [7.1000, 3.0000, 5.9000, 0.0000],\n         [6.3000, 2.9000, 5.6000, 1.8000],\n         [6.5000, 3.0000, 5.8000, 0.0000],\n         [7.6000, 3.0000, 6.6000, 2.1000],\n         [4.9000, 2.5000, 4.5000, 1.7000],\n         [7.3000, 2.9000, 6.3000, 1.8000],\n         [6.7000, 2.5000, 5.8000, 1.8000],\n         [7.2000, 3.6000, 6.1000, 2.5000],\n         [6.5000, 3.2000, 0.0000, 2.0000],\n         [6.4000, 2.7000, 5.3000, 1.9000],\n         [6.8000, 3.0000, 5.5000, 2.1000],\n         [5.7000, 2.5000, 5.0000, 2.0000],\n         [5.8000, 2.8000, 5.1000, 2.4000],\n         [6.4000, 3.2000, 5.3000, 2.3000],\n         [6.5000, 3.0000, 5.5000, 1.8000],\n         [7.7000, 0.0000, 6.7000, 2.2000],\n         [7.7000, 2.6000, 6.9000, 2.3000],\n         [6.0000, 0.0000, 5.0000, 1.5000],\n         [6.9000, 3.2000, 5.7000, 2.3000],\n         [5.6000, 2.8000, 0.0000, 0.0000],\n         [0.0000, 2.8000, 6.7000, 2.0000],\n         [6.3000, 2.7000, 4.9000, 1.8000],\n         [6.7000, 3.3000, 5.7000, 2.1000],\n         [0.0000, 3.2000, 6.0000, 1.8000],\n         [6.2000, 2.8000, 4.8000, 1.8000],\n         [6.1000, 3.0000, 4.9000, 1.8000],\n         [6.4000, 2.8000, 5.6000, 2.1000],\n         [7.2000, 3.0000, 5.8000, 1.6000],\n         [0.0000, 0.0000, 6.1000, 1.9000],\n         [7.9000, 0.0000, 6.4000, 2.0000],\n         [6.4000, 2.8000, 0.0000, 2.2000],\n         [6.3000, 2.8000, 5.1000, 1.5000],\n         [6.1000, 2.6000, 0.0000, 1.4000],\n         [7.7000, 3.0000, 6.1000, 2.3000],\n         [6.3000, 3.4000, 0.0000, 2.4000],\n         [0.0000, 3.1000, 5.5000, 1.8000],\n         [6.0000, 3.0000, 4.8000, 1.8000],\n         [6.9000, 3.1000, 5.4000, 2.1000],\n         [6.7000, 3.1000, 0.0000, 2.4000],\n         [6.9000, 3.1000, 5.1000, 2.3000],\n         [5.8000, 2.7000, 0.0000, 1.9000],\n         [6.8000, 3.2000, 5.9000, 2.3000],\n         [6.7000, 3.3000, 5.7000, 2.5000],\n         [6.7000, 3.0000, 5.2000, 2.3000],\n         [0.0000, 2.5000, 5.0000, 1.9000],\n         [6.5000, 3.0000, 5.2000, 2.0000],\n         [6.2000, 3.4000, 5.4000, 2.3000],\n         [5.9000, 3.0000, 5.1000, 1.8000]]], dtype=torch.float64)"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPLACE_COUNT = 60\n",
    "NAN = 0\n",
    "\n",
    "extracted_data.flat[np.random.choice(extracted_data.size, REPLACE_COUNT, replace=False)] = NAN\n",
    "\n",
    "dataset = torch.from_numpy(extracted_data)\n",
    "dataset.view(1, 150, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = load_model(dataset.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5.0635, 3.3650, 1.5488, 0.2582],\n        [4.8677, 3.3130, 1.3759, 0.2056],\n        [4.6328, 3.1832, 1.3552, 0.2283],\n        [5.0432, 3.5414, 1.0260, 0.0207],\n        [4.9644, 3.3752, 1.3598, 0.1850],\n        [5.3086, 3.4665, 1.6645, 0.2778],\n        [4.6187, 3.1710, 1.3665, 0.2355],\n        [4.9653, 3.3380, 1.4645, 0.2330],\n        [4.4341, 3.0584, 1.3796, 0.2668],\n        [4.8704, 3.2799, 1.4721, 0.2495],\n        [5.3424, 3.4568, 1.7463, 0.3107],\n        [4.7969, 3.2038, 1.5643, 0.3019],\n        [4.7780, 3.2607, 1.3756, 0.2178],\n        [4.6642, 3.3408, 0.9697, 0.0470],\n        [5.3704, 3.4535, 1.8006, 0.3319],\n        [5.3483, 3.4580, 1.7525, 0.3128],\n        [5.2019, 3.4991, 1.4008, 0.1712],\n        [5.0524, 3.4255, 1.3628, 0.1743],\n        [5.9576, 3.2892, 3.2094, 0.8988],\n        [5.3277, 3.4676, 1.6923, 0.2879],\n        [5.3118, 3.4655, 1.6724, 0.2810],\n        [5.0635, 3.3980, 1.4573, 0.2162],\n        [4.6009, 3.3052, 0.9647, 0.0533],\n        [5.0520, 3.3166, 1.6647, 0.3131],\n        [5.1423, 3.5190, 1.2489, 0.1096],\n        [5.2896, 3.4778, 1.6024, 0.2519],\n        [4.9764, 3.3084, 1.5648, 0.2776],\n        [5.6249, 3.3819, 2.4125, 0.5781],\n        [4.8672, 3.3151, 1.3690, 0.2025],\n        [4.7071, 3.1494, 1.5698, 0.3168],\n        [4.7918, 3.1979, 1.5722, 0.3063],\n        [5.2584, 3.4818, 1.5405, 0.2277],\n        [5.1466, 3.4509, 1.4451, 0.1992],\n        [5.2573, 3.4834, 1.5344, 0.2250],\n        [4.8738, 3.2817, 1.4726, 0.2492],\n        [4.9488, 3.4338, 1.1717, 0.1006],\n        [5.5495, 3.4061, 2.2230, 0.5014],\n        [4.8746, 3.3228, 1.3596, 0.1972],\n        [4.7082, 3.2542, 1.2804, 0.1836],\n        [5.0517, 3.3886, 1.4643, 0.2210],\n        [4.9617, 3.4082, 1.2636, 0.1411],\n        [4.5093, 3.1323, 1.2964, 0.2183],\n        [4.4348, 3.0975, 1.2723, 0.2174],\n        [4.9848, 3.3139, 1.5632, 0.2756],\n        [5.0827, 3.2670, 1.8522, 0.3950],\n        [4.7847, 3.2642, 1.3766, 0.2173],\n        [5.2910, 3.4781, 1.6036, 0.2522],\n        [4.6120, 3.1653, 1.3713, 0.2386],\n        [5.0067, 3.3637, 1.4602, 0.2253],\n        [4.9592, 3.3693, 1.3678, 0.1893],\n        [6.6521, 3.0753, 4.9302, 1.5943],\n        [6.3459, 3.0678, 4.4542, 1.4176],\n        [6.6627, 3.0696, 4.9631, 1.6080],\n        [5.5245, 2.7582, 3.9821, 1.3133],\n        [6.4298, 3.0774, 4.5640, 1.4565],\n        [5.7276, 2.7028, 4.4653, 1.5075],\n        [6.5509, 3.1234, 4.6325, 1.4714],\n        [4.9672, 2.6843, 3.2837, 1.0688],\n        [6.5019, 3.1165, 4.5720, 1.4503],\n        [5.8697, 3.3125, 3.0022, 0.8156],\n        [5.0555, 2.6606, 3.4928, 1.1527],\n        [5.8974, 2.9110, 4.1625, 1.3450],\n        [5.9110, 2.9856, 3.9772, 1.2580],\n        [6.0869, 2.8422, 4.6610, 1.5482],\n        [5.6037, 2.9533, 3.5686, 1.1124],\n        [6.7289, 3.0745, 5.0570, 1.6421],\n        [5.6513, 2.6598, 4.4612, 1.5161],\n        [5.7511, 2.8611, 4.0638, 1.3198],\n        [5.9742, 2.8394, 4.4860, 1.4832],\n        [5.6032, 2.8423, 3.8763, 1.2538],\n        [5.9371, 2.7209, 4.7549, 1.6119],\n        [6.0514, 3.0711, 3.9676, 1.2344],\n        [6.2650, 2.8708, 4.8704, 1.6200],\n        [6.5220, 3.0943, 4.6665, 1.4910],\n        [6.3253, 3.1249, 4.2624, 1.3323],\n        [6.1393, 2.9795, 4.3647, 1.4048],\n        [6.6199, 3.0828, 4.8571, 1.5651],\n        [6.6302, 3.0530, 4.9567, 1.6095],\n        [5.5442, 2.5672, 4.5447, 1.5691],\n        [5.6705, 3.0259, 3.4754, 1.0604],\n        [5.4699, 2.7764, 3.8431, 1.2569],\n        [5.5030, 2.8545, 3.6799, 1.1773],\n        [6.1516, 3.2350, 3.6749, 1.0861],\n        [6.0213, 2.6585, 5.0650, 1.7428],\n        [5.4785, 2.5587, 4.4617, 1.5400],\n        [6.0071, 2.8714, 4.4503, 1.4623],\n        [6.5626, 3.0993, 4.7184, 1.5093],\n        [2.3464, 2.1045, 0.6455, 0.2161],\n        [5.6271, 2.7891, 4.0628, 1.3363],\n        [5.5279, 2.7621, 3.9768, 1.3104],\n        [5.5436, 2.6294, 4.3712, 1.4895],\n        [6.0843, 2.8774, 4.5590, 1.5016],\n        [5.7450, 2.8924, 3.9671, 1.2761],\n        [5.0519, 2.7328, 3.2861, 1.0583],\n        [5.6264, 2.7501, 4.1701, 1.3857],\n        [5.7145, 2.8046, 4.1614, 1.3696],\n        [6.3792, 3.1975, 4.1480, 1.2723],\n        [6.1525, 3.0239, 4.2629, 1.3562],\n        [5.1320, 2.8887, 2.9830, 0.9080],\n        [6.4302, 3.1661, 4.3182, 1.3435],\n        [6.3602, 2.5387, 5.9473, 2.1018],\n        [5.8586, 2.5628, 5.0669, 1.7660],\n        [6.9577, 2.9256, 5.8417, 1.9713],\n        [6.3124, 2.6513, 5.5569, 1.9290],\n        [6.4350, 2.6557, 5.7436, 1.9979],\n        [7.3750, 2.8620, 6.6953, 2.3064],\n        [5.0448, 2.3000, 4.4769, 1.6065],\n        [7.1963, 2.9129, 6.2639, 2.1326],\n        [6.6599, 2.7792, 5.7654, 1.9771],\n        [7.1185, 2.9360, 6.0737, 2.0559],\n        [6.8232, 3.0464, 5.2880, 1.7353],\n        [6.3857, 2.7994, 5.2644, 1.7845],\n        [6.7518, 2.9445, 5.4553, 1.8220],\n        [5.7678, 2.5434, 4.9736, 1.7356],\n        [5.8771, 2.5737, 5.0668, 1.7634],\n        [6.4077, 2.8163, 5.2532, 1.7763],\n        [6.4825, 2.7876, 5.4545, 1.8585],\n        [7.4377, 2.8433, 6.8489, 2.3684],\n        [7.4923, 2.8273, 6.9818, 2.4219],\n        [5.9676, 2.6374, 5.0365, 1.7371],\n        [6.8570, 2.9360, 5.6496, 1.8968],\n        [5.5860, 3.3960, 2.3102, 0.5365],\n        [7.3801, 2.8595, 6.7105, 2.3126],\n        [6.2785, 2.8801, 4.8666, 1.6164],\n        [6.6792, 2.8333, 5.6464, 1.9197],\n        [7.0835, 2.9463, 5.9884, 2.0215],\n        [6.1894, 2.8647, 4.7648, 1.5818],\n        [6.1108, 2.7848, 4.8591, 1.6359],\n        [6.4072, 2.7053, 5.5608, 1.9177],\n        [7.0233, 2.9645, 5.8402, 1.9616],\n        [6.4794, 2.5407, 6.1352, 2.1718],\n        [7.4117, 2.8533, 6.7789, 2.3397],\n        [6.8662, 3.0334, 5.3939, 1.7780],\n        [6.2788, 2.8102, 5.0612, 1.7058],\n        [6.4098, 3.1626, 4.2948, 1.3355],\n        [7.2680, 2.8957, 6.4280, 2.1982],\n        [6.8370, 3.0414, 5.3242, 1.7500],\n        [6.6898, 2.9079, 5.4563, 1.8309],\n        [6.0201, 2.7676, 4.7600, 1.6028],\n        [6.8270, 3.0212, 5.3641, 1.7698],\n        [7.0955, 2.9694, 5.9435, 1.9992],\n        [6.7456, 3.0461, 5.1632, 1.6886],\n        [6.4082, 3.1614, 4.2955, 1.3361],\n        [6.7794, 2.8190, 5.8485, 1.9989],\n        [6.6927, 2.8405, 5.6483, 1.9188],\n        [6.6591, 2.9972, 5.1584, 1.6983],\n        [6.0787, 2.7238, 4.9768, 1.6944],\n        [6.4762, 2.8908, 5.1574, 1.7229],\n        [6.2426, 2.6859, 5.3477, 1.8424],\n        [5.9467, 2.6173, 5.0583, 1.7500]], dtype=torch.float64,\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('first_iris.csv', new_dataset.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}