{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondad424485431ff4a8aa076a99cfa3fb48c",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "600"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = iris['data']\n",
    "dataset.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.asarray(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = dataset[:, :-1]\n",
    "target_features = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 2,  2,  2,  2,  2,  4,  3,  2,  2,  1,  2,  2,  1,  1,  2,  4,  4,\n        3,  3,  3,  2,  4,  2,  5,  2,  2,  4,  2,  2,  2,  2,  4,  1,  2,\n        2,  2,  2,  1,  2,  2,  3,  3,  2,  5,  4,  3,  2,  2,  2,  2, 12,\n       13, 13, 11, 13, 11, 14,  9, 11, 12,  9, 13,  9, 12, 11, 12, 13,  9,\n       13,  9, 15, 11, 13, 10, 11, 12, 12, 14, 13,  9,  9,  9, 10, 14, 13,\n       14, 13, 11, 11, 11, 10, 12, 10,  9, 11, 10, 11, 11,  9, 11, 21, 16,\n       18, 15, 18, 18, 14, 15, 15, 21, 17, 16, 18, 17, 20, 19, 15, 18, 19,\n       13, 19, 17, 17, 15, 18, 15, 15, 15, 18, 14, 16, 17, 18, 13, 12, 19,\n       20, 15, 15, 18, 20, 19, 16, 19, 21, 19, 16, 17, 19, 15],\n      dtype=int64)"
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = np.linspace(0, 6)\n",
    "y_binned = np.digitize(target_features, bins)\n",
    "y_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(training_features, target_features, random_state=0, stratify=y_binned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitdata = np.split(training_features, [120])\n",
    "# train_features = splitdata[0]\n",
    "# test_features = splitdata[1]\n",
    "\n",
    "# splitdata = np.split(target_features, [120])\n",
    "# train_target = splitdata[0]\n",
    "# test_target = splitdata[1]\n",
    "\n",
    "# train_missing = train_target.copy()\n",
    "# test_missing = test_target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE_COUNT = x_train.size*0.25\n",
    "# NAN = 0\n",
    "\n",
    "# x_train.flat[np.random.choice(x_train.size, int(REPLACE_COUNT), replace=False)] =  NAN\n",
    "\n",
    "# # Using 0.2 because the test set is a 0.2 subset of the whole dataset\n",
    "# x_test.flat[np.random.choice(x_test.size, int(REPLACE_COUNT*0.2), replace=False)] = NAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(112, 3)"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 112, 3])\n"
    }
   ],
   "source": [
    "x_train = torch.from_numpy(x_train)\n",
    "x_train = x_train.view(1, x_train.shape[0], 3)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "x_test = x_test.view(1, x_test.shape[0], 3)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_train = y_train.view(1, y_train.shape[0], 1)\n",
    "\n",
    "y_test = torch.from_numpy(y_test)\n",
    "y_test = y_test.view(1, y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.enc2 = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "        #self.dec1 = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.enc1(x))\n",
    "        #x = F.leaky_relu(self.enc2(x))\n",
    "        x = self.enc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "torch.manual_seed(random.randint(1, 10))\n",
    "net = Autoencoder().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1500\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1\n"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    train_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data, target in zip(x_train, y_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if epoch == NUM_EPOCHS-1:\n",
    "                for i in range(len(data)):\n",
    "                    print(\"Input: \", data[i])\n",
    "                    print(\"Target: \", target[i])\n",
    "                    print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_train)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'\n",
    "            .format(epoch+1, NUM_EPOCHS, loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(net):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        running_loss = 0.0\n",
    "        for data, target in zip(x_test, y_test):\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "            for i in range(len(data)):\n",
    "                print(\"Input: \", data[i])\n",
    "                print(\"Target: \", target[i])\n",
    "                print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_test)\n",
    "        test_loss.append(loss)\n",
    "        print('Test Loss: {:.3f}'.format(loss))\n",
    "\n",
    "        return test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "37], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 3.4000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([2.0473], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 3.2000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5444], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.6000, 5.6000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.9822], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 3.1000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.7659], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.2268], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([2.0121], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 3.8000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.3559], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.9000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5347], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 3.1000, 5.5000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.0166], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 5.8000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([2.0516], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.6000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.2580], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 3.4000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1923], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.7000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:tensor([1.3957], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.2000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2222], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.3000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.5000], dtype=torch.float64)\nOutputs:  tensor([0.3324], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.7930], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.9000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.6217], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.8149], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.1646], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.6000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.4868], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.0000, 3.2000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5768], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 2.5000, 3.0000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([0.8267], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.3058], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.8000, 3.2000, 5.9000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0929], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.0000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2408], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.4000, 3.0000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1594], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.2000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.8275], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 3.8000, 6.7000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([2.2597], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 2.8000, 6.7000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([2.1947], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 3.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2591], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.2000, 1.2000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.0804], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.1000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2805], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.4000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.3122], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.6000, 1.0000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.0976], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 2.3000, 3.3000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([0.9489], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2608], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 3.2000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0778], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.9000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.4205], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.9000, 3.0000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.8575], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.8000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3536], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.3000, 3.0000, 1.1000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.0723], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 2.8000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5111], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.1000, 3.0000, 5.9000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.0994], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 3.0000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.7376], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 3.1000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9129], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 2.7000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.2925], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 4.0000, 1.2000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1357], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 3.3000, 6.0000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([2.0753], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.5000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2211], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.1000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5918], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.3000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1963], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.9000, 5.6000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.0148], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 1.9000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.5196], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.2000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6548], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.7000, 3.2000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3093], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:tensor([5.6000, 2.9000, 3.6000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.1374], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.2000, 3.4000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0235], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.0121], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.3000, 2.9000, 6.3000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.1405], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.2194], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.2000, 3.2000, 6.0000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.1276], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 3.4000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.6239], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.9000, 3.8000, 6.4000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([2.2469], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.4000, 3.2000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1951], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2632], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.5000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.1830], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.8149], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2534], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.8000, 4.9000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.7566], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 2.6000, 6.9000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.1989], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.5000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.6755], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 3.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2206], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.2000, 3.6000, 6.1000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([2.1622], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.1000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.4447], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 3.1000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6679], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.3000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2373], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.2000, 2.2000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.3878], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.5000, 2.3000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.0235], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 5.5000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.9879], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.0756], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.0000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6182], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 2.0000, 3.5000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([0.9934], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.3000, 3.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2839], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 3.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.2608], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 3.2000, 5.3000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.9365], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2729], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.5000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.2211], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.0000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.7000], dtype=torch.float64)\nOutputs:  tensor([1.7209], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 3.0000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5906], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.3000, 4.4000], dtype=torch.float64)\nTarget: tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3457], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.4000, 3.8000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([1.1571], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.6039], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.4000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3340], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.3248], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 3.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.2206], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.6000, 2.9000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.5180], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 4.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.3650], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.2000, 2.8000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6419], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.9000, 4.3000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3929], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.1646], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.7000, 3.2000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1623], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.5000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([1.2130], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 3.0000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6995], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.4000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3174], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.5000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.6000], dtype=torch.float64)\nOutputs:  tensor([0.3300], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.5000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6265], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 3.0000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.4003], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.6000, 3.5000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.0239], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3726], dtype=torch.float64, grad_fn=<SelectBackward>)\nEpoch 1500 of 1500, Train Loss: 0.032\n"
    }
   ],
   "source": [
    "train_loss1 = train(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Loss')"
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m4d1dcfbaf1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"99.608323\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(90.064573 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"140.217213\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(130.673463 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.826104\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(171.282354 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.434995\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(211.891245 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.043886\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(249.318886 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.652776\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 1200 -->\r\n      <g transform=\"translate(289.927776 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"343.261667\" xlink:href=\"#m4d1dcfbaf1\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1400 -->\r\n      <g transform=\"translate(330.536667 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_9\">\r\n     <!-- Epochs -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(193.265625 268.034687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"me2cbe206ac\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#me2cbe206ac\" y=\"232.699833\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 236.499051)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#me2cbe206ac\" y=\"188.419389\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 192.218608)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#me2cbe206ac\" y=\"144.138945\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 147.938164)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#me2cbe206ac\" y=\"99.858502\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(20.878125 103.657721)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#me2cbe206ac\" y=\"55.578058\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(20.878125 59.377277)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- Loss -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p6137daffe9)\" d=\"M 58.999432 32.201761 \r\nL 60.826832 55.667871 \r\nL 62.451188 73.83528 \r\nL 64.278588 91.669345 \r\nL 66.105988 107.289133 \r\nL 68.542521 125.235552 \r\nL 70.572966 138.40074 \r\nL 72.400366 148.978248 \r\nL 74.024721 157.262518 \r\nL 75.649077 164.445553 \r\nL 77.070388 169.844349 \r\nL 78.491699 174.414761 \r\nL 79.913011 178.194294 \r\nL 81.131277 180.843914 \r\nL 82.349544 182.996626 \r\nL 83.567811 184.70943 \r\nL 84.786077 186.04486 \r\nL 86.004344 187.069341 \r\nL 87.425655 187.958956 \r\nL 89.050011 188.689509 \r\nL 91.080456 189.345309 \r\nL 94.9383 190.301355 \r\nL 101.638767 191.996384 \r\nL 106.30879 193.367665 \r\nL 106.714878 193.579392 \r\nL 106.917923 193.844522 \r\nL 107.933145 196.081941 \r\nL 108.339234 197.908928 \r\nL 108.948367 202.760891 \r\nL 109.354456 206.227615 \r\nL 109.96359 211.981441 \r\nL 110.572723 216.713186 \r\nL 111.181856 219.179355 \r\nL 111.587945 219.594402 \r\nL 111.79099 219.820746 \r\nL 112.806212 221.757116 \r\nL 113.212301 222.130291 \r\nL 114.024479 223.258443 \r\nL 114.633612 223.654118 \r\nL 115.242746 224.156993 \r\nL 117.070146 224.74121 \r\nL 118.491457 224.93393 \r\nL 120.521901 225.065199 \r\nL 122.552346 225.257382 \r\nL 123.770613 225.600517 \r\nL 124.176702 226.090422 \r\nL 124.58279 226.717951 \r\nL 124.785835 226.652296 \r\nL 125.394968 227.350233 \r\nL 125.598013 227.223877 \r\nL 126.004102 227.32774 \r\nL 126.207146 227.388081 \r\nL 126.613235 227.276852 \r\nL 127.019324 227.428377 \r\nL 127.425413 227.463456 \r\nL 127.831502 227.693144 \r\nL 128.237591 227.757915 \r\nL 128.643679 227.893699 \r\nL 129.252813 227.957178 \r\nL 130.47108 228.111231 \r\nL 134.12588 228.538193 \r\nL 138.186769 228.841988 \r\nL 150.775525 229.526248 \r\nL 160.724703 229.76249 \r\nL 174.328682 229.844634 \r\nL 264.886508 229.862787 \r\nL 267.323041 229.857411 \r\nL 271.383931 229.861777 \r\nL 274.429597 229.859543 \r\nL 285.800087 229.859145 \r\nL 289.657931 229.860327 \r\nL 296.561443 229.866153 \r\nL 298.794932 229.863538 \r\nL 306.307577 229.860588 \r\nL 363.363068 229.874212 \r\nL 363.363068 229.874212 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_16\">\r\n    <!-- Train Loss -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n     <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n    </defs>\r\n    <g transform=\"translate(181.72125 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"271.675781\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"325.638672\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"386.820312\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"438.919922\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p6137daffe9\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss1)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input:  tensor([7.4000, 2.8000, 6.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([2.1234], dtype=torch.float64)\nInput:  tensor([6.7000, 2.5000, 5.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.0303], dtype=torch.float64)\nInput:  tensor([6.0000, 2.2000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.1700], dtype=torch.float64)\nInput:  tensor([5.5000, 4.2000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3068], dtype=torch.float64)\nInput:  tensor([7.2000, 3.0000, 5.8000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([2.0651], dtype=torch.float64)\nInput:  tensor([6.3000, 2.7000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6682], dtype=torch.float64)\nInput:  tensor([5.5000, 2.5000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2782], dtype=torch.float64)\nInput:  tensor([5.4000, 3.9000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.2150], dtype=torch.float64)\nInput:  tensor([5.7000, 3.0000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.4439], dtype=torch.float64)\nInput:  tensor([6.3000, 2.8000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.7842], dtype=torch.float64)\nInput:  tensor([5.9000, 3.2000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.7521], dtype=torch.float64)\nInput:  tensor([5.5000, 2.4000, 3.7000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.1131], dtype=torch.float64)\nInput:  tensor([4.9000, 2.4000, 3.3000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([0.9824], dtype=torch.float64)\nInput:  tensor([5.5000, 3.5000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1326], dtype=torch.float64)\nInput:  tensor([6.6000, 3.0000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.4439], dtype=torch.float64)\nInput:  tensor([5.1000, 3.8000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.3280], dtype=torch.float64)\nInput:  tensor([6.8000, 2.8000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5825], dtype=torch.float64)\nInput:  tensor([5.7000, 2.5000, 5.0000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.7469], dtype=torch.float64)\nInput:  tensor([5.6000, 3.0000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6019], dtype=torch.float64)\nInput:  tensor([4.4000, 2.9000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1943], dtype=torch.float64)\nInput:  tensor([7.7000, 3.0000, 6.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.1560], dtype=torch.float64)\nInput:  tensor([5.7000, 2.8000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.5553], dtype=torch.float64)\nInput:  tensor([4.9000, 3.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1577], dtype=torch.float64)\nInput:  tensor([6.5000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.8472], dtype=torch.float64)\nInput:  tensor([6.2000, 2.9000, 4.3000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.4205], dtype=torch.float64)\nInput:  tensor([5.9000, 3.0000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.4221], dtype=torch.float64)\nInput:  tensor([6.8000, 3.0000, 5.5000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9616], dtype=torch.float64)\nInput:  tensor([4.8000, 3.4000, 1.9000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.4854], dtype=torch.float64)\nInput:  tensor([5.4000, 3.9000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.4112], dtype=torch.float64)\nInput:  tensor([5.2000, 4.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.3707], dtype=torch.float64)\nInput:  tensor([5.1000, 3.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2564], dtype=torch.float64)\nInput:  tensor([4.6000, 3.4000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.2619], dtype=torch.float64)\nInput:  tensor([5.0000, 3.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2541], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([2.0756], dtype=torch.float64)\nInput:  tensor([6.3000, 3.3000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.6774], dtype=torch.float64)\nInput:  tensor([5.8000, 2.8000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.8387], dtype=torch.float64)\nInput:  tensor([6.7000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.8254], dtype=torch.float64)\nInput:  tensor([7.6000, 3.0000, 6.6000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.1926], dtype=torch.float64)\nTest Loss: 0.046\n"
    }
   ],
   "source": [
    "test_result = test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './iris_autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Autoencoder(\n  (enc1): Linear(in_features=3, out_features=2, bias=True)\n  (enc2): Linear(in_features=2, out_features=1, bias=True)\n)"
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = torch.load('iris_autoencoder')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'C:\\\\Users\\\\Firel\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(150, 4)"
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data = iris['data']\n",
    "extracted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[5.1000, 3.5000, 1.4000],\n         [4.9000, 3.0000, 1.4000],\n         [4.7000, 3.2000, 1.3000],\n         [4.6000, 3.1000, 1.5000],\n         [5.0000, 3.6000, 1.4000],\n         [5.4000, 3.9000, 1.7000],\n         [4.6000, 3.4000, 1.4000],\n         [5.0000, 3.4000, 1.5000],\n         [4.4000, 2.9000, 1.4000],\n         [4.9000, 3.1000, 1.5000],\n         [5.4000, 3.7000, 1.5000],\n         [4.8000, 3.4000, 1.6000],\n         [4.8000, 3.0000, 1.4000],\n         [4.3000, 3.0000, 1.1000],\n         [5.8000, 4.0000, 1.2000],\n         [5.7000, 4.4000, 1.5000],\n         [5.4000, 3.9000, 1.3000],\n         [5.1000, 3.5000, 1.4000],\n         [5.7000, 3.8000, 1.7000],\n         [5.1000, 3.8000, 1.5000],\n         [5.4000, 3.4000, 1.7000],\n         [5.1000, 3.7000, 1.5000],\n         [4.6000, 3.6000, 1.0000],\n         [5.1000, 3.3000, 1.7000],\n         [4.8000, 3.4000, 1.9000],\n         [5.0000, 3.0000, 1.6000],\n         [5.0000, 3.4000, 1.6000],\n         [5.2000, 3.5000, 1.5000],\n         [5.2000, 3.4000, 1.4000],\n         [4.7000, 3.2000, 1.6000],\n         [4.8000, 3.1000, 1.6000],\n         [5.4000, 3.4000, 1.5000],\n         [5.2000, 4.1000, 1.5000],\n         [5.5000, 4.2000, 1.4000],\n         [4.9000, 3.1000, 1.5000],\n         [5.0000, 3.2000, 1.2000],\n         [5.5000, 3.5000, 1.3000],\n         [4.9000, 3.6000, 1.4000],\n         [4.4000, 3.0000, 1.3000],\n         [5.1000, 3.4000, 1.5000],\n         [5.0000, 3.5000, 1.3000],\n         [4.5000, 2.3000, 1.3000],\n         [4.4000, 3.2000, 1.3000],\n         [5.0000, 3.5000, 1.6000],\n         [5.1000, 3.8000, 1.9000],\n         [4.8000, 3.0000, 1.4000],\n         [5.1000, 3.8000, 1.6000],\n         [4.6000, 3.2000, 1.4000],\n         [5.3000, 3.7000, 1.5000],\n         [5.0000, 3.3000, 1.4000],\n         [7.0000, 3.2000, 4.7000],\n         [6.4000, 3.2000, 4.5000],\n         [6.9000, 3.1000, 4.9000],\n         [5.5000, 2.3000, 4.0000],\n         [6.5000, 2.8000, 4.6000],\n         [5.7000, 2.8000, 4.5000],\n         [6.3000, 3.3000, 4.7000],\n         [4.9000, 2.4000, 3.3000],\n         [6.6000, 2.9000, 4.6000],\n         [5.2000, 2.7000, 3.9000],\n         [5.0000, 2.0000, 3.5000],\n         [5.9000, 3.0000, 4.2000],\n         [6.0000, 2.2000, 4.0000],\n         [6.1000, 2.9000, 4.7000],\n         [5.6000, 2.9000, 3.6000],\n         [6.7000, 3.1000, 4.4000],\n         [5.6000, 3.0000, 4.5000],\n         [5.8000, 2.7000, 4.1000],\n         [6.2000, 2.2000, 4.5000],\n         [5.6000, 2.5000, 3.9000],\n         [5.9000, 3.2000, 4.8000],\n         [6.1000, 2.8000, 4.0000],\n         [6.3000, 2.5000, 4.9000],\n         [6.1000, 2.8000, 4.7000],\n         [6.4000, 2.9000, 4.3000],\n         [6.6000, 3.0000, 4.4000],\n         [6.8000, 2.8000, 4.8000],\n         [6.7000, 3.0000, 5.0000],\n         [6.0000, 2.9000, 4.5000],\n         [5.7000, 2.6000, 3.5000],\n         [5.5000, 2.4000, 3.8000],\n         [5.5000, 2.4000, 3.7000],\n         [5.8000, 2.7000, 3.9000],\n         [6.0000, 2.7000, 5.1000],\n         [5.4000, 3.0000, 4.5000],\n         [6.0000, 3.4000, 4.5000],\n         [6.7000, 3.1000, 4.7000],\n         [6.3000, 2.3000, 4.4000],\n         [5.6000, 3.0000, 4.1000],\n         [5.5000, 2.5000, 4.0000],\n         [5.5000, 2.6000, 4.4000],\n         [6.1000, 3.0000, 4.6000],\n         [5.8000, 2.6000, 4.0000],\n         [5.0000, 2.3000, 3.3000],\n         [5.6000, 2.7000, 4.2000],\n         [5.7000, 3.0000, 4.2000],\n         [5.7000, 2.9000, 4.2000],\n         [6.2000, 2.9000, 4.3000],\n         [5.1000, 2.5000, 3.0000],\n         [5.7000, 2.8000, 4.1000],\n         [6.3000, 3.3000, 6.0000],\n         [5.8000, 2.7000, 5.1000],\n         [7.1000, 3.0000, 5.9000],\n         [6.3000, 2.9000, 5.6000],\n         [6.5000, 3.0000, 5.8000],\n         [7.6000, 3.0000, 6.6000],\n         [4.9000, 2.5000, 4.5000],\n         [7.3000, 2.9000, 6.3000],\n         [6.7000, 2.5000, 5.8000],\n         [7.2000, 3.6000, 6.1000],\n         [6.5000, 3.2000, 5.1000],\n         [6.4000, 2.7000, 5.3000],\n         [6.8000, 3.0000, 5.5000],\n         [5.7000, 2.5000, 5.0000],\n         [5.8000, 2.8000, 5.1000],\n         [6.4000, 3.2000, 5.3000],\n         [6.5000, 3.0000, 5.5000],\n         [7.7000, 3.8000, 6.7000],\n         [7.7000, 2.6000, 6.9000],\n         [6.0000, 2.2000, 5.0000],\n         [6.9000, 3.2000, 5.7000],\n         [5.6000, 2.8000, 4.9000],\n         [7.7000, 2.8000, 6.7000],\n         [6.3000, 2.7000, 4.9000],\n         [6.7000, 3.3000, 5.7000],\n         [7.2000, 3.2000, 6.0000],\n         [6.2000, 2.8000, 4.8000],\n         [6.1000, 3.0000, 4.9000],\n         [6.4000, 2.8000, 5.6000],\n         [7.2000, 3.0000, 5.8000],\n         [7.4000, 2.8000, 6.1000],\n         [7.9000, 3.8000, 6.4000],\n         [6.4000, 2.8000, 5.6000],\n         [6.3000, 2.8000, 5.1000],\n         [6.1000, 2.6000, 5.6000],\n         [7.7000, 3.0000, 6.1000],\n         [6.3000, 3.4000, 5.6000],\n         [6.4000, 3.1000, 5.5000],\n         [6.0000, 3.0000, 4.8000],\n         [6.9000, 3.1000, 5.4000],\n         [6.7000, 3.1000, 5.6000],\n         [6.9000, 3.1000, 5.1000],\n         [5.8000, 2.7000, 5.1000],\n         [6.8000, 3.2000, 5.9000],\n         [6.7000, 3.3000, 5.7000],\n         [6.7000, 3.0000, 5.2000],\n         [6.3000, 2.5000, 5.0000],\n         [6.5000, 3.0000, 5.2000],\n         [6.2000, 3.4000, 5.4000],\n         [5.9000, 3.0000, 5.1000]]], dtype=torch.float64)"
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extracted_data[:, :-1]       # Extract the first three columns\n",
    "\n",
    "input_features = torch.from_numpy(features)\n",
    "input_features.view(1, 150, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = load_model(input_features.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2253],\n        [0.1577],\n        [0.1662],\n        [0.2573],\n        [0.2541],\n        [0.4112],\n        [0.2619],\n        [0.2674],\n        [0.1943],\n        [0.2246],\n        [0.2774],\n        [0.3382],\n        [0.1686],\n        [0.0759],\n        [0.1403],\n        [0.3698],\n        [0.2150],\n        [0.2253],\n        [0.3607],\n        [0.3280],\n        [0.3218],\n        [0.3101],\n        [0.1015],\n        [0.3367],\n        [0.4854],\n        [0.2449],\n        [0.3164],\n        [0.2634],\n        [0.1965],\n        [0.3133],\n        [0.2846],\n        [0.2237],\n        [0.3707],\n        [0.3068],\n        [0.2246],\n        [0.0844],\n        [0.1326],\n        [0.2650],\n        [0.1631],\n        [0.2564],\n        [0.1871],\n        [0.0271],\n        [0.1989],\n        [0.3343],\n        [0.5242],\n        [0.1686],\n        [0.3770],\n        [0.2262],\n        [0.2883],\n        [0.2004],\n        [1.5832],\n        [1.5505],\n        [1.6743],\n        [1.2424],\n        [1.5171],\n        [1.5553],\n        [1.6774],\n        [0.9824],\n        [1.5241],\n        [1.2976],\n        [0.9980],\n        [1.4221],\n        [1.1700],\n        [1.6276],\n        [1.1426],\n        [1.4508],\n        [1.6019],\n        [1.3303],\n        [1.3935],\n        [1.2182],\n        [1.7521],\n        [1.2664],\n        [1.6324],\n        [1.6098],\n        [1.3987],\n        [1.4439],\n        [1.5825],\n        [1.7273],\n        [1.5404],\n        [1.0290],\n        [1.1622],\n        [1.1131],\n        [1.2322],\n        [1.7990],\n        [1.6237],\n        [1.6298],\n        [1.5980],\n        [1.3514],\n        [1.4057],\n        [1.2782],\n        [1.4923],\n        [1.5965],\n        [1.2634],\n        [0.9536],\n        [1.4011],\n        [1.4439],\n        [1.4260],\n        [1.4205],\n        [0.8313],\n        [1.3591],\n        [2.0754],\n        [1.8208],\n        [2.0995],\n        [2.0149],\n        [2.0517],\n        [2.1926],\n        [1.5889],\n        [2.1406],\n        [2.0303],\n        [2.1623],\n        [1.8339],\n        [1.8535],\n        [1.9616],\n        [1.7469],\n        [1.8387],\n        [1.9429],\n        [1.9943],\n        [2.2598],\n        [2.1990],\n        [1.6605],\n        [2.0822],\n        [1.7624],\n        [2.1947],\n        [1.6682],\n        [2.0756],\n        [2.1277],\n        [1.6479],\n        [1.7436],\n        [2.0149],\n        [2.0651],\n        [2.1234],\n        [2.2469],\n        [2.0149],\n        [1.7842],\n        [1.9823],\n        [2.1560],\n        [2.0474],\n        [2.0231],\n        [1.7055],\n        [1.9195],\n        [2.0394],\n        [1.7724],\n        [1.8208],\n        [2.0929],\n        [2.0756],\n        [1.8254],\n        [1.6815],\n        [1.8472],\n        [2.0236],\n        [1.8635]], dtype=torch.float64, grad_fn=<AddmmBackward>)"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "P-value for significance:  [0.94026742]\nTTEST:  [-0.074997]\nConclusion: Accept Null Hypothesis\n"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats import weightstats as stets\n",
    "\n",
    "ttest, pval = stats.ttest_ind(target_features, new_column.detach().numpy())\n",
    "print(\"P-value for significance: \", pval)\n",
    "print(\"TTEST: \", ttest)\n",
    "\n",
    "if pval<0.05:\n",
    "    print(\"Conclusion: Reject Null Hypothesis\")\n",
    "else:\n",
    "    print(\"Conclusion: Accept Null Hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1 , 3.5 , 1.4 , 0.22],\n       [4.9 , 3.  , 1.4 , 0.19],\n       [4.7 , 3.2 , 1.3 , 0.18],\n       [4.6 , 3.1 , 1.5 , 0.26],\n       [5.  , 3.6 , 1.4 , 0.24],\n       [5.4 , 3.9 , 1.7 , 0.37],\n       [4.6 , 3.4 , 1.4 , 0.25],\n       [5.  , 3.4 , 1.5 , 0.26],\n       [4.4 , 2.9 , 1.4 , 0.21],\n       [4.9 , 3.1 , 1.5 , 0.24],\n       [5.4 , 3.7 , 1.5 , 0.26],\n       [4.8 , 3.4 , 1.6 , 0.32],\n       [4.8 , 3.  , 1.4 , 0.2 ],\n       [4.3 , 3.  , 1.1 , 0.1 ],\n       [5.8 , 4.  , 1.2 , 0.13],\n       [5.7 , 4.4 , 1.5 , 0.3 ],\n       [5.4 , 3.9 , 1.3 , 0.19],\n       [5.1 , 3.5 , 1.4 , 0.22],\n       [5.7 , 3.8 , 1.7 , 0.34],\n       [5.1 , 3.8 , 1.5 , 0.29],\n       [5.4 , 3.4 , 1.7 , 0.32],\n       [5.1 , 3.7 , 1.5 , 0.28],\n       [4.6 , 3.6 , 1.  , 0.09],\n       [5.1 , 3.3 , 1.7 , 0.33],\n       [4.8 , 3.4 , 1.9 , 0.45],\n       [5.  , 3.  , 1.6 , 0.27],\n       [5.  , 3.4 , 1.6 , 0.31],\n       [5.2 , 3.5 , 1.5 , 0.26],\n       [5.2 , 3.4 , 1.4 , 0.2 ],\n       [4.7 , 3.2 , 1.6 , 0.31],\n       [4.8 , 3.1 , 1.6 , 0.29],\n       [5.4 , 3.4 , 1.5 , 0.23],\n       [5.2 , 4.1 , 1.5 , 0.31],\n       [5.5 , 4.2 , 1.4 , 0.25],\n       [4.9 , 3.1 , 1.5 , 0.24],\n       [5.  , 3.2 , 1.2 , 0.11],\n       [5.5 , 3.5 , 1.3 , 0.15],\n       [4.9 , 3.6 , 1.4 , 0.24],\n       [4.4 , 3.  , 1.3 , 0.18],\n       [5.1 , 3.4 , 1.5 , 0.25],\n       [5.  , 3.5 , 1.3 , 0.18],\n       [4.5 , 2.3 , 1.3 , 0.11],\n       [4.4 , 3.2 , 1.3 , 0.2 ],\n       [5.  , 3.5 , 1.6 , 0.32],\n       [5.1 , 3.8 , 1.9 , 0.47],\n       [4.8 , 3.  , 1.4 , 0.2 ],\n       [5.1 , 3.8 , 1.6 , 0.34],\n       [4.6 , 3.2 , 1.4 , 0.23],\n       [5.3 , 3.7 , 1.5 , 0.27],\n       [5.  , 3.3 , 1.4 , 0.21],\n       [7.  , 3.2 , 4.7 , 1.52],\n       [6.4 , 3.2 , 4.5 , 1.47],\n       [6.9 , 3.1 , 4.9 , 1.61],\n       [5.5 , 2.3 , 4.  , 1.23],\n       [6.5 , 2.8 , 4.6 , 1.48],\n       [5.7 , 2.8 , 4.5 , 1.49],\n       [6.3 , 3.3 , 4.7 , 1.58],\n       [4.9 , 2.4 , 3.3 , 0.98],\n       [6.6 , 2.9 , 4.6 , 1.48],\n       [5.2 , 2.7 , 3.9 , 1.25],\n       [5.  , 2.  , 3.5 , 1.02],\n       [5.9 , 3.  , 4.2 , 1.36],\n       [6.  , 2.2 , 4.  , 1.19],\n       [6.1 , 2.9 , 4.7 , 1.56],\n       [5.6 , 2.9 , 3.6 , 1.1 ],\n       [6.7 , 3.1 , 4.4 , 1.4 ],\n       [5.6 , 3.  , 4.5 , 1.51],\n       [5.8 , 2.7 , 4.1 , 1.29],\n       [6.2 , 2.2 , 4.5 , 1.4 ],\n       [5.6 , 2.5 , 3.9 , 1.2 ],\n       [5.9 , 3.2 , 4.8 , 1.64],\n       [6.1 , 2.8 , 4.  , 1.24],\n       [6.3 , 2.5 , 4.9 , 1.6 ],\n       [6.1 , 2.8 , 4.7 , 1.55],\n       [6.4 , 2.9 , 4.3 , 1.36],\n       [6.6 , 3.  , 4.4 , 1.4 ],\n       [6.8 , 2.8 , 4.8 , 1.54],\n       [6.7 , 3.  , 5.  , 1.66],\n       [6.  , 2.9 , 4.5 , 1.48],\n       [5.7 , 2.6 , 3.5 , 1.03],\n       [5.5 , 2.4 , 3.8 , 1.16],\n       [5.5 , 2.4 , 3.7 , 1.11],\n       [5.8 , 2.7 , 3.9 , 1.21],\n       [6.  , 2.7 , 5.1 , 1.72],\n       [5.4 , 3.  , 4.5 , 1.53],\n       [6.  , 3.4 , 4.5 , 1.52],\n       [6.7 , 3.1 , 4.7 , 1.53],\n       [6.3 , 2.3 , 4.4 , 1.36],\n       [5.6 , 3.  , 4.1 , 1.34],\n       [5.5 , 2.5 , 4.  , 1.25],\n       [5.5 , 2.6 , 4.4 , 1.44],\n       [6.1 , 3.  , 4.6 , 1.52],\n       [5.8 , 2.6 , 4.  , 1.24],\n       [5.  , 2.3 , 3.3 , 0.96],\n       [5.6 , 2.7 , 4.2 , 1.35],\n       [5.7 , 3.  , 4.2 , 1.37],\n       [5.7 , 2.9 , 4.2 , 1.36],\n       [6.2 , 2.9 , 4.3 , 1.37],\n       [5.1 , 2.5 , 3.  , 0.84],\n       [5.7 , 2.8 , 4.1 , 1.31],\n       [6.3 , 3.3 , 6.  , 2.16],\n       [5.8 , 2.7 , 5.1 , 1.74],\n       [7.1 , 3.  , 5.9 , 2.03],\n       [6.3 , 2.9 , 5.6 , 1.94],\n       [6.5 , 3.  , 5.8 , 2.03],\n       [7.6 , 3.  , 6.6 , 2.3 ],\n       [4.9 , 2.5 , 4.5 , 1.52],\n       [7.3 , 2.9 , 6.3 , 2.18],\n       [6.7 , 2.5 , 5.8 , 1.97],\n       [7.2 , 3.6 , 6.1 , 2.16],\n       [6.5 , 3.2 , 5.1 , 1.73],\n       [6.4 , 2.7 , 5.3 , 1.78],\n       [6.8 , 3.  , 5.5 , 1.87],\n       [5.7 , 2.5 , 5.  , 1.68],\n       [5.8 , 2.8 , 5.1 , 1.75],\n       [6.4 , 3.2 , 5.3 , 1.83],\n       [6.5 , 3.  , 5.5 , 1.89],\n       [7.7 , 3.8 , 6.7 , 2.41],\n       [7.7 , 2.6 , 6.9 , 2.39],\n       [6.  , 2.2 , 5.  , 1.63],\n       [6.9 , 3.2 , 5.7 , 1.97],\n       [5.6 , 2.8 , 4.9 , 1.67],\n       [7.7 , 2.8 , 6.7 , 2.32],\n       [6.3 , 2.7 , 4.9 , 1.61],\n       [6.7 , 3.3 , 5.7 , 1.99],\n       [7.2 , 3.2 , 6.  , 2.08],\n       [6.2 , 2.8 , 4.8 , 1.59],\n       [6.1 , 3.  , 4.9 , 1.65],\n       [6.4 , 2.8 , 5.6 , 1.93],\n       [7.2 , 3.  , 5.8 , 1.98],\n       [7.4 , 2.8 , 6.1 , 2.08],\n       [7.9 , 3.8 , 6.4 , 2.27],\n       [6.4 , 2.8 , 5.6 , 1.93],\n       [6.3 , 2.8 , 5.1 , 1.71],\n       [6.1 , 2.6 , 5.6 , 1.93],\n       [7.7 , 3.  , 6.1 , 2.07],\n       [6.3 , 3.4 , 5.6 , 1.99],\n       [6.4 , 3.1 , 5.5 , 1.91],\n       [6.  , 3.  , 4.8 , 1.62],\n       [6.9 , 3.1 , 5.4 , 1.83],\n       [6.7 , 3.1 , 5.6 , 1.93],\n       [6.9 , 3.1 , 5.1 , 1.7 ],\n       [5.8 , 2.7 , 5.1 , 1.74],\n       [6.8 , 3.2 , 5.9 , 2.07],\n       [6.7 , 3.3 , 5.7 , 1.99],\n       [6.7 , 3.  , 5.2 , 1.75],\n       [6.3 , 2.5 , 5.  , 1.64],\n       [6.5 , 3.  , 5.2 , 1.76],\n       [6.2 , 3.4 , 5.4 , 1.91],\n       [5.9 , 3.  , 5.1 , 1.76]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = np.concatenate((features, new_column.detach().numpy()), axis=1)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('first_iris.csv', new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}