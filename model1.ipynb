{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondad424485431ff4a8aa076a99cfa3fb48c",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "600"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = iris['data']\n",
    "dataset.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.asarray(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = dataset[:, :-1]\n",
    "target_features = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitdata = np.split(training_features, [120])\n",
    "train_features = splitdata[0]\n",
    "test_features = splitdata[1]\n",
    "\n",
    "splitdata = np.split(target_features, [120])\n",
    "train_target = splitdata[0]\n",
    "test_target = splitdata[1]\n",
    "\n",
    "train_missing = train_target.copy()\n",
    "test_missing = test_target.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_COUNT = train_missing.size*0.25\n",
    "NAN = 0\n",
    "\n",
    "train_missing.flat[np.random.choice(train_missing.size, int(REPLACE_COUNT), replace=False)] =  NAN\n",
    "\n",
    "# Using 0.2 because the test set is a 0.2 subset of the whole dataset\n",
    "test_missing.flat[np.random.choice(test_missing.size, int(REPLACE_COUNT*0.2), replace=False)] = NAN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(120, 3)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 120, 3])\n"
    }
   ],
   "source": [
    "x_train = torch.from_numpy(train_features)\n",
    "x_train = x_train.view(1, 120, 3)\n",
    "\n",
    "x_test = torch.from_numpy(test_features)\n",
    "x_test = x_test.view(1, 30, 3)\n",
    "\n",
    "y_train = torch.from_numpy(train_target)\n",
    "y_train = y_train.view(1, 120, 1)\n",
    "\n",
    "y_test = torch.from_numpy(test_target)\n",
    "y_test = y_test.view(1, 30, 1)\n",
    "\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_features=3, out_features=2)\n",
    "        self.enc2 = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "        self.dec1 = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.enc1(x))\n",
    "        x = F.leaky_relu(self.enc2(x))\n",
    "        x = self.dec1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Autoencoder(\n  (enc1): Linear(in_features=3, out_features=2, bias=True)\n  (enc2): Linear(in_features=2, out_features=1, bias=True)\n  (dec1): Linear(in_features=1, out_features=1, bias=True)\n)\n"
    }
   ],
   "source": [
    "net = Autoencoder().double()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1\n"
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    train_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for data, target in zip(x_train, y_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if epoch == NUM_EPOCHS-1:\n",
    "                for i in range(len(data)):\n",
    "                    print(\"Input: \", data[i])\n",
    "                    print(\"Target: \", target[i])\n",
    "                    print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_train)\n",
    "        train_loss.append(loss)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}'\n",
    "            .format(epoch+1, NUM_EPOCHS, loss))\n",
    "    return train_loss\n",
    "\n",
    "def test(net):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        running_loss = 0.0\n",
    "        for data, target in zip(x_test, y_test):\n",
    "            outputs = net(data.double())\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "            for i in range(len(data)):\n",
    "                print(\"Input: \", data[i])\n",
    "                print(\"Target: \", target[i])\n",
    "                print(\"Outputs: \", outputs[i])\n",
    "        loss = running_loss / len(x_test)\n",
    "        test_loss.append(loss)\n",
    "        print('Test Loss: {:.3f}'.format(loss))\n",
    "\n",
    "        return test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ".2608], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.4000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3201], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.1952], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.3000, 3.0000, 1.1000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.0974], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 4.0000, 1.2000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1270], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 4.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.3031], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.9000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.1903], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.5000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.2194], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 3.8000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.3374], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.2909], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.4000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3223], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.2818], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.6000, 1.0000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.0863], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.3000, 1.7000], dtype=torch.float64)\nTarget:  tensor([0.5000], dtype=torch.float64)\nOutputs:  tensor([0.3343], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.4000, 1.9000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.4530], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.0000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2698], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.4000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.3060], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 3.5000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2567], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 3.4000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2034], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.7000, 3.2000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3090], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.1000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2929], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.2337], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 4.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.3110], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 4.2000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2547], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 3.1000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2416], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.2000, 1.2000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1107], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 3.5000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1471], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 3.6000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.1000], dtype=torch.float64)\nOutputs:  tensor([0.2425], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.4000, 3.0000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1790], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.4000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2547], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.5000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.1821], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.5000, 2.3000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.1087], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.4000, 3.2000, 1.3000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.1971], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.5000, 1.6000], dtype=torch.float64)\nTarget:  tensor([0.6000], dtype=torch.float64)\nOutputs:  tensor([0.3151], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 1.9000], dtype=torch.float64)\nTarget:  tensor([0.4000], dtype=torch.float64)\nOutputs:  tensor([0.4681], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.8000, 3.0000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.3000], dtype=torch.float64)\nOutputs:  tensor([0.1952], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 3.8000, 1.6000], dtype=torch.float64)\nTarget:tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.3352], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.6000, 3.2000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2274], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.3000, 3.7000, 1.5000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2678], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 3.3000, 1.4000], dtype=torch.float64)\nTarget:  tensor([0.2000], dtype=torch.float64)\nOutputs:  tensor([0.2084], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.0000, 3.2000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5213], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 3.2000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.4747], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.9000, 3.1000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6079], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.3000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2349], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 2.8000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.4759], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.8000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.4877], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 3.3000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.5794], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 2.4000, 3.3000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([0.9759], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.6000, 2.9000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.4779], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.2000, 2.7000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.2478], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 2.0000, 3.5000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.0213], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.9000, 3.0000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.3588], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.2000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.1908], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.9000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5573], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.9000, 3.6000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.1049], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.1000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.4003], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 3.0000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5128], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.2944], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.2000, 2.2000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.3983], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.5000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([1.2017], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.9000, 3.2000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6427], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2380], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.5000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5957], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 2.8000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.5482], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.9000, 4.3000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3590], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.6000, 3.0000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.3983], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.8000, 2.8000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5434], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.0000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.7000], dtype=torch.float64)\nOutputs:  tensor([1.6572], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.9000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.4757], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.6000, 3.5000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.0265], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.4000, 3.8000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([1.1553], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.4000, 3.7000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([1.1110], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 3.9000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.2057], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.7234], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.4000, 3.0000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5268], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 3.4000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.5209], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 3.1000, 4.7000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.5333], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.3000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3560], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 3.0000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3355], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.5000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.2530], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.5000, 2.6000, 4.4000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.4393], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.1000, 3.0000, 4.6000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.5220], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.6000, 4.0000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.2410], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.0000, 2.3000, 3.3000], dtype=torch.float64)\nTarget:  tensor([1.], dtype=torch.float64)\nOutputs:  tensor([0.9598], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.6000, 2.7000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3527], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 3.0000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.2000], dtype=torch.float64)\nOutputs:  tensor([1.3728], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.9000, 4.2000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3638], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.2000, 2.9000, 4.3000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3730], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.1000, 2.5000, 3.0000], dtype=torch.float64)\nTarget:  tensor([1.1000], dtype=torch.float64)\nOutputs:  tensor([0.8379], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.8000, 4.1000], dtype=torch.float64)\nTarget:  tensor([1.3000], dtype=torch.float64)\nOutputs:  tensor([1.3104], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 3.3000, 6.0000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([2.1555], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.7375], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.1000, 3.0000, 5.9000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.0279], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.3000, 2.9000, 5.6000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.9420], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 5.8000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([2.0257], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.6000, 3.0000, 6.6000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([2.3030], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([4.9000, 2.5000, 4.5000], dtype=torch.float64)\nTarget:  tensor([1.7000], dtype=torch.float64)\nOutputs:  tensor([1.5166], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.3000, 2.9000, 6.3000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.1821], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.7000, 2.5000, 5.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.9664], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.2000, 3.6000, 6.1000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([2.1638], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.2000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.7336], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 2.7000, 5.3000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.7840], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.8000, 3.0000, 5.5000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.8717], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.7000, 2.5000, 5.0000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.6821], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([5.8000, 2.8000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.7465], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.4000, 3.2000, 5.3000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.8292], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.5000, 3.0000, 5.5000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.8927], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:tensor([7.7000, 3.8000, 6.7000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([2.4127], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([7.7000, 2.6000, 6.9000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.3927], dtype=torch.float64, grad_fn=<SelectBackward>)\nInput:  tensor([6.0000, 2.2000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.6339], dtype=torch.float64, grad_fn=<SelectBackward>)\nEpoch 3000 of 3000, Train Loss: 0.024\n"
    }
   ],
   "source": [
    "train_loss1 = train(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'Loss')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"md6d3cad030\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.743619\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 500 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(100.199869 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.487807\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(147.762807 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.231994\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 1500 -->\r\n      <g transform=\"translate(198.506994 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.976182\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2000 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(249.251182 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"312.720369\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2500 -->\r\n      <g transform=\"translate(299.995369 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.464557\" xlink:href=\"#md6d3cad030\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3000 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(350.739557 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- Epochs -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(193.265625 268.034687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m9c5a5bf4cd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"231.217701\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 235.01692)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"203.216541\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(20.878125 207.01576)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"175.215381\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 179.0146)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"147.214221\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(20.878125 151.01344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"119.213061\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(20.878125 123.012279)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"91.211901\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(20.878125 95.011119)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"63.21074\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 3.0 -->\r\n      <g transform=\"translate(20.878125 67.009959)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m9c5a5bf4cd\" y=\"35.20958\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(20.878125 39.008799)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Loss -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-76\"/>\r\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pe6c0087b5e)\" d=\"M 58.999432 32.201761 \r\nL 59.811339 34.886728 \r\nL 61.130688 40.062541 \r\nL 61.739618 43.207056 \r\nL 62.24706 47.044524 \r\nL 62.957478 54.749409 \r\nL 67.930409 113.510387 \r\nL 71.58399 155.779724 \r\nL 72.903339 167.268046 \r\nL 74.73013 181.751652 \r\nL 76.049479 190.59277 \r\nL 77.165851 196.655546 \r\nL 78.180735 200.942486 \r\nL 79.09413 203.833385 \r\nL 79.906037 205.715488 \r\nL 80.717944 207.058092 \r\nL 81.529851 207.980697 \r\nL 82.443246 208.662868 \r\nL 83.559619 209.18319 \r\nL 85.183433 209.658973 \r\nL 90.562316 210.863785 \r\nL 111.062968 215.51064 \r\nL 124.763899 218.849756 \r\nL 154.905946 226.334189 \r\nL 160.995249 227.496802 \r\nL 166.475621 228.319101 \r\nL 171.955993 228.91497 \r\nL 177.842319 229.326781 \r\nL 184.946505 229.589565 \r\nL 195.298319 229.725684 \r\nL 222.09125 229.773231 \r\nL 363.363068 229.874489 \r\nL 363.363068 229.874489 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_18\">\r\n    <!-- Train Loss -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n     <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n    </defs>\r\n    <g transform=\"translate(181.72125 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"271.675781\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"325.638672\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"386.820312\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"438.919922\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pe6c0087b5e\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss1)\n",
    "plt.title('Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input:  tensor([6.9000, 3.2000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.9714], dtype=torch.float64)\nInput:  tensor([5.6000, 2.8000, 4.9000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.6719], dtype=torch.float64)\nInput:  tensor([7.7000, 2.8000, 6.7000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([2.3222], dtype=torch.float64)\nInput:  tensor([6.3000, 2.7000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6138], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9945], dtype=torch.float64)\nInput:  tensor([7.2000, 3.2000, 6.0000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.0833], dtype=torch.float64)\nInput:  tensor([6.2000, 2.8000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.5855], dtype=torch.float64)\nInput:  tensor([6.1000, 3.0000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6550], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9260], dtype=torch.float64)\nInput:  tensor([7.2000, 3.0000, 5.8000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.9766], dtype=torch.float64)\nInput:  tensor([7.4000, 2.8000, 6.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([2.0774], dtype=torch.float64)\nInput:  tensor([7.9000, 3.8000, 6.4000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([2.2657], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([1.9260], dtype=torch.float64)\nInput:  tensor([6.3000, 2.8000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.7114], dtype=torch.float64)\nInput:  tensor([6.1000, 2.6000, 5.6000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.9289], dtype=torch.float64)\nInput:  tensor([7.7000, 3.0000, 6.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0744], dtype=torch.float64)\nInput:  tensor([6.3000, 3.4000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.9873], dtype=torch.float64)\nInput:  tensor([6.4000, 3.1000, 5.5000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.9088], dtype=torch.float64)\nInput:  tensor([6.0000, 3.0000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6177], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.8294], dtype=torch.float64)\nInput:  tensor([6.7000, 3.1000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.9321], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.6965], dtype=torch.float64)\nInput:  tensor([5.8000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.7375], dtype=torch.float64)\nInput:  tensor([6.8000, 3.2000, 5.9000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0671], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([1.9945], dtype=torch.float64)\nInput:  tensor([6.7000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.7458], dtype=torch.float64)\nInput:  tensor([6.3000, 2.5000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.6400], dtype=torch.float64)\nInput:  tensor([6.5000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.7598], dtype=torch.float64)\nInput:  tensor([6.2000, 3.4000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.9057], dtype=torch.float64)\nInput:  tensor([5.9000, 3.0000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.7576], dtype=torch.float64)\nTest Loss: 0.101\n"
    }
   ],
   "source": [
    "test_result = test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-71260b2beae7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./iris_autoencoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(net, './iris_autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Autoencoder(\n  (enc1): Linear(in_features=3, out_features=2, bias=True)\n  (enc2): Linear(in_features=2, out_features=1, bias=True)\n  (dec1): Linear(in_features=1, out_features=1, bias=True)\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = torch.load('iris_autoencoder')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input:  tensor([6.9000, 3.2000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.9714], dtype=torch.float64)\nInput:  tensor([5.6000, 2.8000, 4.9000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.6719], dtype=torch.float64)\nInput:  tensor([7.7000, 2.8000, 6.7000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([2.3222], dtype=torch.float64)\nInput:  tensor([6.3000, 2.7000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6138], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9945], dtype=torch.float64)\nInput:  tensor([7.2000, 3.2000, 6.0000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([2.0833], dtype=torch.float64)\nInput:  tensor([6.2000, 2.8000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.5855], dtype=torch.float64)\nInput:  tensor([6.1000, 3.0000, 4.9000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6550], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.9260], dtype=torch.float64)\nInput:  tensor([7.2000, 3.0000, 5.8000], dtype=torch.float64)\nTarget:  tensor([1.6000], dtype=torch.float64)\nOutputs:  tensor([1.9766], dtype=torch.float64)\nInput:  tensor([7.4000, 2.8000, 6.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([2.0774], dtype=torch.float64)\nInput:  tensor([7.9000, 3.8000, 6.4000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:tensor([2.2657], dtype=torch.float64)\nInput:  tensor([6.4000, 2.8000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.2000], dtype=torch.float64)\nOutputs:  tensor([1.9260], dtype=torch.float64)\nInput:  tensor([6.3000, 2.8000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.5000], dtype=torch.float64)\nOutputs:  tensor([1.7114], dtype=torch.float64)\nInput:  tensor([6.1000, 2.6000, 5.6000], dtype=torch.float64)\nTarget:  tensor([1.4000], dtype=torch.float64)\nOutputs:  tensor([1.9289], dtype=torch.float64)\nInput:  tensor([7.7000, 3.0000, 6.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0744], dtype=torch.float64)\nInput:  tensor([6.3000, 3.4000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.9873], dtype=torch.float64)\nInput:  tensor([6.4000, 3.1000, 5.5000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.9088], dtype=torch.float64)\nInput:  tensor([6.0000, 3.0000, 4.8000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.6177], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.1000], dtype=torch.float64)\nOutputs:  tensor([1.8294], dtype=torch.float64)\nInput:  tensor([6.7000, 3.1000, 5.6000], dtype=torch.float64)\nTarget:  tensor([2.4000], dtype=torch.float64)\nOutputs:  tensor([1.9321], dtype=torch.float64)\nInput:  tensor([6.9000, 3.1000, 5.1000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.6965], dtype=torch.float64)\nInput:  tensor([5.8000, 2.7000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.7375], dtype=torch.float64)\nInput:  tensor([6.8000, 3.2000, 5.9000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([2.0671], dtype=torch.float64)\nInput:  tensor([6.7000, 3.3000, 5.7000], dtype=torch.float64)\nTarget:  tensor([2.5000], dtype=torch.float64)\nOutputs:  tensor([1.9945], dtype=torch.float64)\nInput:  tensor([6.7000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.7458], dtype=torch.float64)\nInput:  tensor([6.3000, 2.5000, 5.0000], dtype=torch.float64)\nTarget:  tensor([1.9000], dtype=torch.float64)\nOutputs:  tensor([1.6400], dtype=torch.float64)\nInput:  tensor([6.5000, 3.0000, 5.2000], dtype=torch.float64)\nTarget:  tensor([2.], dtype=torch.float64)\nOutputs:  tensor([1.7598], dtype=torch.float64)\nInput:  tensor([6.2000, 3.4000, 5.4000], dtype=torch.float64)\nTarget:  tensor([2.3000], dtype=torch.float64)\nOutputs:  tensor([1.9057], dtype=torch.float64)\nInput:  tensor([5.9000, 3.0000, 5.1000], dtype=torch.float64)\nTarget:  tensor([1.8000], dtype=torch.float64)\nOutputs:  tensor([1.7576], dtype=torch.float64)\nTest Loss: 0.101\n"
    }
   ],
   "source": [
    "test_result = test(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'data': array([[5.1, 3.5, 1.4, 0.2],\n        [4.9, 3. , 1.4, 0.2],\n        [4.7, 3.2, 1.3, 0.2],\n        [4.6, 3.1, 1.5, 0.2],\n        [5. , 3.6, 1.4, 0.2],\n        [5.4, 3.9, 1.7, 0.4],\n        [4.6, 3.4, 1.4, 0.3],\n        [5. , 3.4, 1.5, 0.2],\n        [4.4, 2.9, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.1],\n        [5.4, 3.7, 1.5, 0.2],\n        [4.8, 3.4, 1.6, 0.2],\n        [4.8, 3. , 1.4, 0.1],\n        [4.3, 3. , 1.1, 0.1],\n        [5.8, 4. , 1.2, 0.2],\n        [5.7, 4.4, 1.5, 0.4],\n        [5.4, 3.9, 1.3, 0.4],\n        [5.1, 3.5, 1.4, 0.3],\n        [5.7, 3.8, 1.7, 0.3],\n        [5.1, 3.8, 1.5, 0.3],\n        [5.4, 3.4, 1.7, 0.2],\n        [5.1, 3.7, 1.5, 0.4],\n        [4.6, 3.6, 1. , 0.2],\n        [5.1, 3.3, 1.7, 0.5],\n        [4.8, 3.4, 1.9, 0.2],\n        [5. , 3. , 1.6, 0.2],\n        [5. , 3.4, 1.6, 0.4],\n        [5.2, 3.5, 1.5, 0.2],\n        [5.2, 3.4, 1.4, 0.2],\n        [4.7, 3.2, 1.6, 0.2],\n        [4.8, 3.1, 1.6, 0.2],\n        [5.4, 3.4, 1.5, 0.4],\n        [5.2, 4.1, 1.5, 0.1],\n        [5.5, 4.2, 1.4, 0.2],\n        [4.9, 3.1, 1.5, 0.2],\n        [5. , 3.2, 1.2, 0.2],\n        [5.5, 3.5, 1.3, 0.2],\n        [4.9, 3.6, 1.4, 0.1],\n        [4.4, 3. , 1.3, 0.2],\n        [5.1, 3.4, 1.5, 0.2],\n        [5. , 3.5, 1.3, 0.3],\n        [4.5, 2.3, 1.3, 0.3],\n        [4.4, 3.2, 1.3, 0.2],\n        [5. , 3.5, 1.6, 0.6],\n        [5.1, 3.8, 1.9, 0.4],\n        [4.8, 3. , 1.4, 0.3],\n        [5.1, 3.8, 1.6, 0.2],\n        [4.6, 3.2, 1.4, 0.2],\n        [5.3, 3.7, 1.5, 0.2],\n        [5. , 3.3, 1.4, 0.2],\n        [7. , 3.2, 4.7, 1.4],\n        [6.4, 3.2, 4.5, 1.5],\n        [6.9, 3.1, 4.9, 1.5],\n        [5.5, 2.3, 4. , 1.3],\n        [6.5, 2.8, 4.6, 1.5],\n        [5.7, 2.8, 4.5, 1.3],\n        [6.3, 3.3, 4.7, 1.6],\n        [4.9, 2.4, 3.3, 1. ],\n        [6.6, 2.9, 4.6, 1.3],\n        [5.2, 2.7, 3.9, 1.4],\n        [5. , 2. , 3.5, 1. ],\n        [5.9, 3. , 4.2, 1.5],\n        [6. , 2.2, 4. , 1. ],\n        [6.1, 2.9, 4.7, 1.4],\n        [5.6, 2.9, 3.6, 1.3],\n        [6.7, 3.1, 4.4, 1.4],\n        [5.6, 3. , 4.5, 1.5],\n        [5.8, 2.7, 4.1, 1. ],\n        [6.2, 2.2, 4.5, 1.5],\n        [5.6, 2.5, 3.9, 1.1],\n        [5.9, 3.2, 4.8, 1.8],\n        [6.1, 2.8, 4. , 1.3],\n        [6.3, 2.5, 4.9, 1.5],\n        [6.1, 2.8, 4.7, 1.2],\n        [6.4, 2.9, 4.3, 1.3],\n        [6.6, 3. , 4.4, 1.4],\n        [6.8, 2.8, 4.8, 1.4],\n        [6.7, 3. , 5. , 1.7],\n        [6. , 2.9, 4.5, 1.5],\n        [5.7, 2.6, 3.5, 1. ],\n        [5.5, 2.4, 3.8, 1.1],\n        [5.5, 2.4, 3.7, 1. ],\n        [5.8, 2.7, 3.9, 1.2],\n        [6. , 2.7, 5.1, 1.6],\n        [5.4, 3. , 4.5, 1.5],\n        [6. , 3.4, 4.5, 1.6],\n        [6.7, 3.1, 4.7, 1.5],\n        [6.3, 2.3, 4.4, 1.3],\n        [5.6, 3. , 4.1, 1.3],\n        [5.5, 2.5, 4. , 1.3],\n        [5.5, 2.6, 4.4, 1.2],\n        [6.1, 3. , 4.6, 1.4],\n        [5.8, 2.6, 4. , 1.2],\n        [5. , 2.3, 3.3, 1. ],\n        [5.6, 2.7, 4.2, 1.3],\n        [5.7, 3. , 4.2, 1.2],\n        [5.7, 2.9, 4.2, 1.3],\n        [6.2, 2.9, 4.3, 1.3],\n        [5.1, 2.5, 3. , 1.1],\n        [5.7, 2.8, 4.1, 1.3],\n        [6.3, 3.3, 6. , 2.5],\n        [5.8, 2.7, 5.1, 1.9],\n        [7.1, 3. , 5.9, 2.1],\n        [6.3, 2.9, 5.6, 1.8],\n        [6.5, 3. , 5.8, 2.2],\n        [7.6, 3. , 6.6, 2.1],\n        [4.9, 2.5, 4.5, 1.7],\n        [7.3, 2.9, 6.3, 1.8],\n        [6.7, 2.5, 5.8, 1.8],\n        [7.2, 3.6, 6.1, 2.5],\n        [6.5, 3.2, 5.1, 2. ],\n        [6.4, 2.7, 5.3, 1.9],\n        [6.8, 3. , 5.5, 2.1],\n        [5.7, 2.5, 5. , 2. ],\n        [5.8, 2.8, 5.1, 2.4],\n        [6.4, 3.2, 5.3, 2.3],\n        [6.5, 3. , 5.5, 1.8],\n        [7.7, 3.8, 6.7, 2.2],\n        [7.7, 2.6, 6.9, 2.3],\n        [6. , 2.2, 5. , 1.5],\n        [6.9, 3.2, 5.7, 2.3],\n        [5.6, 2.8, 4.9, 2. ],\n        [7.7, 2.8, 6.7, 2. ],\n        [6.3, 2.7, 4.9, 1.8],\n        [6.7, 3.3, 5.7, 2.1],\n        [7.2, 3.2, 6. , 1.8],\n        [6.2, 2.8, 4.8, 1.8],\n        [6.1, 3. , 4.9, 1.8],\n        [6.4, 2.8, 5.6, 2.1],\n        [7.2, 3. , 5.8, 1.6],\n        [7.4, 2.8, 6.1, 1.9],\n        [7.9, 3.8, 6.4, 2. ],\n        [6.4, 2.8, 5.6, 2.2],\n        [6.3, 2.8, 5.1, 1.5],\n        [6.1, 2.6, 5.6, 1.4],\n        [7.7, 3. , 6.1, 2.3],\n        [6.3, 3.4, 5.6, 2.4],\n        [6.4, 3.1, 5.5, 1.8],\n        [6. , 3. , 4.8, 1.8],\n        [6.9, 3.1, 5.4, 2.1],\n        [6.7, 3.1, 5.6, 2.4],\n        [6.9, 3.1, 5.1, 2.3],\n        [5.8, 2.7, 5.1, 1.9],\n        [6.8, 3.2, 5.9, 2.3],\n        [6.7, 3.3, 5.7, 2.5],\n        [6.7, 3. , 5.2, 2.3],\n        [6.3, 2.5, 5. , 1.9],\n        [6.5, 3. , 5.2, 2. ],\n        [6.2, 3.4, 5.4, 2.3],\n        [5.9, 3. , 5.1, 1.8]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n 'feature_names': ['sepal length (cm)',\n  'sepal width (cm)',\n  'petal length (cm)',\n  'petal width (cm)'],\n 'filename': 'C:\\\\Users\\\\Firel\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(150, 4)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data = iris['data']\n",
    "extracted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[5.1000, 3.5000, 1.4000],\n         [4.9000, 3.0000, 1.4000],\n         [4.7000, 3.2000, 1.3000],\n         [4.6000, 3.1000, 1.5000],\n         [5.0000, 3.6000, 1.4000],\n         [5.4000, 3.9000, 1.7000],\n         [4.6000, 3.4000, 1.4000],\n         [5.0000, 3.4000, 1.5000],\n         [4.4000, 2.9000, 1.4000],\n         [4.9000, 3.1000, 1.5000],\n         [5.4000, 3.7000, 1.5000],\n         [4.8000, 3.4000, 1.6000],\n         [4.8000, 3.0000, 1.4000],\n         [4.3000, 3.0000, 1.1000],\n         [5.8000, 4.0000, 1.2000],\n         [5.7000, 4.4000, 1.5000],\n         [5.4000, 3.9000, 1.3000],\n         [5.1000, 3.5000, 1.4000],\n         [5.7000, 3.8000, 1.7000],\n         [5.1000, 3.8000, 1.5000],\n         [5.4000, 3.4000, 1.7000],\n         [5.1000, 3.7000, 1.5000],\n         [4.6000, 3.6000, 1.0000],\n         [5.1000, 3.3000, 1.7000],\n         [4.8000, 3.4000, 1.9000],\n         [5.0000, 3.0000, 1.6000],\n         [5.0000, 3.4000, 1.6000],\n         [5.2000, 3.5000, 1.5000],\n         [5.2000, 3.4000, 1.4000],\n         [4.7000, 3.2000, 1.6000],\n         [4.8000, 3.1000, 1.6000],\n         [5.4000, 3.4000, 1.5000],\n         [5.2000, 4.1000, 1.5000],\n         [5.5000, 4.2000, 1.4000],\n         [4.9000, 3.1000, 1.5000],\n         [5.0000, 3.2000, 1.2000],\n         [5.5000, 3.5000, 1.3000],\n         [4.9000, 3.6000, 1.4000],\n         [4.4000, 3.0000, 1.3000],\n         [5.1000, 3.4000, 1.5000],\n         [5.0000, 3.5000, 1.3000],\n         [4.5000, 2.3000, 1.3000],\n         [4.4000, 3.2000, 1.3000],\n         [5.0000, 3.5000, 1.6000],\n         [5.1000, 3.8000, 1.9000],\n         [4.8000, 3.0000, 1.4000],\n         [5.1000, 3.8000, 1.6000],\n         [4.6000, 3.2000, 1.4000],\n         [5.3000, 3.7000, 1.5000],\n         [5.0000, 3.3000, 1.4000],\n         [7.0000, 3.2000, 4.7000],\n         [6.4000, 3.2000, 4.5000],\n         [6.9000, 3.1000, 4.9000],\n         [5.5000, 2.3000, 4.0000],\n         [6.5000, 2.8000, 4.6000],\n         [5.7000, 2.8000, 4.5000],\n         [6.3000, 3.3000, 4.7000],\n         [4.9000, 2.4000, 3.3000],\n         [6.6000, 2.9000, 4.6000],\n         [5.2000, 2.7000, 3.9000],\n         [5.0000, 2.0000, 3.5000],\n         [5.9000, 3.0000, 4.2000],\n         [6.0000, 2.2000, 4.0000],\n         [6.1000, 2.9000, 4.7000],\n         [5.6000, 2.9000, 3.6000],\n         [6.7000, 3.1000, 4.4000],\n         [5.6000, 3.0000, 4.5000],\n         [5.8000, 2.7000, 4.1000],\n         [6.2000, 2.2000, 4.5000],\n         [5.6000, 2.5000, 3.9000],\n         [5.9000, 3.2000, 4.8000],\n         [6.1000, 2.8000, 4.0000],\n         [6.3000, 2.5000, 4.9000],\n         [6.1000, 2.8000, 4.7000],\n         [6.4000, 2.9000, 4.3000],\n         [6.6000, 3.0000, 4.4000],\n         [6.8000, 2.8000, 4.8000],\n         [6.7000, 3.0000, 5.0000],\n         [6.0000, 2.9000, 4.5000],\n         [5.7000, 2.6000, 3.5000],\n         [5.5000, 2.4000, 3.8000],\n         [5.5000, 2.4000, 3.7000],\n         [5.8000, 2.7000, 3.9000],\n         [6.0000, 2.7000, 5.1000],\n         [5.4000, 3.0000, 4.5000],\n         [6.0000, 3.4000, 4.5000],\n         [6.7000, 3.1000, 4.7000],\n         [6.3000, 2.3000, 4.4000],\n         [5.6000, 3.0000, 4.1000],\n         [5.5000, 2.5000, 4.0000],\n         [5.5000, 2.6000, 4.4000],\n         [6.1000, 3.0000, 4.6000],\n         [5.8000, 2.6000, 4.0000],\n         [5.0000, 2.3000, 3.3000],\n         [5.6000, 2.7000, 4.2000],\n         [5.7000, 3.0000, 4.2000],\n         [5.7000, 2.9000, 4.2000],\n         [6.2000, 2.9000, 4.3000],\n         [5.1000, 2.5000, 3.0000],\n         [5.7000, 2.8000, 4.1000],\n         [6.3000, 3.3000, 6.0000],\n         [5.8000, 2.7000, 5.1000],\n         [7.1000, 3.0000, 5.9000],\n         [6.3000, 2.9000, 5.6000],\n         [6.5000, 3.0000, 5.8000],\n         [7.6000, 3.0000, 6.6000],\n         [4.9000, 2.5000, 4.5000],\n         [7.3000, 2.9000, 6.3000],\n         [6.7000, 2.5000, 5.8000],\n         [7.2000, 3.6000, 6.1000],\n         [6.5000, 3.2000, 5.1000],\n         [6.4000, 2.7000, 5.3000],\n         [6.8000, 3.0000, 5.5000],\n         [5.7000, 2.5000, 5.0000],\n         [5.8000, 2.8000, 5.1000],\n         [6.4000, 3.2000, 5.3000],\n         [6.5000, 3.0000, 5.5000],\n         [7.7000, 3.8000, 6.7000],\n         [7.7000, 2.6000, 6.9000],\n         [6.0000, 2.2000, 5.0000],\n         [6.9000, 3.2000, 5.7000],\n         [5.6000, 2.8000, 4.9000],\n         [7.7000, 2.8000, 6.7000],\n         [6.3000, 2.7000, 4.9000],\n         [6.7000, 3.3000, 5.7000],\n         [7.2000, 3.2000, 6.0000],\n         [6.2000, 2.8000, 4.8000],\n         [6.1000, 3.0000, 4.9000],\n         [6.4000, 2.8000, 5.6000],\n         [7.2000, 3.0000, 5.8000],\n         [7.4000, 2.8000, 6.1000],\n         [7.9000, 3.8000, 6.4000],\n         [6.4000, 2.8000, 5.6000],\n         [6.3000, 2.8000, 5.1000],\n         [6.1000, 2.6000, 5.6000],\n         [7.7000, 3.0000, 6.1000],\n         [6.3000, 3.4000, 5.6000],\n         [6.4000, 3.1000, 5.5000],\n         [6.0000, 3.0000, 4.8000],\n         [6.9000, 3.1000, 5.4000],\n         [6.7000, 3.1000, 5.6000],\n         [6.9000, 3.1000, 5.1000],\n         [5.8000, 2.7000, 5.1000],\n         [6.8000, 3.2000, 5.9000],\n         [6.7000, 3.3000, 5.7000],\n         [6.7000, 3.0000, 5.2000],\n         [6.3000, 2.5000, 5.0000],\n         [6.5000, 3.0000, 5.2000],\n         [6.2000, 3.4000, 5.4000],\n         [5.9000, 3.0000, 5.1000]]], dtype=torch.float64)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extracted_data[:, :-1]       # Extract the first three columns\n",
    "\n",
    "input_features = torch.from_numpy(features)\n",
    "input_features.view(1, 150, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = load_model(input_features.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.2194],\n        [0.1882],\n        [0.1760],\n        [0.2626],\n        [0.2355],\n        [0.3675],\n        [0.2455],\n        [0.2617],\n        [0.2143],\n        [0.2416],\n        [0.2608],\n        [0.3201],\n        [0.1952],\n        [0.0974],\n        [0.1269],\n        [0.3031],\n        [0.1903],\n        [0.2194],\n        [0.3374],\n        [0.2909],\n        [0.3223],\n        [0.2819],\n        [0.0863],\n        [0.3343],\n        [0.4530],\n        [0.2698],\n        [0.3060],\n        [0.2567],\n        [0.2034],\n        [0.3090],\n        [0.2929],\n        [0.2336],\n        [0.3110],\n        [0.2547],\n        [0.2416],\n        [0.1107],\n        [0.1471],\n        [0.2425],\n        [0.1790],\n        [0.2547],\n        [0.1821],\n        [0.1086],\n        [0.1971],\n        [0.3151],\n        [0.4682],\n        [0.1952],\n        [0.3352],\n        [0.2274],\n        [0.2678],\n        [0.2084],\n        [1.5213],\n        [1.4747],\n        [1.6079],\n        [1.2349],\n        [1.4758],\n        [1.4877],\n        [1.5794],\n        [0.9759],\n        [1.4779],\n        [1.2478],\n        [1.0213],\n        [1.3588],\n        [1.1907],\n        [1.5573],\n        [1.1049],\n        [1.4003],\n        [1.5128],\n        [1.2943],\n        [1.3983],\n        [1.2017],\n        [1.6428],\n        [1.2380],\n        [1.5957],\n        [1.5482],\n        [1.3590],\n        [1.3983],\n        [1.5434],\n        [1.6572],\n        [1.4757],\n        [1.0264],\n        [1.1553],\n        [1.1110],\n        [1.2057],\n        [1.7234],\n        [1.5268],\n        [1.5209],\n        [1.5333],\n        [1.3560],\n        [1.3355],\n        [1.2530],\n        [1.4393],\n        [1.5220],\n        [1.2410],\n        [0.9598],\n        [1.3527],\n        [1.3728],\n        [1.3638],\n        [1.3730],\n        [0.8379],\n        [1.3104],\n        [2.1555],\n        [1.7375],\n        [2.0279],\n        [1.9421],\n        [2.0257],\n        [2.3030],\n        [1.5167],\n        [2.1821],\n        [1.9664],\n        [2.1638],\n        [1.7336],\n        [1.7840],\n        [1.8717],\n        [1.6821],\n        [1.7465],\n        [1.8293],\n        [1.8928],\n        [2.4127],\n        [2.3927],\n        [1.6339],\n        [1.9714],\n        [1.6719],\n        [2.3222],\n        [1.6138],\n        [1.9945],\n        [2.0833],\n        [1.5855],\n        [1.6550],\n        [1.9260],\n        [1.9766],\n        [2.0774],\n        [2.2657],\n        [1.9260],\n        [1.7114],\n        [1.9289],\n        [2.0744],\n        [1.9873],\n        [1.9088],\n        [1.6177],\n        [1.8294],\n        [1.9321],\n        [1.6965],\n        [1.7375],\n        [2.0671],\n        [1.9945],\n        [1.7458],\n        [1.6400],\n        [1.7598],\n        [1.9057],\n        [1.7576]], dtype=torch.float64, grad_fn=<AddmmBackward>)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1 , 3.5 , 1.4 , 0.22],\n       [4.9 , 3.  , 1.4 , 0.19],\n       [4.7 , 3.2 , 1.3 , 0.18],\n       [4.6 , 3.1 , 1.5 , 0.26],\n       [5.  , 3.6 , 1.4 , 0.24],\n       [5.4 , 3.9 , 1.7 , 0.37],\n       [4.6 , 3.4 , 1.4 , 0.25],\n       [5.  , 3.4 , 1.5 , 0.26],\n       [4.4 , 2.9 , 1.4 , 0.21],\n       [4.9 , 3.1 , 1.5 , 0.24],\n       [5.4 , 3.7 , 1.5 , 0.26],\n       [4.8 , 3.4 , 1.6 , 0.32],\n       [4.8 , 3.  , 1.4 , 0.2 ],\n       [4.3 , 3.  , 1.1 , 0.1 ],\n       [5.8 , 4.  , 1.2 , 0.13],\n       [5.7 , 4.4 , 1.5 , 0.3 ],\n       [5.4 , 3.9 , 1.3 , 0.19],\n       [5.1 , 3.5 , 1.4 , 0.22],\n       [5.7 , 3.8 , 1.7 , 0.34],\n       [5.1 , 3.8 , 1.5 , 0.29],\n       [5.4 , 3.4 , 1.7 , 0.32],\n       [5.1 , 3.7 , 1.5 , 0.28],\n       [4.6 , 3.6 , 1.  , 0.09],\n       [5.1 , 3.3 , 1.7 , 0.33],\n       [4.8 , 3.4 , 1.9 , 0.45],\n       [5.  , 3.  , 1.6 , 0.27],\n       [5.  , 3.4 , 1.6 , 0.31],\n       [5.2 , 3.5 , 1.5 , 0.26],\n       [5.2 , 3.4 , 1.4 , 0.2 ],\n       [4.7 , 3.2 , 1.6 , 0.31],\n       [4.8 , 3.1 , 1.6 , 0.29],\n       [5.4 , 3.4 , 1.5 , 0.23],\n       [5.2 , 4.1 , 1.5 , 0.31],\n       [5.5 , 4.2 , 1.4 , 0.25],\n       [4.9 , 3.1 , 1.5 , 0.24],\n       [5.  , 3.2 , 1.2 , 0.11],\n       [5.5 , 3.5 , 1.3 , 0.15],\n       [4.9 , 3.6 , 1.4 , 0.24],\n       [4.4 , 3.  , 1.3 , 0.18],\n       [5.1 , 3.4 , 1.5 , 0.25],\n       [5.  , 3.5 , 1.3 , 0.18],\n       [4.5 , 2.3 , 1.3 , 0.11],\n       [4.4 , 3.2 , 1.3 , 0.2 ],\n       [5.  , 3.5 , 1.6 , 0.32],\n       [5.1 , 3.8 , 1.9 , 0.47],\n       [4.8 , 3.  , 1.4 , 0.2 ],\n       [5.1 , 3.8 , 1.6 , 0.34],\n       [4.6 , 3.2 , 1.4 , 0.23],\n       [5.3 , 3.7 , 1.5 , 0.27],\n       [5.  , 3.3 , 1.4 , 0.21],\n       [7.  , 3.2 , 4.7 , 1.52],\n       [6.4 , 3.2 , 4.5 , 1.47],\n       [6.9 , 3.1 , 4.9 , 1.61],\n       [5.5 , 2.3 , 4.  , 1.23],\n       [6.5 , 2.8 , 4.6 , 1.48],\n       [5.7 , 2.8 , 4.5 , 1.49],\n       [6.3 , 3.3 , 4.7 , 1.58],\n       [4.9 , 2.4 , 3.3 , 0.98],\n       [6.6 , 2.9 , 4.6 , 1.48],\n       [5.2 , 2.7 , 3.9 , 1.25],\n       [5.  , 2.  , 3.5 , 1.02],\n       [5.9 , 3.  , 4.2 , 1.36],\n       [6.  , 2.2 , 4.  , 1.19],\n       [6.1 , 2.9 , 4.7 , 1.56],\n       [5.6 , 2.9 , 3.6 , 1.1 ],\n       [6.7 , 3.1 , 4.4 , 1.4 ],\n       [5.6 , 3.  , 4.5 , 1.51],\n       [5.8 , 2.7 , 4.1 , 1.29],\n       [6.2 , 2.2 , 4.5 , 1.4 ],\n       [5.6 , 2.5 , 3.9 , 1.2 ],\n       [5.9 , 3.2 , 4.8 , 1.64],\n       [6.1 , 2.8 , 4.  , 1.24],\n       [6.3 , 2.5 , 4.9 , 1.6 ],\n       [6.1 , 2.8 , 4.7 , 1.55],\n       [6.4 , 2.9 , 4.3 , 1.36],\n       [6.6 , 3.  , 4.4 , 1.4 ],\n       [6.8 , 2.8 , 4.8 , 1.54],\n       [6.7 , 3.  , 5.  , 1.66],\n       [6.  , 2.9 , 4.5 , 1.48],\n       [5.7 , 2.6 , 3.5 , 1.03],\n       [5.5 , 2.4 , 3.8 , 1.16],\n       [5.5 , 2.4 , 3.7 , 1.11],\n       [5.8 , 2.7 , 3.9 , 1.21],\n       [6.  , 2.7 , 5.1 , 1.72],\n       [5.4 , 3.  , 4.5 , 1.53],\n       [6.  , 3.4 , 4.5 , 1.52],\n       [6.7 , 3.1 , 4.7 , 1.53],\n       [6.3 , 2.3 , 4.4 , 1.36],\n       [5.6 , 3.  , 4.1 , 1.34],\n       [5.5 , 2.5 , 4.  , 1.25],\n       [5.5 , 2.6 , 4.4 , 1.44],\n       [6.1 , 3.  , 4.6 , 1.52],\n       [5.8 , 2.6 , 4.  , 1.24],\n       [5.  , 2.3 , 3.3 , 0.96],\n       [5.6 , 2.7 , 4.2 , 1.35],\n       [5.7 , 3.  , 4.2 , 1.37],\n       [5.7 , 2.9 , 4.2 , 1.36],\n       [6.2 , 2.9 , 4.3 , 1.37],\n       [5.1 , 2.5 , 3.  , 0.84],\n       [5.7 , 2.8 , 4.1 , 1.31],\n       [6.3 , 3.3 , 6.  , 2.16],\n       [5.8 , 2.7 , 5.1 , 1.74],\n       [7.1 , 3.  , 5.9 , 2.03],\n       [6.3 , 2.9 , 5.6 , 1.94],\n       [6.5 , 3.  , 5.8 , 2.03],\n       [7.6 , 3.  , 6.6 , 2.3 ],\n       [4.9 , 2.5 , 4.5 , 1.52],\n       [7.3 , 2.9 , 6.3 , 2.18],\n       [6.7 , 2.5 , 5.8 , 1.97],\n       [7.2 , 3.6 , 6.1 , 2.16],\n       [6.5 , 3.2 , 5.1 , 1.73],\n       [6.4 , 2.7 , 5.3 , 1.78],\n       [6.8 , 3.  , 5.5 , 1.87],\n       [5.7 , 2.5 , 5.  , 1.68],\n       [5.8 , 2.8 , 5.1 , 1.75],\n       [6.4 , 3.2 , 5.3 , 1.83],\n       [6.5 , 3.  , 5.5 , 1.89],\n       [7.7 , 3.8 , 6.7 , 2.41],\n       [7.7 , 2.6 , 6.9 , 2.39],\n       [6.  , 2.2 , 5.  , 1.63],\n       [6.9 , 3.2 , 5.7 , 1.97],\n       [5.6 , 2.8 , 4.9 , 1.67],\n       [7.7 , 2.8 , 6.7 , 2.32],\n       [6.3 , 2.7 , 4.9 , 1.61],\n       [6.7 , 3.3 , 5.7 , 1.99],\n       [7.2 , 3.2 , 6.  , 2.08],\n       [6.2 , 2.8 , 4.8 , 1.59],\n       [6.1 , 3.  , 4.9 , 1.65],\n       [6.4 , 2.8 , 5.6 , 1.93],\n       [7.2 , 3.  , 5.8 , 1.98],\n       [7.4 , 2.8 , 6.1 , 2.08],\n       [7.9 , 3.8 , 6.4 , 2.27],\n       [6.4 , 2.8 , 5.6 , 1.93],\n       [6.3 , 2.8 , 5.1 , 1.71],\n       [6.1 , 2.6 , 5.6 , 1.93],\n       [7.7 , 3.  , 6.1 , 2.07],\n       [6.3 , 3.4 , 5.6 , 1.99],\n       [6.4 , 3.1 , 5.5 , 1.91],\n       [6.  , 3.  , 4.8 , 1.62],\n       [6.9 , 3.1 , 5.4 , 1.83],\n       [6.7 , 3.1 , 5.6 , 1.93],\n       [6.9 , 3.1 , 5.1 , 1.7 ],\n       [5.8 , 2.7 , 5.1 , 1.74],\n       [6.8 , 3.2 , 5.9 , 2.07],\n       [6.7 , 3.3 , 5.7 , 1.99],\n       [6.7 , 3.  , 5.2 , 1.75],\n       [6.3 , 2.5 , 5.  , 1.64],\n       [6.5 , 3.  , 5.2 , 1.76],\n       [6.2 , 3.4 , 5.4 , 1.91],\n       [5.9 , 3.  , 5.1 , 1.76]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = np.concatenate((features, new_column.detach().numpy()), axis=1)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('first_iris.csv', new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}