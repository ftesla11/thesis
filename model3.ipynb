{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondad424485431ff4a8aa076a99cfa3fb48c",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "from distcorr import distcorr\n",
    "from fractions import gcd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN = 0\n",
    "\n",
    "def fill_nan(array, REPLACE_COUNT):\n",
    "    array.flat[np.random.choice(array.size, int(REPLACE_COUNT), replace=False)] = NAN\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENABLE/DISABLE GPU\n",
    "# UNCOMMENT to.device() FOR TENSORS AND NEURAL NETWORKS TO ENABLE GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('breast/breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NARROW THE FEATURES\n",
    "array = np.array(['radius_mean', 'perimeter_mean', 'area_mean', 'radius_worst', 'perimeter_worst', 'area_worst'])\n",
    "processed_data = dataset[array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO NUMPY\n",
    "numpy_data = processed_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE THE DATA\n",
    "scaled_data = scaler.fit_transform(numpy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = scaled_data.copy()\n",
    "missing_data = fill_nan(scaled_data, scaled_data.size*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 10)\n",
    "y_binned = np.digitize(full_data[:,0], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(missing_data, full_data, stratify=y_binned, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_features=6, out_features=8)\n",
    "        self.enc2 = nn.Linear(in_features=8, out_features=16)\n",
    "\n",
    "        self.dec2 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.dec3 = nn.Linear(in_features=8, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.enc1(x))\n",
    "        x = F.leaky_relu(self.enc2(x))\n",
    "        x = F.leaky_relu(self.dec2(x))\n",
    "        x = self.dec3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A SEED FOR CONSISTENT WEIGHT INITIALIZATIONS - FOR TESTING PURPOSES\n",
    "random.seed(2)\n",
    "torch.manual_seed(random.randint(1, 10))\n",
    "net = Autoencoder().double()\n",
    "#net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 110\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = -1\n",
    "NUM_FEATURES = 6\n",
    "BATCH_SIZE_TEST = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([426, 6])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.from_numpy(x_train)\n",
    "#x_train = x_train.to(device)\n",
    "x_train = x_train.view(BATCH_SIZE, NUM_FEATURES)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "#y_train = y_train.to(device)\n",
    "y_train = y_train.view(BATCH_SIZE, NUM_FEATURES)\n",
    "\n",
    "x_test = torch.from_numpy(x_test)\n",
    "#x_test = x_test.to(device)\n",
    "x_test = x_test.view(BATCH_SIZE_TEST, NUM_FEATURES)\n",
    "\n",
    "y_test = torch.from_numpy(y_test)\n",
    "#y_test = y_test.to(device)\n",
    "y_test = y_test.view(BATCH_SIZE_TEST, NUM_FEATURES)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net):\n",
    "    train_loss = []\n",
    "    torch.set_printoptions(precision=2, sci_mode=True)\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # running_loss: LOSS OF THE PREDICTED MISSING VALUE ONLY\n",
    "        # overall_loss: LOSS OF ALL RECONSTRUCTED VALUES\n",
    "\n",
    "        running_loss = 0.0\n",
    "        overall_loss = 0.0\n",
    "        count = 0\n",
    "        for missing_data, full_data in zip(x_train, y_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(missing_data.double())\n",
    "\n",
    "            # LEARN FROM LOSS OF ALL RECONSTRUCTED VALUES\n",
    "            loss = criterion(outputs, full_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            # COMPUTE LOSS OF PREDICTED MISSING VALUE\n",
    "            if NAN in missing_data:\n",
    "                for index in range(len(missing_data)):\n",
    "                    if missing_data[index] == NAN:\n",
    "                        predicted_loss = criterion(outputs[index], full_data[index])\n",
    "                        running_loss += predicted_loss.item()\n",
    "                        count += 1\n",
    "            \n",
    "            # PRINT ALL VALUES ON LAST EPOCH FOR TESTING PURPOSES\n",
    "            if epoch == NUM_EPOCHS-1:\n",
    "                if missing_data.detach().numpy().all() == full_data.detach().numpy().all():\n",
    "                    print(\"Input: \", scaler.inverse_transform(missing_data.reshape(-1,6)))\n",
    "                    print(\"Target: \", scaler.inverse_transform(full_data.reshape(-1,6)))\n",
    "                    print(\"Outputs: \", scaler.inverse_transform(outputs.reshape(-1,6).detach().numpy()))\n",
    "                else:\n",
    "                    print(\"Input (missing): \", scaler.inverse_transform(missing_data.reshape(-1,6)))\n",
    "                    print(\"Target (missing): \", scaler.inverse_transform(full_data.reshape(-1,6)))\n",
    "                    print(\"Outputs (missing): \", scaler.inverse_transform(outputs.reshape(-1,6).detach().numpy()))\n",
    "        \n",
    "      #  loss = running_loss / count\n",
    "        overall_loss = overall_loss / len(x_train)\n",
    "        x_loss = running_loss / count\n",
    "        train_loss.append(loss)\n",
    "\n",
    "        print('Epoch {} of {}, Train Loss: {:.5f}, Overall: {:.5f}'\n",
    "             .format(epoch+1, NUM_EPOCHS, x_loss, overall_loss))\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(net):\n",
    "\n",
    "    net.eval()\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for missing_data, full_data in zip(x_test, y_test):\n",
    "            outputs = net(missing_data.double())\n",
    "            if NAN in missing_data:\n",
    "                for index in range(len(missing_data)):\n",
    "                    if missing_data[index] == NAN:\n",
    "                        predicted_loss = criterion(outputs[index], full_data[index])\n",
    "                        running_loss += predicted_loss.item()\n",
    "                        count += 1\n",
    "            \n",
    "            if missing_data.detach().numpy().all() == full_data.detach().numpy().all():\n",
    "                print(\"Input: \", scaler.inverse_transform(missing_data.reshape(-1,6)))\n",
    "                print(\"Target: \", scaler.inverse_transform(full_data.reshape(-1,6)))\n",
    "                print(\"Outputs: \", scaler.inverse_transform(outputs.reshape(-1,6).detach().numpy()))\n",
    "            else:\n",
    "                print(\"Input (missing): \", scaler.inverse_transform(missing_data.reshape(-1,6)))\n",
    "                print(\"Target (missing): \", scaler.inverse_transform(full_data.reshape(-1,6)))\n",
    "                print(\"Outputs (missing): \", scaler.inverse_transform(outputs.reshape(-1,6).detach().numpy()))\n",
    "\n",
    "        overall_loss = running_loss / len(x_test)\n",
    "        x_loss = running_loss / count\n",
    "        test_loss.append(loss)\n",
    "        print('Test Loss: {:.3f}, Overall: {:.5f}'.format(x_loss, overall_loss))\n",
    "\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nOutputs (missing):  [[  18.67  124.86 1142.02   23.17  149.61 1659.13]]\nInput (missing):  [[   6.98  152.8  1509.     28.4   206.8  2360.  ]]\nTarget (missing):  [[  22.27  152.8  1509.     28.4   206.8  2360.  ]]\nOutputs (missing):  [[  23.19  154.92 1597.49   28.89  197.45 2336.95]]\nInput (missing):  [[  6.98  43.79 143.5   13.09  85.07 523.7 ]]\nTarget (missing):  [[ 12.    76.77 442.5   13.09  85.07 523.7 ]]\nOutputs (missing):  [[ 11.36  72.94 384.71  12.56  80.85 456.31]]\nInput:  [[ 14.4   92.25 646.1   15.4  100.4  734.6 ]]\nTarget:  [[ 14.4   92.25 646.1   15.4  100.4  734.6 ]]\nOutputs:  [[ 14.22  92.01 643.54  16.11 107.96 877.67]]\nInput (missing):  [[ 12.21  78.78 462.    13.13  50.41 529.9 ]]\nTarget (missing):  [[ 12.21  78.78 462.    13.13  87.65 529.9 ]]\nOutputs (missing):  [[ 12.15  78.8  477.93  13.79  89.11 590.01]]\nInput (missing):  [[ 10.91  69.14 143.5   11.37  50.41 392.2 ]]\nTarget (missing):  [[ 10.91  69.14 363.7   11.37  72.42 392.2 ]]\nOutputs (missing):  [[ 10.65  68.29 321.58  11.69  74.24 351.11]]\nInput (missing):  [[ 13.61  43.79 572.6   16.89  50.41 848.7 ]]\nTarget (missing):  [[ 13.61  87.76 572.6   16.89 113.2  848.7 ]]\nOutputs (missing):  [[ 13.77  90.32 641.28  16.16 103.57 859.38]]\nInput:  [[ 11.62  76.38 408.8   13.36  88.14 528.1 ]]\nTarget:  [[ 11.62  76.38 408.8   13.36  88.14 528.1 ]]\nOutputs:  [[ 12.46  80.08 477.22  13.82  91.28 609.46]]\nInput (missing):  [[ 10.71  43.79 344.9   11.69  50.41 185.2 ]]\nTarget (missing):  [[ 10.71  69.5  344.9   11.69  76.51 410.4 ]]\nOutputs (missing):  [[ 10.37  66.09 277.77  11.15  70.82 293.67]]\nInput (missing):  [[ 13.27  84.55 143.5   15.14  98.84 708.8 ]]\nTarget (missing):  [[ 13.27  84.55 546.4   15.14  98.84 708.8 ]]\nOutputs (missing):  [[ 13.51  87.47 585.15  15.37 100.85 787.01]]\nInput (missing):  [[  6.98  86.34 557.2   15.05  96.69 185.2 ]]\nTarget (missing):  [[ 13.38  86.34 557.2   15.05  96.69 705.6 ]]\nOutputs (missing):  [[ 12.66  82.05 527.39  14.43  94.25 671.1 ]]\nInput (missing):  [[ 14.03  89.79 603.4   15.33  98.27 185.2 ]]\nTarget (missing):  [[ 14.03  89.79 603.4   15.33  98.27 715.5 ]]\nOutputs (missing):  [[ 13.35  86.14 564.82  15.01  99.94 749.43]]\nInput:  [[  21.37  141.3  1386.     22.69  152.1  1535.  ]]\nTarget:  [[  21.37  141.3  1386.     22.69  152.1  1535.  ]]\nOutputs:  [[  20.7   136.14 1261.01   24.66  169.12 1874.6 ]]\nInput:  [[  17.91  124.4   994.     20.8   149.6  1304.  ]]\nTarget:  [[  17.91  124.4   994.     20.8   149.6  1304.  ]]\nOutputs:  [[  18.39  120.23 1039.27   21.53  147.79 1511.55]]\nInput (missing):  [[  6.98  43.79 541.8    7.93  96.59 623.7 ]]\nTarget (missing):  [[ 13.28  85.79 541.8   14.24  96.59 623.7 ]]\nOutputs (missing):  [[ 11.13  70.92 352.95  12.05  79.07 404.86]]\nInput (missing):  [[  6.98  43.79 143.5    9.26  58.36 259.2 ]]\nTarget (missing):  [[  8.67  54.42 227.2    9.26  58.36 259.2 ]]\nOutputs (missing):  [[  9.48  60.16 203.44  10.07  62.86 166.27]]\nInput:  [[ 14.19  92.87 610.7   16.86 115.   811.3 ]]\nTarget:  [[ 14.19  92.87 610.7   16.86 115.   811.3 ]]\nOutputs:  [[ 14.89  96.52 706.55  16.96 114.38 975.67]]\nInput (missing):  [[  16.24  108.8   143.5    18.55  126.9  1031.  ]]\nTarget (missing):  [[  16.24  108.8   805.1    18.55  126.9  1031.  ]]\nOutputs (missing):  [[  16.09  105.17  837.17   18.86  125.07 1192.15]]\nInput:  [[ 11.64  75.17 412.5   13.14  85.51 521.7 ]]\nTarget:  [[ 11.64  75.17 412.5   13.14  85.51 521.7 ]]\nOutputs:  [[ 12.32  79.17 463.26  13.62  89.91 584.92]]\nInput (missing):  [[ 14.97  96.22 685.9   16.11  50.41 793.7 ]]\nTarget (missing):  [[ 14.97  96.22 685.9   16.11 104.6  793.7 ]]\nOutputs (missing):  [[ 14.33  94.16 705.55  16.84 110.68 936.11]]\nInput (missing):  [[  21.61  144.4  1407.      7.93  172.   2081.  ]]\nTarget (missing):  [[  21.61  144.4  1407.     26.23  172.   2081.  ]]\nOutputs (missing):  [[  21.93  145.17 1414.92   26.79  180.74 2118.66]]\nInput:  [[ 11.16  70.95 380.3   12.36  79.26 458.  ]]\nTarget:  [[ 11.16  70.95 380.3   12.36  79.26 458.  ]]\nOutputs:  [[ 11.79  75.59 412.8   12.93  84.78 503.43]]\nInput (missing):  [[ 14.04  89.78 611.2    7.93 101.2  750.  ]]\nTarget (missing):  [[ 14.04  89.78 611.2   15.66 101.2  750.  ]]\nOutputs (missing):  [[ 14.05  91.21 644.8   16.13 106.28 876.42]]\nInput:  [[  16.84  108.4   880.2    18.22  120.3  1032.  ]]\nTarget:  [[  16.84  108.4   880.2    18.22  120.3  1032.  ]]\nOutputs:  [[  16.38  106.7   846.52   18.89  128.32 1198.99]]\nInput (missing):  [[  18.61  122.1  1094.     21.31   50.41 1403.  ]]\nTarget (missing):  [[  18.61  122.1  1094.     21.31  139.9  1403.  ]]\nOutputs (missing):  [[  18.12  120.96 1108.15   22.24  148.17 1547.09]]\nInput (missing):  [[ 11.95  77.23 143.5    7.93  83.09 185.2 ]]\nTarget (missing):  [[ 11.95  77.23 426.7   12.81  83.09 496.2 ]]\nOutputs (missing):  [[ 11.26  72.2  378.27  12.45  79.95 445.96]]\nInput (missing):  [[   6.98  132.9   143.5     7.93   50.41 1844.  ]]\nTarget (missing):  [[  20.31  132.9  1288.     24.33  162.3  1844.  ]]\nOutputs (missing):  [[  15.9   107.38  990.79   20.25  131.41 1293.21]]\nInput (missing):  [[ 19.18 127.5  143.5   23.36 166.4  185.2 ]]\nTarget (missing):  [[  19.18  127.5  1148.     23.36  166.4  1688.  ]]\nOutputs (missing):  [[  17.25  113.04  955.72   20.42  136.44 1374.27]]\nInput (missing):  [[  18.94  123.6  1130.     24.86   50.41 1866.  ]]\nTarget (missing):  [[  18.94  123.6  1130.     24.86  165.9  1866.  ]]\nOutputs (missing):  [[  19.48  130.8  1259.57   24.29  161.4  1773.74]]\nInput:  [[ 11.54  73.73 409.1   12.34  81.23 467.8 ]]\nTarget:  [[ 11.54  73.73 409.1   12.34  81.23 467.8 ]]\nOutputs:  [[ 11.97  76.74 428.2   13.13  86.39 525.41]]\nInput (missing):  [[   6.98  147.3   143.5    28.19   50.41 2384.  ]]\nTarget (missing):  [[  21.75  147.3  1491.     28.19  195.9  2384.  ]]\nOutputs (missing):  [[  18.9   129.56 1333.49   24.94  160.82 1802.85]]\nInput (missing):  [[ 13.78  88.37 585.9    7.93  97.9  706.6 ]]\nTarget (missing):  [[ 13.78  88.37 585.9   15.27  97.9  706.6 ]]\nOutputs (missing):  [[ 13.84  89.75 622.41  15.82 104.07 838.68]]\nInput (missing):  [[  6.98  90.63 588.9    7.93  50.41 806.9 ]]\nTarget (missing):  [[ 13.77  90.63 588.9   16.39 111.6  806.9 ]]\nOutputs (missing):  [[ 13.32  88.25 666.24  16.06 103.54 830.55]]\nInput (missing):  [[ 10.17  64.55 311.9    7.93  69.86 368.6 ]]\nTarget (missing):  [[ 10.17  64.55 311.9   11.02  69.86 368.6 ]]\nOutputs (missing):  [[ 10.83  69.07 324.41  11.69  75.78 358.31]]\nInput (missing):  [[ 12.65  82.69 485.6   14.38  50.41 633.7 ]]\nTarget (missing):  [[ 12.65  82.69 485.6   14.38  95.29 633.7 ]]\nOutputs (missing):  [[ 12.76  83.22 544.11  14.65  95.15 683.02]]\nInput (missing):  [[ 12.36  43.79 466.1   13.83  91.46 185.2 ]]\nTarget (missing):  [[ 12.36  79.78 466.1   13.83  91.46 574.7 ]]\nOutputs (missing):  [[ 12.15  78.14 449.16  13.47  87.84 562.83]]\nInput (missing):  [[ 11.41  73.53 402.     7.93  79.12 467.2 ]]\nTarget (missing):  [[ 11.41  73.53 402.    12.37  79.12 467.2 ]]\nOutputs (missing):  [[ 11.9   76.44 430.17  13.16  85.82 529.51]]\nInput (missing):  [[  9.78  62.5  143.5   11.05  71.68 185.2 ]]\nTarget (missing):  [[  9.78  62.5  290.2   11.05  71.68 367.  ]]\nOutputs (missing):  [[ 10.55  67.12 294.79  11.3   72.93 312.15]]\nInput (missing):  [[ 16.13 107.   807.2   20.21 132.7  185.2 ]]\nTarget (missing):  [[  16.13  107.    807.2    20.21  132.7  1261.  ]]\nOutputs (missing):  [[  15.9   103.4   804.25   18.28  123.95 1127.84]]\nInput (missing):  [[ 11.2   43.79 143.5    7.93  75.19 439.6 ]]\nTarget (missing):  [[ 11.2   70.67 386.    11.92  75.19 439.6 ]]\nOutputs (missing):  [[ 10.52  67.11 298.97  11.39  72.54 321.23]]\nInput (missing):  [[  6.98  43.79 288.5   10.62  66.53 342.9 ]]\nTarget (missing):  [[  9.74  61.24 288.5   10.62  66.53 342.9 ]]\nOutputs (missing):  [[ 10.28  65.68 280.44  11.11  70.56 283.78]]\nInput:  [[  17.6   119.    980.5    21.57  143.6  1437.  ]]\nTarget:  [[  17.6   119.    980.5    21.57  143.6  1437.  ]]\nOutputs:  [[  18.55  121.35 1045.58   21.64  148.67 1518.46]]\nInput (missing):  [[ 11.52  73.87 143.5   12.65  50.41 491.8 ]]\nTarget (missing):  [[ 11.52  73.87 406.3   12.65  80.88 491.8 ]]\nOutputs (missing):  [[ 11.35  73.35 397.95  12.69  81.26 460.78]]\nInput (missing):  [[  20.55   43.79 1308.     24.3   160.2  1809.  ]]\nTarget (missing):  [[  20.55  137.8  1308.     24.3   160.2  1809.  ]]\nOutputs (missing):  [[  21.46  142.29 1348.21   26.13  173.86 2027.29]]\nInput:  [[ 11.81  75.27 428.9   12.57  79.57 489.5 ]]\nTarget:  [[ 11.81  75.27 428.9   12.57  79.57 489.5 ]]\nOutputs:  [[ 12.14  77.92 442.6   13.35  87.89 551.18]]\nInput (missing):  [[ 12.67  81.25 489.9    7.93  50.41 185.2 ]]\nTarget (missing):  [[ 12.67  81.25 489.9   13.71  88.7  574.4 ]]\nOutputs (missing):  [[ 12.08  78.39 474.42  13.76  88.06 586.76]]\nInput (missing):  [[ 12.34  81.15 477.4   15.65 101.7  185.2 ]]\nTarget (missing):  [[ 12.34  81.15 477.4   15.65 101.7  768.9 ]]\nOutputs (missing):  [[ 13.11  84.43 536.4   14.6   97.45 698.93]]\nInput:  [[  9.03  58.79 250.5   10.31  65.5  324.7 ]]\nTarget:  [[  9.03  58.79 250.5   10.31  65.5  324.7 ]]\nOutputs:  [[ 10.47  66.81 292.28  11.28  72.21 307.16]]\nInput (missing):  [[ 11.68  75.49 143.5   13.32  50.41 549.8 ]]\nTarget (missing):  [[ 11.68  75.49 420.5   13.32  86.57 549.8 ]]\nOutputs (missing):  [[ 11.69  75.8  436.68  13.21  84.7  518.44]]\nInput:  [[ 14.61  92.68 664.9   16.46 103.7  840.8 ]]\nTarget:  [[ 14.61  92.68 664.9   16.46 103.7  840.8 ]]\nOutputs:  [[ 14.73  95.48 685.46  16.69 112.45 941.18]]\nInput (missing):  [[  6.98  96.71 719.5   17.26 110.1  931.4 ]]\nTarget (missing):  [[ 15.13  96.71 719.5   17.26 110.1  931.4 ]]\nOutputs (missing):  [[  14.89   97.84  764.08   17.58  116.43 1023.37]]\nInput:  [[ 12.88  82.5  514.3   13.89  88.84 595.7 ]]\nTarget:  [[ 12.88  82.5  514.3   13.89  88.84 595.7 ]]\nOutputs:  [[ 13.05  84.07 527.84  14.52  96.6  687.6 ]]\nInput (missing):  [[ 13.11  43.79 529.4   14.55  50.41 639.3 ]]\nTarget (missing):  [[ 13.11  87.02 529.4   14.55  99.48 639.3 ]]\nOutputs (missing):  [[ 12.66  82.39 517.77  14.46  92.56 664.77]]\nInput (missing):  [[  8.88  56.74 241.     9.98  65.27 185.2 ]]\nTarget (missing):  [[  8.88  56.74 241.     9.98  65.27 302.  ]]\nOutputs (missing):  [[ 10.14  64.41 256.25  10.8   68.88 252.67]]\nInput (missing):  [[ 11.69  76.37 406.4   12.98  50.41 487.7 ]]\nTarget (missing):  [[ 11.69  76.37 406.4   12.98  86.12 487.7 ]]\nOutputs (missing):  [[ 11.87  77.   451.44  13.4   86.51 541.28]]\nInput:  [[ 14.96  97.03 687.3   16.25 109.1  809.8 ]]\nTarget:  [[ 14.96  97.03 687.3   16.25 109.1  809.8 ]]\nOutputs:  [[ 14.9   96.64 703.58  16.93 114.22 969.2 ]]\nInput (missing):  [[ 17.08 111.2  930.9   22.96 152.1  185.2 ]]\nTarget (missing):  [[  17.08  111.2   930.9    22.96  152.1  1648.  ]]\nOutputs (missing):  [[  17.24  112.49  931.42   20.04  136.64 1332.96]]\nInput (missing):  [[ 12.89  81.89 515.9   13.62  50.41 577.  ]]\nTarget (missing):  [[ 12.89  81.89 515.9   13.62  87.4  577.  ]]\nOutputs (missing):  [[ 12.66  82.45 530.71  14.47  94.19 664.21]]\nInput:  [[  25.73  174.2  2010.     33.13  229.3  3234.  ]]\nTarget:  [[  25.73  174.2  2010.     33.13  229.3  3234.  ]]\nOutputs:  [[  28.4   188.56 1990.82   34.59  242.73 3022.26]]\nInput (missing):  [[  18.03  117.5   143.5     7.93  133.3  1292.  ]]\nTarget (missing):  [[  18.03  117.5   990.     20.38  133.3  1292.  ]]\nOutputs (missing):  [[  15.95  104.47  839.64   18.88  123.95 1192.84]]\nInput (missing):  [[ 12.3   77.88 464.4   13.35  50.41 544.3 ]]\nTarget (missing):  [[ 12.3   77.88 464.4   13.35  84.53 544.3 ]]\nOutputs (missing):  [[ 12.23  79.45 486.49  13.88  89.92 597.3 ]]\nInput:  [[  21.09  142.7  1311.     26.68  176.5  2089.  ]]\nTarget:  [[  21.09  142.7  1311.     26.68  176.5  2089.  ]]\nOutputs:  [[  22.39  147.65 1417.38   26.75  185.23 2112.83]]\nInput (missing):  [[   6.98  117.3   981.6    21.53  145.4  1437.  ]]\nTarget (missing):  [[  17.75  117.3   981.6    21.53  145.4  1437.  ]]\nOutputs (missing):  [[  17.91  118.71 1068.89   21.7   146.   1499.74]]\nInput:  [[  17.29  114.4   947.8    20.39  137.9  1295.  ]]\nTarget:  [[  17.29  114.4   947.8    20.39  137.9  1295.  ]]\nOutputs:  [[  17.79  116.27  977.06   20.69  141.62 1410.25]]\nInput (missing):  [[ 13.59  43.79 572.3   15.5   98.91 739.1 ]]\nTarget (missing):  [[ 13.59  86.24 572.3   15.5   98.91 739.1 ]]\nOutputs (missing):  [[ 14.08  91.58 637.58  16.16 105.48 874.1 ]]\nInput:  [[ 11.34  72.48 396.5   13.01  83.99 518.1 ]]\nTarget:  [[ 11.34  72.48 396.5   13.01  83.99 518.1 ]]\nOutputs:  [[ 12.2   78.4  450.45  13.46  88.56 565.09]]\nInput (missing):  [[  17.06   43.79  918.6    20.99  143.2  1362.  ]]\nTarget (missing):  [[  17.06  111.8   918.6    20.99  143.2  1362.  ]]\nOutputs (missing):  [[  18.42  121.44 1058.85   22.05  145.97 1558.02]]\nInput (missing):  [[  6.98  82.38 512.2   13.9   50.41 597.5 ]]\nTarget (missing):  [[ 12.87  82.38 512.2   13.9   89.27 597.5 ]]\nOutputs (missing):  [[ 12.43  81.96 559.96  14.73  93.94 679.34]]\nInput (missing):  [[ 13.05  43.79 530.6    7.93  93.96 672.4 ]]\nTarget (missing):  [[ 13.05  82.71 530.6   14.73  93.96 672.4 ]]\nOutputs (missing):  [[ 11.76  75.36 414.08  12.94  84.65 505.32]]\nInput (missing):  [[  19.07  128.3   143.5    24.09  177.4  1651.  ]]\nTarget (missing):  [[  19.07  128.3  1104.     24.09  177.4  1651.  ]]\nOutputs (missing):  [[  20.26  133.9  1243.71   24.53  164.14 1850.01]]\nInput (missing): [[ 12.8   83.05 508.3    7.93  90.72 591.  ]]\nTarget (missing):  [[ 12.8   83.05 508.3   13.74  90.72 591.  ]]\nOutputs (missing):  [[ 13.02  84.2  543.01  14.74  96.51 714.41]]\nInput (missing):  [[  6.98  62.92 295.4   10.42  67.08 331.6 ]]\nTarget (missing):  [[  9.88  62.92 295.4   10.42  67.08 331.6 ]]\nOutputs (missing):  [[ 10.49  67.27 312.1   11.51  73.04 329.32]]\nInput (missing):  [[ 23.51  43.79 143.5   30.67 202.4  185.2 ]]\nTarget (missing):  [[  23.51  155.1  1747.     30.67  202.4  2906.  ]]\nOutputs (missing):  [[  21.07  140.69 1361.     26.48  169.69 2060.95]]\nInput (missing):  [[  6.98  94.89 673.7   16.31 102.3  777.5 ]]\nTarget (missing):  [[ 14.86  94.89 673.7   16.31 102.3  777.5 ]]\nOutputs (missing):  [[ 14.21  93.23 698.68  16.71 109.98 924.76]]\nInput (missing):  [[  6.98  43.79 143.5   14.98 101.1  686.6 ]]\nTarget (missing):  [[ 13.56  88.59 561.3   14.98 101.1  686.6 ]]\nOutputs (missing):  [[ 12.46  80.53 491.74  14.01  91.53 623.75]]\nInput (missing):  [[ 14.95  96.85 678.1   18.55 121.4  185.2 ]]\nTarget (missing):  [[ 14.95  96.85 678.1   18.55 121.4  971.4 ]]\nOutputs (missing):  [[ 14.78  95.83 699.67  16.86 113.5  964.12]]\nInput (missing):  [[  6.98  80.88 495.    13.65  88.12 566.9 ]]\nTarget (missing):  [[ 12.7   80.88 495.    13.65  88.12 566.9 ]]\nOutputs (missing):  [[ 12.54  81.53 523.26  14.36  93.37 656.78]]\nInput (missing):  [[ 13.16  84.06 538.7   14.5   95.29 185.2 ]]\nTarget (missing):  [[ 13.16  84.06 538.7   14.5   95.29 648.3 ]]\nOutputs (missing):  [[ 12.81  82.43 510.62  14.26  94.8  660.29]]\nInput (missing):  [[  8.2   43.79 143.5    8.96  57.26 185.2 ]]\nTarget (missing):  [[  8.2   51.71 201.9    8.96  57.26 242.2 ]]\nOutputs (missing):  [[  9.46  59.86 195.03   9.96  62.58 156.11]]\nInput (missing):  [[ 15.05  97.26 701.9    7.93 113.8  967.  ]]\nTarget (missing):  [[ 15.05  97.26 701.9   17.58 113.8  967.  ]]\nOutputs (missing):  [[  15.17   98.86  753.43   17.66  116.97 1054.5 ]]\nInput:  [[  19.8   129.7  1230.     25.73  170.3  2009.  ]]\nTarget:  [[  19.8   129.7  1230.     25.73  170.3  2009.  ]]\nOutputs:  [[  21.51  141.69 1333.31   25.65  176.97 1988.05]]\nInput (missing):  [[ 15.    97.45 143.5   16.41 114.2  808.2 ]]\nTarget (missing):  [[ 15.    97.45 684.5   16.41 114.2  808.2 ]]\nOutputs (missing):  [[ 14.64  95.25 695.54  16.9  111.63 964.12]]\nInput (missing):  [[ 15.66 110.2  143.5   19.85 143.7  185.2 ]]\nTarget (missing):  [[  15.66  110.2   773.5    19.85  143.7  1226.  ]]\nOutputs (missing):  [[  15.36  100.05  769.62   17.87  119.07 1079.07]]\nInput:  [[ 12.04  76.85 449.9   13.6   87.24 567.6 ]]\nTarget:  [[ 12.04  76.85 449.9   13.6   87.24 567.6 ]]\nOutputs:  [[ 12.61  81.14 489.5   14.01  92.7  629.15]]\nInput (missing):  [[  6.98  78.41 466.1   14.1   50.41 185.2 ]]\nTarget (missing):  [[ 12.27  78.41 466.1   14.1   89.   610.2 ]]\nOutputs (missing):  [[ 11.56  75.61 458.64  13.42  85.03 533.37]]\nInput (missing):  [[ 14.68  94.74 684.5   19.07 123.4  185.2 ]]\nTarget (missing):  [[  14.68   94.74  684.5    19.07  123.4  1138.  ]]\nOutputs (missing):  [[ 14.89  96.6  710.72  17.03 114.75 983.32]]\nInput (missing):  [[ 12.49  79.19 143.5   13.34  84.48 544.2 ]]\nTarget (missing):  [[ 12.49  79.19 481.6   13.34  84.48 544.2 ]]\nOutputs (missing):  [[ 12.29  79.11 464.72  13.7   89.48 591.76]]\nInput:  [[ 14.74  94.7  668.6   16.51 107.4  826.4 ]]\nTarget:  [[ 14.74  94.7  668.6   16.51 107.4  826.4 ]]\nOutputs:  [[ 14.76  95.73 693.2   16.82 113.29 957.45]]\nInput (missing):  [[ 14.11  43.79 616.5   15.53  98.4  185.2 ]]\nTarget (missing):  [[ 14.11  90.03 616.5   15.53  98.4  749.9 ]]\nOutputs (missing):  [[ 13.07  84.55 541.34  14.81  96.66 718.71]]\nInput:  [[ 12.45  82.57 477.1   15.47 103.4  741.6 ]]\nTarget:  [[ 12.45  82.57 477.1   15.47 103.4  741.6 ]]\nOutputs:  [[ 13.77  88.96 599.13  15.52 103.87 805.73]]\nInput (missing):  [[ 10.26  65.85 320.8   10.83  50.41 357.4 ]]\nTarget (missing):  [[ 10.26  65.85 320.8   10.83  71.08 357.4 ]]\nOutputs (missing):  [[ 10.71  68.77 328.14  11.78  75.05 359.9 ]]\nInput (missing):  [[  19.68  129.9  1194.      7.93  157.6  1540.  ]]\nTarget (missing):  [[  19.68  129.9  1194.     22.75  157.6  1540.  ]]\nOutputs (missing):  [[  19.71  130.04 1201.48   23.9   160.31 1780.39]]\nInput (missing):  [[  6.98  90.96 578.9   15.75 104.4  750.1 ]]\nTarget (missing):  [[ 13.86  90.96 578.9   15.75 104.4  750.1 ]]\nOutputs (missing):  [[ 13.8   90.11 646.82  16.07 105.69 854.2 ]]\nInput:  [[ 13.68  87.76 575.5   15.85 101.6  773.4 ]]\nTarget:  [[ 13.68  87.76 575.5   15.85 101.6  773.4 ]]\nOutputs:  [[ 14.11  91.31 630.83  15.97 107.15 857.36]]\nInput:  [[ 15.32 103.2  713.3   17.73 119.8  928.8 ]]\nTarget:  [[ 15.32 103.2  713.3   17.73 119.8  928.8 ]]\nOutputs:  [[  15.67  101.88  780.06   18.02  122.09 1096.81]]\nInput:  [[ 15.04  98.73 689.4   16.76 109.7  856.9 ]]\nTarget:  [[ 15.04  98.73 689.4   16.76 109.7  856.9 ]]\nOutputs:  [[ 15.01  97.38 716.18  17.15 115.69 994.44]]\nInput:  [[ 12.31  79.19 470.9   14.11  89.71 611.1 ]]\nTarget:  [[ 12.31  79.19 470.9   14.11  89.71 611.1 ]]\nOutputs:  [[ 12.9   83.1  516.39  14.4   95.55 673.22]]\nInput (missing):  [[ 15.37  43.79 143.5   16.43 107.5  830.9 ]]\nTarget (missing):  [[ 15.37 100.2  728.2   16.43 107.5  830.9 ]]\nOutputs (missing):  [[  14.98   98.35  746.01   17.84  113.46 1061.1 ]]\nInput (missing):  [[ 14.64  43.79 666.    16.46 106.   831.  ]]\nTarget (missing):  [[ 14.64  94.21 666.    16.46 106.   831.  ]]\nOutputs (missing):  [[ 14.84  96.83 712.97  17.26 112.97 999.68]]\nInput (missing):  [[ 11.54  74.65 143.5   12.26  78.78 457.8 ]]\nTarget (missing):  [[ 11.54  74.65 402.9   12.26  78.78 457.8 ]]\nOutputs (missing):  [[ 11.57  74.11 392.95  12.7   82.85 474.2 ]]\nInput:  [[ 13.66  88.27 580.6   14.54  97.96 657.  ]]\nTarget:  [[ 13.66  88.27 580.6   14.54  97.96 657.  ]]\nOutputs:  [[ 13.62  87.92 584.19  15.32 102.55 780.13]]\nInput (missing):  [[  20.48  132.5  1306.      7.93  161.7  1750.  ]]\nTarget (missing):  [[  20.48  132.5  1306.     24.22  161.7  1750.  ]]\nOutputs (missing):  [[  20.4   134.77 1265.49   24.8   166.82 1881.84]]\nInput:  [[ 14.29  90.3  632.6   14.91  94.44 684.6 ]]\nTarget:  [[ 14.29  90.3  632.6   14.91  94.44 684.6 ]]\nOutputs:  [[ 13.87  89.71 608.05  15.67 104.74 819.6 ]]\nInput:  [[ 14.78  97.4  668.3   17.31 114.6  925.1 ]]\nTarget:  [[ 14.78  97.4  668.3   17.31 114.6  925.1 ]]\nOutputs:  [[  15.23   98.89  736.03   17.42  117.78 1023.96]]\nInput (missing):  [[ 14.22  94.37 609.9   15.74  50.41 762.4 ]]\nTarget (missing):  [[ 14.22  94.37 609.9   15.74 106.4  762.4 ]]\nOutputs (missing):  [[ 13.89  91.15 660.66  16.28 106.56 866.99]]\nEpoch 110 of 110, Train Loss: 0.00037, Overall: 0.00201\n"
    }
   ],
   "source": [
    "# TRAIN THE NEURAL NETWORK\n",
    "results = train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x17e187d4c50>]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m17462d2560\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.530346\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(101.167846 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.376884\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(157.014384 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"219.223423\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(212.860923 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.069962\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(268.707462 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.916501\" xlink:href=\"#m17462d2560\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(321.372751 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m77e037a985\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"215.887175\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.00 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 219.686393)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"184.67349\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.01 -->\r\n      <g transform=\"translate(7.2 188.472708)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"153.459804\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.02 -->\r\n      <g transform=\"translate(7.2 157.259023)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"122.246119\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.03 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 126.045338)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"91.032434\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.04 -->\r\n      <g transform=\"translate(7.2 94.831653)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"59.818749\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.05 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 63.617968)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m77e037a985\" y=\"28.605064\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.06 -->\r\n      <g transform=\"translate(7.2 32.404283)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p3def8450b6)\" d=\"M 51.683807 17.083636 \r\nL 54.476134 171.994736 \r\nL 57.268461 209.676712 \r\nL 60.060788 212.667168 \r\nL 62.853115 212.929479 \r\nL 65.645442 212.654723 \r\nL 68.437768 212.245456 \r\nL 71.230095 211.885913 \r\nL 74.022422 211.412264 \r\nL 76.814749 210.812908 \r\nL 79.607076 210.322715 \r\nL 82.399403 209.948097 \r\nL 85.19173 209.601833 \r\nL 87.984057 208.91423 \r\nL 90.776384 208.592855 \r\nL 93.568711 208.564325 \r\nL 96.361038 208.658655 \r\nL 99.153365 208.850829 \r\nL 101.945692 209.124875 \r\nL 104.738019 209.47032 \r\nL 107.530346 209.760877 \r\nL 110.322673 210.079535 \r\nL 113.114999 210.383904 \r\nL 115.907326 210.68551 \r\nL 118.699653 210.976777 \r\nL 121.49198 211.252651 \r\nL 124.284307 211.252563 \r\nL 127.076634 211.173547 \r\nL 129.868961 211.420071 \r\nL 132.661288 211.634903 \r\nL 135.453615 211.858103 \r\nL 138.245942 212.091288 \r\nL 141.038269 212.303846 \r\nL 143.830596 212.491943 \r\nL 146.622923 212.692681 \r\nL 149.41525 212.864559 \r\nL 152.207577 212.99657 \r\nL 154.999904 213.166729 \r\nL 157.792231 213.304754 \r\nL 160.584557 213.453369 \r\nL 163.376884 213.615141 \r\nL 166.169211 213.7427 \r\nL 168.961538 213.861277 \r\nL 171.753865 213.992382 \r\nL 174.546192 214.099296 \r\nL 177.338519 214.178898 \r\nL 180.130846 214.215559 \r\nL 182.923173 214.24597 \r\nL 185.7155 214.272154 \r\nL 188.507827 214.305582 \r\nL 191.300154 214.346752 \r\nL 194.092481 214.391384 \r\nL 196.884808 214.425855 \r\nL 199.677135 214.458596 \r\nL 202.469462 214.461752 \r\nL 205.261788 214.498662 \r\nL 208.054115 214.530199 \r\nL 210.846442 214.540789 \r\nL 213.638769 214.564887 \r\nL 216.431096 214.592029 \r\nL 219.223423 214.624575 \r\nL 222.01575 214.63828 \r\nL 224.808077 214.656308 \r\nL 227.600404 214.678577 \r\nL 230.392731 214.696669 \r\nL 233.185058 214.70475 \r\nL 235.977385 214.718037 \r\nL 238.769712 214.734863 \r\nL 241.562039 214.743982 \r\nL 244.354366 214.751379 \r\nL 247.146693 214.722819 \r\nL 249.939019 214.720796 \r\nL 252.731346 214.727433 \r\nL 255.523673 214.731966 \r\nL 258.316 214.738003 \r\nL 261.108327 214.740556 \r\nL 263.900654 214.736952 \r\nL 266.692981 214.735839 \r\nL 269.485308 214.744123 \r\nL 272.277635 214.736427 \r\nL 275.069962 214.732059 \r\nL 277.862289 214.728732 \r\nL 280.654616 214.720694 \r\nL 283.446943 214.725589 \r\nL 286.23927 214.725657 \r\nL 289.031597 214.72569 \r\nL 291.823924 214.725452 \r\nL 294.616251 214.717528 \r\nL 297.408577 214.754224 \r\nL 300.200904 214.715824 \r\nL 302.993231 214.749968 \r\nL 305.785558 214.749017 \r\nL 308.577885 214.756364 \r\nL 311.370212 214.751661 \r\nL 314.162539 214.751791 \r\nL 316.954866 214.750866 \r\nL 319.747193 214.743536 \r\nL 322.53952 214.741797 \r\nL 325.331847 214.743046 \r\nL 328.124174 214.746604 \r\nL 330.916501 214.747196 \r\nL 333.708828 214.749049 \r\nL 336.501155 214.743142 \r\nL 339.293482 214.749166 \r\nL 342.085808 214.745652 \r\nL 344.878135 214.743591 \r\nL 347.670462 214.734048 \r\nL 350.462789 214.731554 \r\nL 353.255116 214.730782 \r\nL 356.047443 214.730635 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p3def8450b6\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "   85.24 546.1   14.2   92.94 621.2 ]]\nOutputs (missing):  [[ 12.42  80.6  486.26  14.11  90.01 624.88]]\nInput (missing):  [[ 11.3   73.93 389.4   12.58  87.16 185.2 ]]\nTarget (missing):  [[ 11.3   73.93 389.4   12.58  87.16 472.9 ]]\nOutputs (missing):  [[ 11.71  74.92 404.72  12.81  84.58 487.86]]\nInput (missing):  [[  6.98  43.79 143.5   13.37  50.41 185.2 ]]\nTarget (missing):  [[ 12.56  81.92 485.8   13.37  89.02 547.4 ]]\nOutputs (missing):  [[ 10.14  65.23 286.49  11.2   69.83 282.41]]\nInput (missing):  [[  6.98  94.25 648.2   16.21 108.4  185.2 ]]\nTarget (missing):  [[ 14.48  94.25 648.2   16.21 108.4  808.9 ]]\nOutputs (missing):  [[ 13.41  87.17 597.56  15.42 101.59 781.06]]\nInput (missing):  [[ 11.32  43.79 395.7    7.93  79.82 452.3 ]]\nTarget (missing):  [[ 11.32  71.76 395.7   12.08  79.82 452.3 ]]\nOutputs (missing):  [[ 10.68  67.94 307.94  11.5   74.58 333.76]]\nInput:  [[ 15.61 100.   758.6   17.91 115.9  988.6 ]]\nTarget:  [[ 15.61 100.   758.6   17.91 115.9  988.6 ]]\nOutputs:  [[  15.66  101.83  775.99   17.97  121.79 1088.07]]\nInput (missing):  [[  21.16  137.2  1404.      7.93  188.   2615.  ]]\nTarget (missing):  [[  21.16  137.2  1404.     29.17  188.   2615.  ]]\nOutputs (missing):  [[  22.14  146.4  1425.6    26.96  183.63 2133.76]]\nInput (missing):  [[ 16.17 106.3  143.5   16.97 113.1  861.5 ]]\nTarget (missing):  [[ 16.17 106.3  788.5   16.97 113.1  861.5 ]]\nOutputs (missing):  [[  15.02   97.88  731.6    17.45  115.13 1021.93]]\nInput (missing):  [[ 14.05  91.38 143.5   15.3  100.2  706.7 ]]\nTarget (missing):  [[ 14.05  91.38 600.4   15.3  100.2  706.7 ]]\nOutputs (missing):  [[ 13.65  88.48 597.1   15.56 102.35 803.72]]\nInput:  [[ 12.83  82.89 506.9   14.09  93.22 605.8 ]]\nTarget:  [[ 12.83  82.89 506.9   14.09  93.22 605.8 ]]\nOutputs:  [[ 13.08  84.27 532.26  14.6   97.36 694.93]]\nInput:  [[  8.89  58.79 244.     9.73  62.56 284.4 ]]\nTarget:  [[  8.89  58.79 244.     9.73  62.56 284.4 ]]\nOutputs:  [[ 10.09  64.17 253.07  10.79  68.57 249.19]]\nInput (missing):  [[  6.98  60.21 143.5   10.51  50.41 185.2 ]]\nTarget (missing):  [[  9.57  60.21 279.6   10.51  65.74 335.9 ]]\nOutputs (missing):  [[  9.64  61.68 236.05  10.49  65.14 203.94]]\nInput (missing): [[ 12.47  80.45 480.1    7.93  92.82 607.3 ]]\nTarget (missing):  [[ 12.47  80.45 480.1   14.06  92.82 607.3 ]]\nOutputs (missing):  [[ 12.79  82.49 518.4   14.42  94.61 673.13]]\nInput (missing):  [[ 13.    43.79 143.5    7.93  90.82 616.7 ]]\nTarget (missing):  [[ 13.    83.51 519.4   14.16  90.82 616.7 ]]\nOutputs (missing):  [[ 11.5   73.93 400.74  12.86  81.85 488.57]]\nInput (missing):  [[ 14.99  95.54 698.8    7.93  95.54 698.8 ]]\nTarget (missing):  [[ 14.99  95.54 698.8   14.99  95.54 698.8 ]]\nOutputs (missing):  [[ 14.48  94.3  687.31  16.82 110.19 949.75]]\nInput (missing):  [[ 10.65  68.01 143.5   12.25  77.98 455.7 ]]\nTarget (missing):  [[ 10.65  68.01 347.    12.25  77.98 455.7 ]]\nOutputs (missing):  [[ 11.35  72.61 370.51  12.39  80.69 436.5 ]]\nInput (missing):  [[  6.98  92.51 641.2   16.77 110.4  873.2 ]]\nTarget (missing):  [[ 14.34  92.51 641.2   16.77 110.4  873.2 ]]\nOutputs (missing):  [[ 14.37  94.09 702.93  16.84 111.22 938.93]]\nInput:  [[ 11.14  71.24 384.6   12.12  79.62 453.5 ]]\nTarget:  [[ 11.14  71.24 384.6   12.12  79.62 453.5 ]]\nOutputs:  [[ 11.72  75.1  404.89  12.86  84.29 491.31]]\nInput:  [[  18.08  117.4  1024.     19.76  129.1  1228.  ]]\nTarget:  [[  18.08  117.4  1024.     19.76  129.1  1228.  ]]\nOutputs:  [[  17.56  114.71  955.79   20.47  139.69 1378.02]]\nInput (missing):  [[   6.98  140.9  1546.     30.75  199.5  3143.  ]]\nTarget (missing):  [[  21.71  140.9  1546.     30.75  199.5  3143.  ]]\nOutputs (missing):  [[  24.36  163.72 1729.29   30.71  209.72 2524.32]]\nInput:  [[  8.22  53.27 203.9    9.09  58.08 249.8 ]]\nTarget:  [[  8.22  53.27 203.9    9.09  58.08 249.8 ]]\nOutputs:  [[  9.64  61.11 210.69  10.21  64.21 181.19]]\nInput (missing):  [[   6.98   43.79 1264.     24.22  156.1  1750.  ]]\nTarget (missing):  [[  20.26  132.4  1264.     24.22  156.1  1750.  ]]\nOutputs (missing):  [[  18.43  121.58 1074.37   22.02  149.15 1542.03]]\nInput (missing):  [[  6.98  76.53 143.5   13.67  87.54 583.  ]]\nTarget (missing):  [[ 11.93  76.53 438.6   13.67  87.54 583.  ]]\nOutputs (missing):  [[ 11.97  77.51 462.18  13.57  87.75 562.61]]\nInput (missing):  [[  6.98 111.   933.1    7.93  50.41 185.2 ]]\nTarget (missing):  [[  17.35  111.    933.1    19.85  128.2  1218.  ]]\nOutputs (missing):  [[  14.58   97.28  814.62   18.03  117.23 1051.79]]\nInput (missing):  [[ 10.57  66.82 143.5   10.94  50.41 185.2 ]]\nTarget (missing):  [[ 10.57  66.82 340.9   10.94  69.35 366.3 ]]\nOutputs (missing):  [[ 10.1   64.36 258.33  10.86  68.76 253.71]]\nInput (missing):  [[ 10.16  64.73 311.7    7.93  67.88 347.3 ]]\nTarget (missing):  [[ 10.16  64.73 311.7   10.65  67.88 347.3 ]]\nOutputs (missing):  [[ 10.71  68.29 313.26  11.58  74.85 342.94]]\nInput (missing):  [[   6.98  101.7   748.9    19.26  124.9  1156.  ]]\nTarget (missing):  [[  15.46  101.7   748.9    19.26  124.9  1156.  ]]\nOutputs (missing):  [[  15.89  104.68  859.5    18.95  126.24 1180.61]]\nInput (missing):  [[  8.73  55.27 143.5    7.93  64.01 317.  ]]\nTarget (missing):  [[  8.73  55.27 234.3   10.17  64.01 317.  ]]\nOutputs (missing):  [[  9.66  61.16 212.98  10.2   64.79 182.23]]\nInput (missing):  [[ 11.84  77.93 143.5   16.82 119.4  888.7 ]]\nTarget (missing):  [[ 11.84  77.93 440.6   16.82 119.4  888.7 ]]\nOutputs (missing):  [[ 14.4   93.47 666.54  16.5  109.62 913.77]]\nInput (missing):  [[ 11.46  43.79 403.1   12.68  82.69 489.8 ]]\nTarget (missing):  [[ 11.46  73.59 403.1   12.68  82.69 489.8 ]]\nOutputs (missing):  [[ 11.97  76.99 431.88  13.28  86.28 538.56]]\nInput (missing):  [[ 13.65  87.88 568.9   15.34  99.71 185.2 ]]\nTarget (missing):  [[ 13.65  87.88 568.9   15.34  99.71 706.2 ]]\nOutputs (missing):  [[ 13.18  84.96 545.44  14.77  98.46 715.22]]\nInput (missing):  [[ 12.2   78.01 457.9    7.93  50.41 583.1 ]]\nTarget (missing):  [[ 12.2   78.01 457.9   13.75  91.11 583.1 ]]\nOutputs (missing):  [[ 12.06  78.21 470.3   13.7   88.4  578.09]]\nInput:  [[ 14.25  96.42 645.7   17.67 119.1  959.5 ]]\nTarget:  [[ 14.25  96.42 645.7   17.67 119.1  959.5 ]]\nOutputs:  [[  15.36   99.74  747.67   17.56  119.07 1041.13]]\nInput (missing):  [[ 11.71  43.79 143.5   13.06  84.16 185.2 ]]\nTarget (missing):  [[ 11.71  75.03 420.3   13.06  84.16 516.4 ]]\nOutputs (missing):  [[ 11.53  74.18 398.37  12.85  81.93 486.58]]\nInput:  [[ 12.68  82.69 499.    17.09 111.8  888.3 ]]\nTarget:  [[ 12.68  82.69 499.    17.09 111.8  888.3 ]]\nOutputs:  [[ 14.51  93.99 666.94  16.47 110.77 912.67]]\nInput (missing):  [[ 13.62  87.19 143.5   15.35  97.58 185.2 ]]\nTarget (missing):  [[ 13.62  87.19 573.2   15.35  97.58 729.8 ]]\nOutputs (missing):  [[ 12.69  81.84 505.65  14.27  93.44 654.56]]\nInput (missing):  [[ 11.42  77.58 386.1   14.91  98.87 185.2 ]]\nTarget (missing):  [[ 11.42  77.58 386.1   14.91  98.87 567.7 ]]\nOutputs (missing):  [[ 12.52  80.42 481.61  13.87  92.24 611.64]]\nInput:  [[ 11.93  76.14 442.7   13.8   87.64 589.5 ]]\nTarget:  [[ 11.93  76.14 442.7   13.8   87.64 589.5 ]]\nOutputs:  [[ 12.62  81.23 489.26  14.03  92.81 627.38]]\nInput:  [[ 12.77  81.35 507.9   13.87  88.1  594.7 ]]\nTarget:  [[ 12.77  81.35 507.9   13.87  88.1  594.7 ]]\nOutputs:  [[ 12.87  82.84 511.61  14.32  95.19 662.71]]\nInput (missing):  [[ 10.49  66.86 143.5    7.93  70.76 375.4 ]]\nTarget (missing):  [[ 10.49  66.86 334.3   11.06  70.76 375.4 ]]\nOutputs (missing):  [[ 10.55  67.19 299.77  11.4   73.28 321.32]]\nInput (missing):  [[ 24.25 166.2  143.5   26.02 180.9  185.2 ]]\nTarget (missing):  [[  24.25  166.2  1761.     26.02  180.9  2073.  ]]\nOutputs (missing):  [[  19.14  126.13 1144.16   23.15  154.56 1685.28]]\nInput (missing):  [[ 14.95  43.79 689.5   16.25 107.1  809.7 ]]\nTarget (missing):  [[ 14.95  97.84 689.5   16.25 107.1  809.7 ]]\nOutputs (missing):  [[ 14.81  96.59 709.05  17.2  112.62 990.47]]\nInput (missing):  [[ 13.    87.5  519.8   15.49  50.41 739.3 ]]\nTarget (missing):  [[ 13.    87.5  519.8   15.49 106.2  739.3 ]]\nOutputs (missing):  [[ 13.24  86.63 595.2   15.39 100.07 764.74]]\nInput (missing):  [[  17.42  114.5   143.5    18.07  120.4  1021.  ]]\nTarget (missing):  [[  17.42  114.5   948.     18.07  120.4  1021.  ]]\nOutputs (missing):  [[  15.98  104.55  826.66   18.79  124.09 1176.78]]\nInput:  [[ 14.2   92.41 618.4   16.45 112.1  828.5 ]]\nTarget:  [[ 14.2   92.41 618.4   16.45 112.1  828.5 ]]\nOutputs:  [[ 14.7   95.27 686.26  16.72 112.78 942.55]]\nInput (missing):  [[  6.98  73.7  143.5   13.28  50.41 542.5 ]]\nTarget (missing):  [[ 11.66  73.7  421.    13.28  83.61 542.5 ]]\nOutputs (missing):  [[ 10.94  71.2  389.31  12.52  78.65 427.26]]\nInput (missing):  [[  17.47  116.1   984.6    23.14   50.41 1660.  ]]\nTarget (missing):  [[  17.47  116.1   984.6    23.14  155.3  1660.  ]]\nOutputs (missing):  [[  18.12  121.28 1116.29   22.45  148.45 1560.99]]\nInput (missing):  [[  20.94  138.9   143.5    25.58  165.3  2010.  ]]\nTarget (missing):  [[  20.94  138.9  1364.     25.58  165.3  2010.  ]]\nOutputs (missing):  [[  21.39  142.   1359.82   26.27  174.72 2039.99]]\nInput:  [[  16.13  108.1   798.8    20.96  136.8  1315.  ]]\nTarget:  [[  16.13  108.1   798.8    20.96  136.8  1315.  ]]\nOutputs:  [[  17.45  114.01  945.64   20.32  138.76 1361.31]]\nInput (missing):  [[ 13.7   87.76 143.5   14.96  95.78 686.5 ]]\nTarget (missing):  [[ 13.7   87.76 571.1   14.96  95.78 686.5 ]]\nOutputs (missing):  [[ 13.38  86.65 570.79  15.2   99.74 761.73]]\nInput (missing):  [[  6.98  96.03 651.     7.93 101.7  767.3 ]]\nTarget (missing):  [[ 14.41  96.03 651.    15.77 101.7  767.3 ]]\nOutputs (missing):  [[ 14.08  92.13 687.96  16.54 109.36 907.04]]\nInput (missing):  [[ 15.71 102.   143.5    7.93 114.3  922.8 ]]\nTarget (missing):  [[ 15.71 102.   761.7   17.5  114.3  922.8 ]]\nOutputs (missing):  [[ 14.12  91.87 659.61  16.41 107.11 903.07]]\nInput (missing):  [[ 16.14 104.3  800.    17.71  50.41 947.9 ]]\nTarget (missing):  [[ 16.14 104.3  800.    17.71 115.9  947.9 ]]\nOutputs (missing):  [[  15.37  101.46  811.94   18.33  121.01 1100.52]]\nInput (missing):  [[ 10.51  43.79 334.2    7.93  50.41 362.7 ]]\nTarget (missing):  [[ 10.51  66.85 334.2   10.93  70.1  362.7 ]]\nOutputs (missing):  [[  9.79  62.   221.66  10.34  65.83 197.31]]\nInput:  [[  19.45  126.5  1169.     25.7   163.1  1972.  ]]\nTarget:  [[  19.45  126.5  1169.     25.7   163.1  1972.  ]]\nOutputs:  [[  21.16  139.38 1301.69   25.26  173.79 1933.49]]\nInput:  [[  16.65  110.    904.6    26.46  177.   2215.  ]]\nTarget:  [[  16.65  110.    904.6    26.46  177.   2215.  ]]\nOutputs:  [[  21.64  143.01 1366.01   26.1   179.06 2025.52]]\nInput (missing):  [[  6.98  86.49 143.5   15.53  50.41 185.2 ]]\nTarget (missing):  [[ 13.34  86.49 520.    15.53  96.66 614.9 ]]\nOutputs (missing):  [[ 11.22  73.36 428.05  13.01  81.87 481.16]]\nInput:  [[ 13.01  82.01 526.4   14.    88.18 608.8 ]]\nTarget:  [[ 13.01  82.01 526.4   14.    88.18 608.8 ]]\nOutputs:  [[ 12.96  83.47 520.13  14.44  96.08 676.44]]\nInput (missing):  [[ 11.8   78.99 143.5   13.74  91.93 591.7 ]]\nTarget (missing):  [[ 11.8   78.99 432.    13.74  91.93 591.7 ]]\nOutputs (missing):  [[ 12.45  80.09 476.69  13.85  91.21 606.91]]\nInput:  [[ 12.54  81.25 476.3   13.57  86.67 552.  ]]\nTarget:  [[ 12.54  81.25 476.3   13.57  86.67 552.  ]]\nOutputs:  [[ 12.66  81.44 492.66  14.06  93.25 632.11]]\nInput (missing):  [[ 11.22  43.79 386.8   12.36  78.44 470.9 ]]\nTarget (missing):  [[ 11.22  70.79 386.8   12.36  78.44 470.9 ]]\nOutputs (missing):  [[ 11.71  75.2  406.26  12.93  83.8  497.29]]\nInput (missing):  [[ 10.75  68.26 355.3   11.95  50.41 185.2 ]]\nTarget (missing):  [[ 10.75  68.26 355.3   11.95  77.79 441.2 ]]\nOutputs (missing):  [[ 10.72  68.76 323.84  11.74  74.93 353.64]]\nInput (missing):  [[ 12.99  84.08 143.5   13.72  87.38 576.  ]]\nTarget (missing):  [[ 12.99  84.08 514.3   13.72  87.38 576.  ]]\nOutputs (missing):  [[ 12.55  80.86 488.36  14.04  91.92 627.64]]\nInput (missing):  [[  6.98  98.22 656.1   16.46  50.41 809.2 ]]\nTarget (missing):  [[ 14.69  98.22 656.1   16.46 114.1  809.2 ]]\nOutputs (missing):  [[ 13.93  92.84 732.33  17.05 109.59 933.67]]\nInput (missing):  [[  6.98  84.1  537.9    7.93  50.41 632.9 ]]\nTarget (missing):  [[ 13.21  84.1  537.9   14.35  91.29 632.9 ]]\nOutputs (missing):  [[ 12.51  82.38 570.38  14.87  95.28 694.89]]\nInput (missing):  [[ 11.04  70.92 373.2    7.93  79.93 471.4 ]]\nTarget (missing):  [[ 11.04  70.92 373.2   12.41  79.93 471.4 ]]\nOutputs (missing):  [[ 11.57  74.13 397.68  12.74  83.06 478.33]]\nInput (missing):  [[ 16.3  104.7  143.5   17.32 109.8  928.2 ]]\nTarget (missing):  [[ 16.3  104.7  819.8   17.32 109.8  928.2 ]]\nOutputs (missing):  [[  15.21   99.27  750.86   17.74  116.69 1055.07]]\nInput (missing):  [[ 13.64  43.79 575.3   14.85  94.11 683.4 ]]\nTarget (missing):  [[ 13.64  87.38 575.3   14.85  94.11 683.4 ]]\nOutputs (missing):  [[ 13.64  88.53 594.71  15.6  101.63 804.92]]\nInput (missing):  [[  15.75   43.79  761.3    19.56  125.9  1088.  ]]\nTarget (missing):  [[  15.75  102.6   761.3    19.56  125.9  1088.  ]]\nOutputs (missing):  [[  16.77  110.1   899.28   19.88  130.87 1300.04]]\nInput (missing): [[  18.81  120.9  1102.      7.93  129.    185.2 ]]\nTarget (missing):  [[  18.81  120.9  1102.     19.96  129.   1236.  ]]\nOutputs (missing):  [[  17.11  112.55  959.64   20.6   135.35 1388.82]]\nInput (missing):  [[ 14.92  96.45 686.9   17.18  50.41 906.6 ]]\nTarget (missing):  [[ 14.92  96.45 686.9   17.18 112.   906.6 ]]\nOutputs (missing):  [[ 14.58  95.99 731.8   17.25 113.19 976.94]]\nInput (missing):  [[  20.13  131.2  1261.      7.93  155.   1731.  ]]\nTarget (missing):  [[  20.13  131.2  1261.     23.69  155.   1731.  ]]\nOutputs (missing):  [[  20.05  132.4  1230.37   24.32  163.33 1823.55]]\nInput (missing):  [[ 13.66  89.46 575.3   15.14 101.4  185.2 ]]\nTarget (missing):  [[ 13.66  89.46 575.3   15.14 101.4  708.8 ]]\nOutputs (missing):  [[ 13.22  85.23 549.94  14.82  98.93 721.91]]\nInput (missing):  [[  6.98  59.6  271.2    7.93  68.73 359.4 ]]\nTarget (missing):  [[  9.4   59.6  271.2   10.85  68.73 359.4 ]]\nOutputs (missing):  [[ 10.21  65.12 275.91  11.06  70.2  278.47]]\nInput:  [[  19.4   127.2  1145.     23.79  152.4  1628.  ]]\nTarget:  [[  19.4   127.2  1145.     23.79  152.4  1628.  ]]\nOutputs:  [[  19.74  129.52 1160.27   23.29  160.35 1707.48]]\nInput:  [[ 12.58  79.83 489.    13.5   85.56 564.1 ]]\nTarget:  [[ 12.58  79.83 489.    13.5   85.56 564.1 ]]\nOutputs:  [[ 12.63  81.26 489.62  14.02  92.95 627.44]]\nInput (missing):  [[ 13.8   90.43 584.1   16.57  50.41 812.4 ]]\nTarget (missing):  [[ 13.8   90.43 584.1   16.57 110.3  812.4 ]]\nOutputs (missing):  [[ 13.79  90.5  651.25  16.16 105.43 852.41]]\nInput (missing):  [[  20.59  137.8  1320.      7.93  163.2  1760.  ]]\nTarget (missing):  [[  20.59  137.8  1320.     23.86  163.2  1760.  ]]\nOutputs (missing):  [[  20.74  137.08 1298.48   25.26  169.86 1932.93]]\nInput (missing):  [[  6.98  73.38 143.5   12.04  79.73 450.  ]]\nTarget (missing):  [[ 11.27  73.38 392.    12.04  79.73 450.  ]]\nOutputs (missing):  [[ 11.21  72.24 384.43  12.52  80.29 442.62]]\nInput:  [[  20.58  134.7  1290.     23.24  158.3  1656.  ]]\nTarget:  [[  20.58  134.7  1290.     23.24  158.3  1656.  ]]\nOutputs:  [[  20.17  132.41 1202.19   23.85  164.68 1773.33]]\nInput (missing):  [[ 11.89  76.39 433.8    7.93  85.09 522.9 ]]\nTarget (missing):  [[ 11.89  76.39 433.8   13.05  85.09 522.9 ]]\nOutputs (missing):  [[ 12.21  78.56 461.74  13.64  89.09 582.21]]\nInput (missing):  [[ 14.02  89.59 143.5   14.91  96.53 185.2 ]]\nTarget (missing):  [[ 14.02  89.59 606.5   14.91  96.53 688.9 ]]\nOutputs (missing):  [[ 12.65  81.62 503.28  14.24  93.1  650.82]]\nInput (missing):  [[ 12.25  78.18 466.5   14.17  92.74 185.2 ]]\nTarget (missing):  [[ 12.25  78.18 466.5   14.17  92.74 622.9 ]]\nOutputs (missing):  [[ 12.4   79.6  469.7   13.72  90.99 593.23]]\nInput (missing):  [[ 15.75 107.1  758.6   17.36  50.41 915.3 ]]\nTarget (missing):  [[ 15.75 107.1  758.6   17.36 119.4  915.3 ]]\nOutputs (missing):  [[  15.28  100.93  807.77   18.25  120.43 1090.42]]\nInput:  [[  16.69  107.1   857.6    19.18  127.3  1084.  ]]\nTarget:  [[  16.69  107.1   857.6    19.18  127.3  1084.  ]]\nOutputs:  [[  16.64  108.48  869.62   19.26  131.13 1238.47]]\nInput (missing):  [[ 12.9   83.74 512.2   14.48  97.17 185.2 ]]\nTarget (missing):  [[ 12.9   83.74 512.2   14.48  97.17 643.8 ]]\nOutputs (missing):  [[ 12.76  82.09 505.5   14.21  94.52 650.45]]\nInput (missing):  [[   6.98  109.3   886.3     7.93  130.7  1260.  ]]\nTarget (missing):  [[  16.78  109.3   886.3    20.05  130.7  1260.  ]]\nOutputs (missing):  [[  16.28  107.05  902.85   19.45  130.74 1246.52]]\nInput:  [[  18.45  120.2  1075.     22.52  145.6  1590.  ]]\nTarget:  [[  18.45  120.2  1075.     22.52  145.6  1590.  ]]\nOutputs:  [[  19.03  124.71 1093.12   22.37  153.63 1599.36]]\nInput (missing):  [[  16.5   106.6   838.1     7.93  117.2  1009.  ]]\nTarget (missing):  [[  16.5   106.6   838.1    18.13  117.2  1009.  ]]\nOutputs (missing):  [[  16.15  105.73  851.19   19.07  126.27 1212.63]]\nInput (missing):  [[  6.98  89.75 609.1    7.93  50.41 185.2 ]]\nTarget (missing):  [[ 14.06  89.75 609.1   14.92  96.42 684.5 ]]\nOutputs (missing):  [[ 12.5   82.29 571.89  14.86  95.4  695.9 ]]\nInput (missing):  [[ 11.34  43.79 391.2   12.47  79.15 185.2 ]]\nTarget (missing):  [[ 11.34  72.76 391.2   12.47  79.15 478.6 ]]\nOutputs (missing):  [[ 11.22  71.78 358.79  12.25  79.36 419.07]]\nInput (missing):  [[  27.22   43.79 2250.     33.12  220.8  3216.  ]]\nTarget (missing):  [[  27.22  182.1  2250.     33.12  220.8  3216.  ]]\nOutputs (missing):  [[  29.02  194.3  2077.08   36.42  245.18 3214.86]]\nInput (missing):  [[ 17.14  43.79 912.7   22.25 152.4  185.2 ]]\nTarget (missing):  [[  17.14  116.    912.7    22.25  152.4  1461.  ]]\nOutputs (missing):  [[  16.56  108.32  877.61   19.48  129.68 1258.29]]\nInput (missing):  [[  6.98  43.79 443.3   13.67  87.78 567.9 ]]\nTarget (missing):  [[ 12.    76.95 443.3   13.67  87.78 567.9 ]]\nOutputs (missing):  [[ 11.79  75.87 420.56  13.08  84.95 512.39]]\nInput (missing):  [[  9.76  61.68 290.9   10.67  50.41 185.2 ]]\nTarget (missing):  [[  9.76  61.68 290.9   10.67  68.03 349.9 ]]\nOutputs (missing):  [[ 10.13  64.63 263.83  10.93  69.12 260.73]]\nInput (missing):  [[ 15.73 102.8  143.5   17.01 112.5  185.2 ]]\nTarget (missing):  [[ 15.73 102.8  747.2   17.01 112.5  854.3 ]]\nOutputs (missing):  [[ 13.84  89.75 620.19  15.86 104.35 839.38]]\nInput (missing):  [[ 11.71  74.68 420.3    7.93  84.42 521.5 ]]\nTarget (missing):  [[ 11.71  74.68 420.3   13.01  84.42 521.5 ]]\nOutputs (missing):  [[ 12.06  77.5  446.38  13.42  87.68 557.16]]\nInput (missing):  [[ 14.58  97.41 644.8   17.62 122.4  185.2 ]]\nTarget (missing):  [[ 14.58  97.41 644.8   17.62 122.4  896.9 ]]\nOutputs (missing):  [[ 14.53  94.12 676.49  16.56 111.5  924.6 ]]\nInput: [[  9.29  59.96 257.8   10.57  67.84 326.6 ]]\nTarget:  [[  9.29  59.96 257.8   10.57  67.84 326.6 ]]\nOutputs:  [[ 10.51  66.97 291.8   11.32  72.58 311.06]]\nInput:  [[  16.02  102.7   797.8    19.19  123.8  1150.  ]]\nTarget:  [[  16.02  102.7   797.8    19.19  123.8  1150.  ]]\nOutputs:  [[  16.44  107.11  848.8    18.98  129.11 1205.57]]\nInput (missing):  [[  20.18   43.79 1245.     23.37  170.3  1623.  ]]\nTarget (missing):  [[  20.18  143.7  1245.     23.37  170.3  1623.  ]]\nOutputs (missing):  [[  20.73  137.22 1282.6    25.22  168.17 1920.41]]\nInput (missing):  [[  9.33  59.01 264.     7.93  62.86 295.8 ]]\nTarget (missing):  [[  9.33  59.01 264.     9.85  62.86 295.8 ]]\nOutputs (missing):  [[ 10.12  64.3  256.66  10.81  69.1  252.94]]\nInput (missing):  [[   6.98   43.79 1033.     20.6   135.1   185.2 ]]\nTarget (missing):  [[  18.22  120.3  1033.     20.6   135.1  1321.  ]]\nOutputs (missing):  [[ 14.47  93.73 663.47  16.41 110.52 906.61]]\nInput:  [[ 11.61  75.46 408.2   12.64  81.93 475.7 ]]\nTarget:  [[ 11.61  75.46 408.2   12.64  81.93 475.7 ]]\nOutputs:  [[ 12.03  77.19 433.95  13.26  87.24 537.92]]\nTest Loss: 0.005\n"
    }
   ],
   "source": [
    "# TEST THE NEURAL NETWORK\n",
    "test_result = test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './model3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following section is for reconstructing a brand new dataset from an input dataset that has missing values.\n",
    "# This new dataset can be used in some classifiers to see if the accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Autoencoder(\n  (enc1): Linear(in_features=6, out_features=8, bias=True)\n  (enc2): Linear(in_features=8, out_features=16, bias=True)\n  (dec2): Linear(in_features=16, out_features=8, bias=True)\n  (dec3): Linear(in_features=8, out_features=6, bias=True)\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = torch.load('model3')\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_set = my_model(x_test.double())\n",
    "predicted_set = scaler.inverse_transform(predicted_set.detach().numpy())\n",
    "\n",
    "real_set = scaler.inverse_transform(y_test.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_values = []\n",
    "predicted_values = []\n",
    "x_values = []\n",
    "\n",
    "\n",
    "for missing_data, full_data, predicted in zip(x_test, y_test, predicted_set):\n",
    "    if NAN in missing_data:\n",
    "        for i in range(len(missing_data)):\n",
    "            if missing_data[i] == NAN:\n",
    "                x_values.append(full_data[0].item())\n",
    "                real_values.append(full_data[i].item())\n",
    "                predicted_values.append(predicted[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "P-value for significance:  0.9781250871884785\nTTEST:  -0.027443738629426527\nConclusion: Accept Null Hypothesis\nP-value for significance:  0.9689931830306344\nTTEST:  -0.03890535292110859\nConclusion: Accept Null Hypothesis\nP-value for significance:  0.9458356840114299\nTTEST:  -0.06799723359707457\nConclusion: Accept Null Hypothesis\nP-value for significance:  0.9693676952140369\nTTEST:  0.03843520550227176\nConclusion: Accept Null Hypothesis\nP-value for significance:  0.9646707574449513\nTTEST:  -0.044332202040331836\nConclusion: Accept Null Hypothesis\nP-value for significance:  0.9762008844173685\nTTEST:  0.0298584879989458\nConclusion: Accept Null Hypothesis\nMean P-Value:  0.9671988818844831\n"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats import weightstats as stets\n",
    "\n",
    "p_values = []\n",
    "\n",
    "for i in range(6):\n",
    "    ttest, pval = stats.ttest_ind(real_set[:, i], predicted_set[:, i])\n",
    "    print(\"P-value for significance: \", pval)\n",
    "    print(\"TTEST: \", ttest)\n",
    "    p_values.append(pval)\n",
    "\n",
    "    if pval<0.05:\n",
    "        print(\"Conclusion: Reject Null Hypothesis\")\n",
    "    else:\n",
    "        print(\"Conclusion: Accept Null Hypothesis\")\n",
    "\n",
    "print(\"Mean P-Value: \", np.mean(p_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('breast/breast.csv')\n",
    "array = np.array(['radius_mean', 'perimeter_mean', 'area_mean', 'radius_worst', 'perimeter_worst', 'area_worst'])\n",
    "filtered_data = dataset[array]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "scaled_set = scaler.fit_transform(filtered_data.to_numpy())\n",
    "scaled_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = fill_nan(scaled_set, scaled_set.size*0.2)\n",
    "\n",
    "input_tensor = torch.from_numpy(missing_data)\n",
    "input_tensor = input_tensor.view(input_tensor.shape[0], NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.6767,  0.6652,  0.4957,  0.6209,  0.6213,  0.4340],\n        [ 0.6617,  0.6589,  0.4991,  0.6331,  0.5989,  0.4400],\n        [ 0.3974,  0.3961,  0.2622,  0.3637,  0.3270,  0.2225],\n        ...,\n        [ 0.4296,  0.4286,  0.3109,  0.4001,  0.3859,  0.2512],\n        [ 0.5644,  0.5569,  0.4080,  0.5235,  0.5057,  0.3541],\n        [ 0.1237,  0.1187,  0.0298,  0.0821,  0.0679, -0.0010]],\n       dtype=torch.float64, grad_fn=<AddmmBackward>)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset = my_model(input_tensor.double())\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "new_dataset = scaler.inverse_transform(new_dataset.detach().numpy())\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset = filtered_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0.9978552814938108, 0.0)\n(0.9453786462552797, 3.343434346388964e-278)\n"
    }
   ],
   "source": [
    "print(scipy.stats.pearsonr(real_dataset[:, 0], real_dataset[:, 1]))\n",
    "print(scipy.stats.pearsonr(real_dataset[:, 0], new_dataset[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0.9873571700566124, 0.0)\n(0.9483294771934577, 7.439654388571332e-285)\n"
    }
   ],
   "source": [
    "print(scipy.stats.pearsonr(real_dataset[:, 0], real_dataset[:, 2]))\n",
    "print(scipy.stats.pearsonr(real_dataset[:, 0], new_dataset[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0.9695389726112063, 0.0)\n(0.9484417182646606, 4.081794241280509e-285)\n"
    }
   ],
   "source": [
    "print(scipy.stats.pearsonr(real_dataset[:, 0], real_dataset[:, 3]))\n",
    "print(scipy.stats.pearsonr(real_dataset[:, 0], new_dataset[:, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0.9651365139559875, 0.0)\n(0.9448805305620419, 4.079523599935882e-277)\n"
    }
   ],
   "source": [
    "print(scipy.stats.pearsonr(real_dataset[:, 0], real_dataset[:, 4]))\n",
    "print(scipy.stats.pearsonr(real_dataset[:, 0], new_dataset[:, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(0.9410824595860467, 3.767446784305406e-269)\n(0.9483678761118814, 6.059396608737617e-285)\n"
    }
   ],
   "source": [
    "print(scipy.stats.pearsonr(real_dataset[:, 0], real_dataset[:, 5]))\n",
    "print(scipy.stats.pearsonr(real_dataset[:, 0], new_dataset[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('new_breast.csv', new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}